{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7cd14fd-0fe0-4480-af12-932cb1fa96b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08393b45-0653-4364-818b-c02f661614de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for os\n"
     ]
    }
   ],
   "source": [
    "pip install os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff160d5-0c4d-42f0-bbed-ddc2e28c77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9ddccd-b6b8-4567-bd08-54dd3c03f46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 633.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 633.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 633.8 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 425.8 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 425.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 476.9 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 476.9 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 539.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.8 MB 583.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 647.7 kB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "Successfully installed pip-25.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1fb9994-6e92-4758-ad3f-644a22469b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8df34c3-b25f-4007-bb08-4fe53f9696b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mammoth in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: cobble<0.2,>=0.1.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mammoth) (0.1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mammoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbdb50a0-a888-4d12-8ec0-c02b318f8f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "606105ae-9772-4a7a-ab11-4fd1b1bb0678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d127d44b-19e7-4db0-9612-33f8bbbf6c42",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (1722728746.py, line 90)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpdf.output(file_path)\u001b[39m\n                         ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donn√© en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier t√©l√©charg√© pour extraire son texte.\"\"\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        return self.current_document_text[:1000] + \"\\n\\n[... Texte tronqu√© pour la pr√©visualisation]\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:300]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel √† l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la g√©n√©ration de la r√©ponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"T√©l√©charge la r√©ponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf43f87-5840-446b-830e-7e5cbfcaad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5496\\41155677.py:138: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Chat\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:8033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 05:08:15,675 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:08:16,417 - INFO: HTTP Request: GET http://localhost:8033/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:08:18,478 - INFO: HTTP Request: HEAD http://localhost:8033/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:08:20,601 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://6a6b014a475eaac76c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 05:08:25,012 - INFO: HTTP Request: HEAD https://6a6b014a475eaac76c.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6a6b014a475eaac76c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donn√© en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier t√©l√©charg√© pour extraire son texte.\"\"\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        return self.current_document_text[:1000] + \"\\n\\n[... Texte tronqu√© pour la pr√©visualisation]\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:300]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel √† l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la g√©n√©ration de la r√©ponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"T√©l√©charge la r√©ponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la cr√©ation du fichier : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# üìÑ Document Chat Assistant\", elem_classes=\"text-2xl font-bold text-blue-600 mb-4\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=300):\n",
    "                gr.Markdown(\"## Document Upload\", elem_classes=\"text-xl font-semibold\")\n",
    "                file_upload = gr.File(file_types=['.pdf', '.docx', '.txt'], label=\"T√©l√©verser un document\")\n",
    "                file_preview = gr.Textbox(label=\"Aper√ßu du document\", lines=10)\n",
    "                context_radio = gr.Radio(choices=[\"Standard\", \"Avec contexte du document\"], value=\"Standard\", label=\"Mode de conversation\")\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                gr.Markdown(\"## Conversation\", elem_classes=\"text-xl font-semibold\")\n",
    "                chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
    "                msg = gr.Textbox(label=\"Votre message\")\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Envoyer\")\n",
    "                    clear_btn = gr.Button(\"R√©initialiser\")\n",
    "                    format_dropdown = gr.Dropdown(choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format de t√©l√©chargement\")\n",
    "                    download_btn = gr.Button(\"T√©l√©charger la r√©ponse\")\n",
    "                    response_file = gr.File()\n",
    "        \n",
    "        file_upload.upload(agent.process_document, inputs=[file_upload], outputs=[file_preview])\n",
    "        submit_btn.click(agent.generate_response, inputs=[msg, chatbot, context_radio], outputs=[chatbot])\n",
    "        download_btn.click(agent.download_response, inputs=[format_dropdown], outputs=[response_file])\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(server_name=\"0.0.0.0\", server_port=8033, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "086bc0d4-8668-4950-a82f-0e00e344ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5496\\3114139170.py:138: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Chat\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 05:44:37,679 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:44:38,561 - INFO: HTTP Request: GET http://localhost:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:44:40,610 - INFO: HTTP Request: HEAD http://localhost:7860/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:44:42,896 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://c759d952ff2df04b83.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 05:44:47,919 - INFO: HTTP Request: HEAD https://c759d952ff2df04b83.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c759d952ff2df04b83.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donn√© en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier t√©l√©charg√© pour extraire son texte.\"\"\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        return self.current_document_text[:1000] + \"\\n\\n[... Texte tronqu√© pour la pr√©visualisation]\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:300]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel √† l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la g√©n√©ration de la r√©ponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"T√©l√©charge la r√©ponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la cr√©ation du fichier : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# üìÑ Document Chat Assistant\", elem_classes=\"text-2xl font-bold text-blue-600 mb-4\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=300):\n",
    "                gr.Markdown(\"## Document Upload\", elem_classes=\"text-xl font-semibold\")\n",
    "                file_upload = gr.File(file_types=['.pdf', '.docx', '.txt'], label=\"T√©l√©verser un document\")\n",
    "                file_preview = gr.Textbox(label=\"Aper√ßu du document\", lines=10)\n",
    "                context_radio = gr.Radio(choices=[\"Standard\", \"Avec contexte du document\"], value=\"Standard\", label=\"Mode de conversation\")\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                gr.Markdown(\"## Conversation\", elem_classes=\"text-xl font-semibold\")\n",
    "                chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
    "                msg = gr.Textbox(label=\"Votre message\")\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Envoyer\")\n",
    "                    clear_btn = gr.Button(\"R√©initialiser\")\n",
    "                    format_dropdown = gr.Dropdown(choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format de t√©l√©chargement\")\n",
    "                    download_btn = gr.Button(\"T√©l√©charger la r√©ponse\")\n",
    "                    response_file = gr.File()\n",
    "        \n",
    "        file_upload.upload(agent.process_document, inputs=[file_upload], outputs=[file_preview])\n",
    "        submit_btn.click(agent.generate_response, inputs=[msg, chatbot, context_radio], outputs=[chatbot])\n",
    "        download_btn.click(agent.download_response, inputs=[format_dropdown], outputs=[response_file])\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(server_name=\"0.0.0.0\", server_port= 7860, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf38caa-b93e-4040-a9e3-3f04e5afbef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\1654830701.py:144: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"üí¨ Conversation\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:58:55,817 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 10:58:56,978 - INFO: HTTP Request: GET http://localhost:7865/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 10:58:59,011 - INFO: HTTP Request: HEAD http://localhost:7865/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 10:59:00,431 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://62117fb62850c0f2da.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:59:02,943 - INFO: HTTP Request: HEAD https://62117fb62850c0f2da.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://62117fb62850c0f2da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Chargement des variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "JIRA_USER_EMAIL = os.getenv(\"JIRA_USER_EMAIL\")\n",
    "JIRA_URL = os.getenv(\"JIRA_URL\")  # ex: https://ton-espace.atlassian.net\n",
    "JIRA_PROJECT_KEY = os.getenv(\"JIRA_PROJECT_KEY\")\n",
    "\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    return \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])[:max_chars]\n",
    "            elif extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    return mammoth.extract_raw_text(file).value[:max_chars]\n",
    "            elif extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'extraction : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "class JiraClient:\n",
    "    def __init__(self):\n",
    "        self.auth = (JIRA_USER_EMAIL, JIRA_API_TOKEN)\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_issue(self, summary: str, description: str) -> str:\n",
    "        try:\n",
    "            payload = {\n",
    "                \"fields\": {\n",
    "                    \"project\": {\"key\": JIRA_PROJECT_KEY},\n",
    "                    \"summary\": summary,\n",
    "                    \"description\": description,\n",
    "                    \"issuetype\": {\"name\": \"Task\"}\n",
    "                }\n",
    "            }\n",
    "            response = requests.post(f\"{JIRA_URL}/rest/api/3/issue\", json=payload, headers=self.headers, auth=self.auth)\n",
    "            response.raise_for_status()\n",
    "            return f\"T√¢che cr√©√©e avec succ√®s : {response.json()['key']}\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de cr√©ation de t√¢che : {e}\")\n",
    "            return f\"Erreur Jira : {e}\"\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.jira = JiraClient()\n",
    "        self.document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "\n",
    "    def process_file(self, file) -> str:\n",
    "        self.document_text = self.processor.extract_text_from_file(file.name)\n",
    "        return self.document_text[:1000] + \"\\n\\n[... Texte tronqu√©]\"\n",
    "\n",
    "    def generate_test_cases(self, prompt: str, chat_history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        try:\n",
    "            context = f\"Contexte:\\n{self.document_text[:300]}\\n\\n\" if context_type == \"Avec contexte\" and self.document_text else \"\"\n",
    "            for user_msg, assistant_msg in chat_history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {prompt}\\nAssistant: \"\n",
    "\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "\n",
    "            reply = result.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Erreur de r√©ponse\")\n",
    "            self.last_response = reply\n",
    "            chat_history.append((prompt, reply))\n",
    "            return chat_history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            chat_history.append((prompt, f\"Erreur : {e}\"))\n",
    "            return chat_history\n",
    "\n",
    "    def download_test_cases(self, file_format: str) -> str:\n",
    "        filename = f\"test_cases.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                df = pd.DataFrame([line.split(\";\") for line in lines])\n",
    "                df.to_excel(filename, index=False, header=False)\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def send_to_jira(self) -> str:\n",
    "        summary = \"Cas de test g√©n√©r√© automatiquement\"\n",
    "        return self.jira.create_issue(summary, self.last_response)\n",
    "\n",
    "def build_ui():\n",
    "    assistant = TestCaseGenerator()\n",
    "\n",
    "    with gr.Blocks(theme=gr.themes.Base()) as ui:\n",
    "        gr.Markdown(\"# ü§ñ G√©n√©rateur de cas de tests IA + Int√©gration Jira\", elem_classes=\"text-2xl font-bold mb-4\")\n",
    "\n",
    "        with gr.Row():\n",
    "            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"üìÑ T√©l√©verser un document\")\n",
    "            preview = gr.Textbox(label=\"üìÉ Aper√ßu\", lines=10)\n",
    "        \n",
    "        context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode IA\")\n",
    "        chatbot = gr.Chatbot(label=\"üí¨ Conversation\")\n",
    "        user_input = gr.Textbox(label=\"Entrez votre message\")\n",
    "\n",
    "        with gr.Row():\n",
    "            send_btn = gr.Button(\"Envoyer\")\n",
    "            clear_btn = gr.Button(\"R√©initialiser\")\n",
    "\n",
    "        with gr.Row():\n",
    "            download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format export\")\n",
    "            download_btn = gr.Button(\"T√©l√©charger\")\n",
    "            download_file = gr.File()\n",
    "\n",
    "        with gr.Row():\n",
    "            jira_btn = gr.Button(\"üìå Cr√©er une t√¢che dans Jira\")\n",
    "            jira_result = gr.Textbox(label=\"R√©sultat Jira\")\n",
    "\n",
    "        file_input.upload(assistant.process_file, inputs=[file_input], outputs=[preview])\n",
    "        send_btn.click(assistant.generate_test_cases, inputs=[user_input, chatbot, context_mode], outputs=[chatbot])\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_test_cases, inputs=[download_format], outputs=[download_file])\n",
    "        jira_btn.click(assistant.send_to_jira, outputs=[jira_result])\n",
    "\n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7865, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e70a7a-f379-4ad3-9af6-1bdb0cd4e182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\2417973827.py:161: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"üí¨ Conversation\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 12:57:06,536 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 12:57:06,898 - INFO: HTTP Request: GET http://localhost:7863/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 12:57:08,945 - INFO: HTTP Request: HEAD http://localhost:7863/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 12:57:10,531 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://af4e23ccca0b2fc1ef.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 12:57:15,138 - INFO: HTTP Request: HEAD https://af4e23ccca0b2fc1ef.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://af4e23ccca0b2fc1ef.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    return \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])[:max_chars]\n",
    "            elif extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    return mammoth.extract_raw_text(file).value[:max_chars]\n",
    "            elif extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'extraction : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "class JiraClient:\n",
    "    def __init__(self, email: str, url: str):\n",
    "        self.email = email\n",
    "        self.url = url\n",
    "        self.auth = (self.email, JIRA_API_TOKEN)\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_issue(self, project_key: str, summary: str, description: str, issue_type: str = \"Task\") -> str:\n",
    "        try:\n",
    "            payload = {\n",
    "                \"fields\": {\n",
    "                    \"project\": {\"key\": project_key},\n",
    "                    \"summary\": summary,\n",
    "                    \"description\": description,\n",
    "                    \"issuetype\": {\"name\": issue_type}\n",
    "                }\n",
    "            }\n",
    "            response = requests.post(f\"{self.url}/rest/api/3/issue\", json=payload, headers=self.headers, auth=self.auth)\n",
    "            response.raise_for_status()\n",
    "            return f\"T√¢che cr√©√©e avec succ√®s : {response.json()['key']}\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de cr√©ation de t√¢che : {e}\")\n",
    "            return f\"Erreur Jira : {e}\"\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "        self.jira_email = \"\"\n",
    "        self.jira_url = \"\"\n",
    "        self.jira_project_key = \"\"\n",
    "        self.jira_issue_type = \"Task\"\n",
    "\n",
    "    def set_jira_credentials(self, email: str, url: str, project_key: str, issue_type: str):\n",
    "        self.jira_email = email\n",
    "        self.jira_url = url\n",
    "        self.jira_project_key = project_key\n",
    "        self.jira_issue_type = issue_type\n",
    "\n",
    "    def process_file(self, file) -> str:\n",
    "        self.document_text = self.processor.extract_text_from_file(file.name)\n",
    "        return self.document_text[:1000] + \"\\n\\n[... Texte tronqu√©]\"\n",
    "\n",
    "    def generate_test_cases(self, prompt: str, chat_history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        try:\n",
    "            context = f\"Contexte:\\n{self.document_text[:300]}\\n\\n\" if context_type == \"Avec contexte\" and self.document_text else \"\"\n",
    "            for user_msg, assistant_msg in chat_history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {prompt}\\nAssistant: \"\n",
    "\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "\n",
    "            reply = result.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Erreur de r√©ponse\")\n",
    "            self.last_response = reply\n",
    "            chat_history.append((prompt, reply))\n",
    "            return chat_history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            chat_history.append((prompt, f\"Erreur : {e}\"))\n",
    "            return chat_history\n",
    "\n",
    "    def download_test_cases(self, file_format: str):\n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "\n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                df = pd.DataFrame([line.split(\";\") for line in lines])\n",
    "                df.to_excel(filename, index=False, header=False)\n",
    "\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def send_to_jira(self) -> str:\n",
    "        if not all([self.jira_email, self.jira_url, self.jira_project_key]):\n",
    "            return \"Veuillez remplir les identifiants Jira.\"\n",
    "        jira_client = JiraClient(email=self.jira_email, url=self.jira_url)\n",
    "        summary = \"Cas de test g√©n√©r√© automatiquement\"\n",
    "        return jira_client.create_issue(self.jira_project_key, summary, self.last_response, self.jira_issue_type)\n",
    "\n",
    "def build_ui():\n",
    "    assistant = TestCaseGenerator()\n",
    "\n",
    "    with gr.Blocks(theme=gr.themes.Base()) as ui:\n",
    "        gr.Markdown(\"# üß† G√©n√©rateur de cas de tests IA + Int√©gration Jira\", elem_classes=\"text-2xl font-bold mb-4\")\n",
    "\n",
    "        with gr.Row():\n",
    "            jira_email = gr.Textbox(label=\"Email User\")\n",
    "            jira_url = gr.Textbox(label=\"URL Jira (ex: https://ton-espace.atlassian.net)\")\n",
    "            jira_project = gr.Textbox(label=\"Cl√© du projet Jira\")\n",
    "            jira_type = gr.Dropdown(label=\"Type de t√¢che\", choices=[\"Task\", \"Bug\", \"Story\"], value=\"Task\")\n",
    "            set_jira = gr.Button(\"Valider les identifiants\")\n",
    "\n",
    "        with gr.Row():\n",
    "            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"üìÑ T√©l√©verser un document\")\n",
    "            preview = gr.Textbox(label=\"üìú Aper√ßu\", lines=10)\n",
    "\n",
    "        context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode IA\")\n",
    "        chatbot = gr.Chatbot(label=\"üí¨ Conversation\")\n",
    "        user_input = gr.Textbox(label=\"Entrez votre message\")\n",
    "\n",
    "        with gr.Row():\n",
    "            send_btn = gr.Button(\"Envoyer\")\n",
    "            clear_btn = gr.Button(\"R√©initialiser\")\n",
    "\n",
    "        with gr.Row():\n",
    "            download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format export\")\n",
    "            download_btn = gr.Button(\"T√©l√©charger\")\n",
    "            download_file = gr.File(label=\"Fichier export√©\", interactive=False)\n",
    "\n",
    "        with gr.Row():\n",
    "            jira_btn = gr.Button(\"üìå Cr√©er une t√¢che dans Jira\")\n",
    "            jira_result = gr.Textbox(label=\"R√©sultat Jira\")\n",
    "\n",
    "        set_jira.click(assistant.set_jira_credentials, inputs=[jira_email, jira_url, jira_project, jira_type])\n",
    "        file_input.upload(assistant.process_file, inputs=[file_input], outputs=[preview])\n",
    "        send_btn.click(assistant.generate_test_cases, inputs=[user_input, chatbot, context_mode], outputs=[chatbot])\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_test_cases, inputs=[download_format], outputs=[download_file])\n",
    "        jira_btn.click(assistant.send_to_jira, outputs=[jira_result])\n",
    "\n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7863, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2b0573-5437-4a82-9e2a-dd45a3d35f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\3105611391.py:347: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 07:15:53,395 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 07:15:53,947 - INFO: HTTP Request: GET http://localhost:7866/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 07:15:56,013 - INFO: HTTP Request: HEAD http://localhost:7866/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 07:15:58,112 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://2837ce49d40679e548.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 07:16:12,951 - INFO: HTTP Request: HEAD https://2837ce49d40679e548.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2837ce49d40679e548.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Variables d'environnement\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte de diff√©rents formats de fichiers\"\"\"\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    return \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])[:max_chars]\n",
    "            elif extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    return mammoth.extract_raw_text(file).value[:max_chars]\n",
    "            elif extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'extraction : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "class JiraClient:\n",
    "    def __init__(self, email: str, url: str):\n",
    "        self.email = email\n",
    "        self.url = url\n",
    "        self.auth = (self.email, JIRA_API_TOKEN)\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_issue(self, project_key: str, summary: str, description: str, issue_type: str = \"Task\") -> dict:\n",
    "        \"\"\"Cr√©e une issue dans Jira et renvoie le r√©sultat et le statut\"\"\"\n",
    "        try:\n",
    "            payload = {\n",
    "                \"fields\": {\n",
    "                    \"project\": {\"key\": project_key},\n",
    "                    \"summary\": summary,\n",
    "                    \"description\": description,\n",
    "                    \"issuetype\": {\"name\": issue_type}\n",
    "                }\n",
    "            }\n",
    "            response = requests.post(f\"{self.url}/rest/api/3/issue\", json=payload, headers=self.headers, auth=self.auth)\n",
    "            response.raise_for_status()\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"T√¢che cr√©√©e avec succ√®s : {response.json()['key']}\",\n",
    "                \"key\": response.json()['key']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de cr√©ation de t√¢che : {e}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"message\": f\"Erreur Jira : {str(e)}\"\n",
    "            }\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "        self.jira_email = \"\"\n",
    "        self.jira_url = \"\"\n",
    "        self.jira_project_key = \"\"\n",
    "        self.jira_issue_type = \"Task\"\n",
    "        self.processing = False\n",
    "\n",
    "    def set_jira_credentials(self, email: str, url: str, project_key: str, issue_type: str):\n",
    "        \"\"\"Configure les identifiants Jira\"\"\"\n",
    "        self.jira_email = email\n",
    "        self.jira_url = url\n",
    "        self.jira_project_key = project_key\n",
    "        self.jira_issue_type = issue_type\n",
    "        \n",
    "        # V√©rification basique\n",
    "        valid = all([email, url, project_key])\n",
    "        message = \"‚úÖ Identifiants Jira enregistr√©s\" if valid else \"‚ùå Veuillez compl√©ter tous les champs requis\"\n",
    "        return message\n",
    "\n",
    "    def process_file(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.document_text = self.processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Pr√©parer l'aper√ßu (limit√©)\n",
    "            preview = self.document_text[:1500]\n",
    "            if len(self.document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqu√©]\"\n",
    "                \n",
    "            char_count = len(self.document_text)\n",
    "            file_info = f\"üìÑ Fichier : {self.document_name} | üìä {char_count} caract√®res\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_test_cases(self, prompt: str, chat_history: List[Tuple[str, str]], context_type: str, generation_type: str):\n",
    "        \"\"\"G√©n√®re des cas de tests en fonction du prompt et de l'historique\"\"\"\n",
    "        if not prompt:\n",
    "            chat_history.append((\"\", \"Veuillez entrer une requ√™te.\"))\n",
    "            return chat_history, gr.update(value=\"\")\n",
    "        \n",
    "        try:\n",
    "            self.processing = True\n",
    "            \n",
    "            # Construction du contexte\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.document_text[:2000]}\\n\\n\"\n",
    "                if len(self.document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqu√© pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Construction de l'instruction bas√©e sur le type de g√©n√©ration\n",
    "            system_instruction = \"Tu es un expert en test logiciel. \"\n",
    "            if generation_type == \"Cas de test fonctionnels\":\n",
    "                system_instruction += \"G√©n√®re des cas de test fonctionnels d√©taill√©s avec pr√©conditions, √©tapes et r√©sultats attendus.\"\n",
    "            elif generation_type == \"Tests d'acceptation\":\n",
    "                system_instruction += \"G√©n√®re des tests d'acceptation au format Gherkin (Given-When-Then).\"\n",
    "            elif generation_type == \"Sc√©narios de test BDD\":\n",
    "                system_instruction += \"Cr√©e des sc√©narios de tests comportementaux d√©taill√©s.\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in chat_history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{system_instruction}\\n\\n{context}Utilisateur: {prompt}\\nAssistant: \"\n",
    "            \n",
    "            # Appel √† l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            reply = result.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Erreur de r√©ponse\")\n",
    "            self.last_response = reply\n",
    "            chat_history.append((prompt, reply))\n",
    "            \n",
    "            self.processing = False\n",
    "            return chat_history, gr.update(value=\"\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            chat_history.append((prompt, f\"Erreur : {str(e)}\"))\n",
    "            self.processing = False\n",
    "            return chat_history, gr.update(value=\"\")\n",
    "\n",
    "    def download_test_cases(self, file_format: str):\n",
    "        \"\"\"G√©n√®re un fichier t√©l√©chargeable au format sp√©cifi√©\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger. G√©n√©rez d'abord des cas de test.\"\n",
    "        \n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "            \n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"cas_de_test_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traitement du texte pour FPDF (√©viter les probl√®mes d'encodage)\n",
    "                clean_text = self.last_response.encode('latin-1', 'replace').decode('latin-1')\n",
    "                pdf.multi_cell(190, 7, clean_text)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"Cas de Tests G√©n√©r√©s\", level=1)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter une conversion en tableau simple\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Cr√©er des colonnes par d√©faut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_excel(filename, index=False)\n",
    "            \n",
    "            return (filename, display_name), \"‚úÖ Fichier g√©n√©r√© avec succ√®s\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"‚ùå Erreur lors de la g√©n√©ration du fichier : {str(e)}\"\n",
    "\n",
    "    def send_to_jira(self, title: str):\n",
    "        \"\"\"Envoie les cas de test √† Jira\"\"\"\n",
    "        if not self.last_response:\n",
    "            return \"‚ùå Aucun contenu √† envoyer √† Jira. G√©n√©rez d'abord des cas de test.\"\n",
    "        \n",
    "        if not all([self.jira_email, self.jira_url, self.jira_project_key]):\n",
    "            return \"‚ùå Veuillez configurer vos identifiants Jira avant de cr√©er une t√¢che.\"\n",
    "        \n",
    "        if not title:\n",
    "            title = f\"Cas de tests g√©n√©r√©s ({datetime.now().strftime('%d/%m/%Y')})\"\n",
    "        \n",
    "        jira_client = JiraClient(email=self.jira_email, url=self.jira_url)\n",
    "        result = jira_client.create_issue(self.jira_project_key, title, self.last_response, self.jira_issue_type)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            return f\"‚úÖ {result['message']}\"\n",
    "        else:\n",
    "            return f\"‚ùå {result['message']}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = TestCaseGenerator()\n",
    "    \n",
    "    # Th√®me personnalis√© - CORRIG√â pour supprimer la propri√©t√© checkbox_text_color_selected\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"indigo\",\n",
    "        secondary_hue=\"blue\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"TestCaseGenius\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üß™ TestCaseGenius\n",
    "            ### G√©n√©rateur intelligent de cas de tests avec int√©gration Jira\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"G√©n√©rateur de tests\", id=\"generator\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Remplacement de gr.Box par gr.Group avec div CSS\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"T√©l√©versez un document de sp√©cification (optionnel)\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document charg√©\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de g√©n√©ration\"\n",
    "                                )\n",
    "                                generation_type = gr.Radio(\n",
    "                                    [\"Cas de test fonctionnels\", \"Tests d'acceptation\", \"Sc√©narios de test BDD\"],\n",
    "                                    value=\"Cas de test fonctionnels\",\n",
    "                                    label=\"Type de tests\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"T√©l√©charger le fichier g√©n√©r√©\", \n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Remplacement de gr.Box par gr.Group avec div CSS\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ G√©n√©rateur de cas de tests\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Demandez √† l'IA de g√©n√©rer des cas de tests\",\n",
    "                                    placeholder=\"Exemple: G√©n√®re des cas de test pour une fonctionnalit√© de connexion\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"üí¨ G√©n√©rer les cas de test\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Int√©gration Jira\", id=\"jira\", elem_classes=\"tab-nav\"):\n",
    "                # Remplacement de gr.Box par gr.Group avec div CSS\n",
    "                with gr.Group(elem_classes=\"container\"):\n",
    "                    gr.Markdown(\"### üîê Param√®tres de connexion Jira\")\n",
    "                    with gr.Row():\n",
    "                        jira_email = gr.Textbox(\n",
    "                            label=\"Email Jira\",\n",
    "                            placeholder=\"votre.email@entreprise.com\"\n",
    "                        )\n",
    "                        jira_url = gr.Textbox(\n",
    "                            label=\"URL Jira\",\n",
    "                            placeholder=\"https://votre-espace.atlassian.net\"\n",
    "                        )\n",
    "                    with gr.Row():\n",
    "                        jira_project = gr.Textbox(\n",
    "                            label=\"Cl√© du projet Jira\",\n",
    "                            placeholder=\"Ex: PROJ\"\n",
    "                        )\n",
    "                        jira_type = gr.Dropdown(\n",
    "                            label=\"Type de t√¢che\", \n",
    "                            choices=[\"Task\", \"Bug\", \"Story\", \"Test Case\", \"Epic\"],\n",
    "                            value=\"Task\"\n",
    "                        )\n",
    "                    set_jira = gr.Button(\"üíæ Enregistrer les param√®tres\", variant=\"primary\")\n",
    "                    jira_creds_status = gr.Textbox(\n",
    "                        label=\"Statut\",\n",
    "                        interactive=False\n",
    "                    )\n",
    "                    \n",
    "                    gr.Markdown(\"### üìå Cr√©ation de t√¢che Jira\")\n",
    "                    jira_title = gr.Textbox(\n",
    "                        label=\"Titre de la t√¢che\",\n",
    "                        placeholder=\"Titre descriptif pour votre t√¢che Jira\"\n",
    "                    )\n",
    "                    jira_btn = gr.Button(\"üöÄ Cr√©er une t√¢che dans Jira\", variant=\"primary\")\n",
    "                    jira_result = gr.Textbox(\n",
    "                        label=\"R√©sultat\",\n",
    "                        interactive=False\n",
    "                    )\n",
    "                \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "                \n",
    "                **TestCaseGenius** est un outil qui utilise l'IA pour g√©n√©rer des cas de tests √† partir de vos sp√©cifications.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **T√©l√©chargez un document** (optionnel) \n",
    "                   - Formats support√©s : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte √† l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : g√©n√©ration sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme r√©f√©rence\n",
    "                   - **Type de tests** : choisissez le format de test souhait√©\n",
    "                \n",
    "                3. **Demandez √† l'IA**\n",
    "                   - Soyez pr√©cis dans vos requ√™tes\n",
    "                   - Exemple : \"G√©n√®re des cas de test pour la fonctionnalit√© de r√©initialisation de mot de passe\"\n",
    "                \n",
    "                4. **Exportez le r√©sultat**\n",
    "                   - T√©l√©chargez au format TXT, PDF, DOCX ou XLSX\n",
    "                   - Envoyez directement vers Jira (configuration requise)\n",
    "                \n",
    "                5. **Configuration Jira**\n",
    "                   - Renseignez vos identifiants dans l'onglet \"Int√©gration Jira\"\n",
    "                   - La variable d'environnement JIRA_API_TOKEN doit √™tre configur√©e\n",
    "                \n",
    "                #### Exemples de requ√™tes efficaces :\n",
    "                \n",
    "                - \"Cr√©e 5 cas de test pour valider un formulaire d'inscription\"\n",
    "                - \"G√©n√®re des tests de non-r√©gression pour une API de paiement\"\n",
    "                - \"√âcris des tests d'acceptation pour la fonctionnalit√© de recherche avanc√©e\"\n",
    "                - \"D√©veloppe des sc√©narios BDD pour un panier d'achat e-commerce\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 TestCaseGenius | Propuls√© par IA | Cr√©√© pour les √©quipes QA</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des √©v√©nements\n",
    "        file_input.upload(\n",
    "            assistant.process_file, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_test_cases, \n",
    "            inputs=[user_input, chatbot, context_mode, generation_type], \n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_test_cases, \n",
    "            inputs=[user_input, chatbot, context_mode, generation_type], \n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_test_cases, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "        set_jira.click(\n",
    "            assistant.set_jira_credentials, \n",
    "            inputs=[jira_email, jira_url, jira_project, jira_type], \n",
    "            outputs=[jira_creds_status]\n",
    "        )\n",
    "        \n",
    "        jira_btn.click(\n",
    "            assistant.send_to_jira, \n",
    "            inputs=[jira_title], \n",
    "            outputs=[jira_result]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7866, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef33d76-9e79-4e6f-a94d-feed75d9f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\4248759264.py:278: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:29:13,625 - INFO: HTTP Request: GET http://localhost:7869/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:29:14,836 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:29:15,715 - INFO: HTTP Request: HEAD http://localhost:7869/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:29:17,800 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://dd3272df300909736f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:29:22,511 - INFO: HTTP Request: HEAD https://dd3272df300909736f.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dd3272df300909736f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Pr√©parer l'aper√ßu (limit√©)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqu√©]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"üìÑ Fichier : {self.document_name} | üìä {char_count} caract√®res\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse bas√©e sur le prompt et l'historique de conversation\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqu√© pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel √† l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            if \"candidates\" in result and result[\"candidates\"]:\n",
    "                raw_reply = result[\"candidates\"][0].get(\"content\", {})\n",
    "                if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                    reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "                else:\n",
    "                    reply = \"Erreur: Format de r√©ponse inattendu\"\n",
    "            else:\n",
    "                reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            self.last_response = reply\n",
    "            history.append((message, reply))\n",
    "            \n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append((message, f\"Erreur : {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"G√©n√®re un fichier t√©l√©chargeable au format sp√©cifi√©\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger. G√©n√©rez d'abord une r√©ponse.\"\n",
    "        \n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "            \n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"reponse_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traitement du texte pour FPDF (√©viter les probl√®mes d'encodage)\n",
    "                clean_text = self.last_response.encode('latin-1', 'replace').decode('latin-1')\n",
    "                pdf.multi_cell(190, 7, clean_text)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"R√©ponse g√©n√©r√©e\", level=1)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter une conversion en tableau simple\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Cr√©er des colonnes par d√©faut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_excel(filename, index=False)\n",
    "            elif file_format == \"csv\":\n",
    "                # Tenter une conversion en CSV\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Cr√©er des colonnes par d√©faut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_csv(filename, index=False)\n",
    "            \n",
    "            return (filename, display_name), \"‚úÖ Fichier g√©n√©r√© avec succ√®s\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"‚ùå Erreur lors de la g√©n√©ration du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Th√®me personnalis√© - inspir√© de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"T√©l√©versez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document charg√©\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de g√©n√©ration\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"T√©l√©charger le fichier g√©n√©r√©\", \n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents gr√¢ce √† l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **T√©l√©chargez un document**\n",
    "                   - Formats support√©s : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte √† l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme r√©f√©rence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez pr√©cis dans vos requ√™tes\n",
    "                   - Exemple : \"Peux-tu r√©sumer ce document ?\" ou \"Quels sont les points principaux abord√©s ?\"\n",
    "                \n",
    "                4. **Exportez le r√©sultat**\n",
    "                   - T√©l√©chargez les r√©ponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requ√™tes efficaces :\n",
    "                \n",
    "                - \"R√©sume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations cl√©s pr√©sent√©es dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des √©v√©nements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7869, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5b915a-6fb8-494e-a311-bafdfc668395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:53:01,188 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:53:01,824 - INFO: HTTP Request: GET http://localhost:7870/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:53:04,156 - INFO: HTTP Request: HEAD http://localhost:7870/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from openpyxl import Workbook\n",
    "from gemini_api import generate_test_cases_with_gemini\n",
    "\n",
    "def generate_test_cases(input_text, file_format, project_name, jira_link):\n",
    "    # G√©n√©rer les cas de test √† l'aide de l'API Gemini\n",
    "    test_cases = generate_test_cases_with_gemini(input_text, project_name, jira_link)\n",
    "    \n",
    "    # D√©terminer le dossier de sortie\n",
    "    output_dir = Path(\"generated_files\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # G√©n√©rer le nom du fichier avec horodatage\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if file_format == \"Excel\":\n",
    "        output_file_path = output_dir / f\"{project_name}_test_cases_{timestamp}.xlsx\"\n",
    "        df = pd.DataFrame(test_cases)\n",
    "        df.to_excel(output_file_path, index=False)\n",
    "        return output_file_path\n",
    "\n",
    "    elif file_format == \"CSV\":\n",
    "        output_file_path = output_dir / f\"{project_name}_test_cases_{timestamp}.csv\"\n",
    "        df = pd.DataFrame(test_cases)\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "        return output_file_path\n",
    "\n",
    "    elif file_format == \"JSON\":\n",
    "        output_file_path = output_dir / f\"{project_name}_test_cases_{timestamp}.json\"\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(test_cases, f, indent=4, ensure_ascii=False)\n",
    "        return output_file_path\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Format de fichier non support√©.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31d45c99-35d9-4a3e-a98d-914163891652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:09:25,979 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:09:27,045 - INFO: HTTP Request: GET http://localhost:7871/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:09:29,107 - INFO: HTTP Request: HEAD http://localhost:7871/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Logger config\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join([p.extract_text() or \"\" for p in reader.pages])\n",
    "                return text[:max_chars]\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    result = mammoth.extract_raw_text(f)\n",
    "                return result.value[:max_chars]\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    return f.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "\n",
    "    def process_document(self, file):\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        try:\n",
    "            self.document_name = os.path.basename(file.name)\n",
    "            self.current_document_text = self.processor.extract_text_from_file(file.name)\n",
    "\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqu√©]\"\n",
    "\n",
    "            return (\n",
    "                f\"üìÑ {self.document_name} | {len(self.current_document_text)} caract√®res\",\n",
    "                gr.update(visible=True),\n",
    "                preview\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur : {e}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Dict[str, str]], context_type: str):\n",
    "        if not message:\n",
    "            history.append({\"role\": \"assistant\", \"content\": \"Veuillez entrer un message.\"})\n",
    "            return history\n",
    "\n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context += f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Document tronqu√©]\\n\\n\"\n",
    "\n",
    "            for msg in history:\n",
    "                role = \"Utilisateur\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "                context += f\"{role}: {msg['content']}\\n\"\n",
    "\n",
    "            prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=payload, headers=headers)\n",
    "            data = response.json()\n",
    "\n",
    "            reply = \"R√©ponse indisponible.\"\n",
    "            if \"candidates\" in data and data[\"candidates\"]:\n",
    "                candidate = data[\"candidates\"][0]\n",
    "                if \"content\" in candidate and \"parts\" in candidate[\"content\"]:\n",
    "                    reply = candidate[\"content\"][\"parts\"][0].get(\"text\", reply)\n",
    "\n",
    "            self.last_response = reply\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Erreur : {e}\"})\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucune r√©ponse disponible √† exporter.\"\n",
    "\n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "\n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                clean_text = self.last_response.encode(\"latin-1\", \"replace\").decode(\"latin-1\")\n",
    "                pdf.multi_cell(190, 8, clean_text)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"R√©ponse g√©n√©r√©e\", 0)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                lines = [line.strip() for line in self.last_response.splitlines() if line.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                if file_format == \"xlsx\":\n",
    "                    df.to_excel(filename, index=False)\n",
    "                else:\n",
    "                    df.to_csv(filename, index=False)\n",
    "\n",
    "            return (filename, os.path.basename(filename)), \"‚úÖ Fichier g√©n√©r√© avec succ√®s\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"‚ùå Erreur : {e}\"\n",
    "\n",
    "\n",
    "def build_ui():\n",
    "    assistant = DocumentChatAssistant()\n",
    "    theme = gr.themes.Soft(primary_hue=\"blue\").set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_text_color=\"white\",\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme) as ui:\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\"# üí¨ Document Chat Assistant\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.Tab(\"üìÑ Chat avec documents\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"])\n",
    "                        file_info = gr.Textbox(label=\"Fichier\", interactive=False)\n",
    "                        with gr.Accordion(\"Aper√ßu\", visible=False) as preview_accordion:\n",
    "                            preview = gr.Textbox(label=\"Aper√ßu\", lines=10, interactive=False)\n",
    "                        context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode\")\n",
    "\n",
    "                        with gr.Row():\n",
    "                            download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"pdf\")\n",
    "                            download_btn = gr.Button(\"T√©l√©charger\")\n",
    "                        download_file = gr.File()\n",
    "                        download_status = gr.Textbox(visible=True, interactive=False)\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        chatbot = gr.Chatbot(label=\"Conversation\", height=400, type=\"messages\")\n",
    "                        user_input = gr.Textbox(label=\"Votre question\")\n",
    "                        with gr.Row():\n",
    "                            send_btn = gr.Button(\"üí¨ Envoyer\")\n",
    "                            clear_btn = gr.Button(\"üßπ Effacer\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(assistant.process_document, [file_input], [file_info, preview_accordion, preview])\n",
    "        send_btn.click(assistant.generate_response, [user_input, chatbot, context_mode], [chatbot]).then(\n",
    "            lambda: \"\", None, [user_input]\n",
    "        )\n",
    "        user_input.submit(assistant.generate_response, [user_input, chatbot, context_mode], [chatbot]).then(\n",
    "            lambda: \"\", None, [user_input]\n",
    "        )\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_response, [download_format], [download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7871)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "842cc5c5-e932-4eb4-999d-a1a42341af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\301940393.py:165: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation IA\", show_copy_button=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:34:01,115 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:34:02,238 - INFO: HTTP Request: GET http://localhost:7872/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:34:04,296 - INFO: HTTP Request: HEAD http://localhost:7872/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:34:05,542 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://5186faeea363d25555.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:34:08,432 - INFO: HTTP Request: HEAD https://5186faeea363d25555.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5186faeea363d25555.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    return \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])[:max_chars]\n",
    "            elif extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    return mammoth.extract_raw_text(file).value[:max_chars]\n",
    "            elif extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'extraction : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "class JiraClient:\n",
    "    def __init__(self, email: str, url: str):\n",
    "        self.email = email\n",
    "        self.url = url\n",
    "        self.auth = (self.email, JIRA_API_TOKEN)\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_issue(self, project_key: str, summary: str, description: str, issue_type: str = \"Task\") -> str:\n",
    "        try:\n",
    "            payload = {\n",
    "                \"fields\": {\n",
    "                    \"project\": {\"key\": project_key},\n",
    "                    \"summary\": summary,\n",
    "                    \"description\": description,\n",
    "                    \"issuetype\": {\"name\": issue_type}\n",
    "                }\n",
    "            }\n",
    "            response = requests.post(f\"{self.url}/rest/api/3/issue\", json=payload, headers=self.headers, auth=self.auth)\n",
    "            response.raise_for_status()\n",
    "            return f\"T√¢che cr√©√©e avec succ√®s : {response.json()['key']}\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de cr√©ation de t√¢che : {e}\")\n",
    "            return f\"Erreur Jira : {e}\"\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "        self.jira_email = \"\"\n",
    "        self.jira_url = \"\"\n",
    "        self.jira_project_key = \"\"\n",
    "        self.jira_issue_type = \"Task\"\n",
    "\n",
    "    def set_jira_credentials(self, email: str, url: str, project_key: str, issue_type: str):\n",
    "        self.jira_email = email\n",
    "        self.jira_url = url\n",
    "        self.jira_project_key = project_key\n",
    "        self.jira_issue_type = issue_type\n",
    "\n",
    "    def process_file(self, file) -> str:\n",
    "        self.document_text = self.processor.extract_text_from_file(file.name)\n",
    "        return self.document_text[:1000] + \"\\n\\n[... Texte tronqu√©]\"\n",
    "\n",
    "    def generate_test_cases(self, prompt: str, chat_history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        try:\n",
    "            context = f\"Contexte:\\n{self.document_text[:300]}\\n\\n\" if context_type == \"Avec contexte\" and self.document_text else \"\"\n",
    "            for user_msg, assistant_msg in chat_history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {prompt}\\nAssistant: \"\n",
    "\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "\n",
    "            reply = result.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Erreur de r√©ponse\")\n",
    "            self.last_response = reply\n",
    "            chat_history.append((prompt, reply))\n",
    "            return chat_history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            chat_history.append((prompt, f\"Erreur : {e}\"))\n",
    "            return chat_history\n",
    "\n",
    "    def download_test_cases(self, file_format: str):\n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "\n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                df = pd.DataFrame([line.split(\";\") for line in lines])\n",
    "                df.to_excel(filename, index=False, header=False)\n",
    "\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def send_to_jira(self) -> str:\n",
    "        if not all([self.jira_email, self.jira_url, self.jira_project_key]):\n",
    "            return \"Veuillez remplir les identifiants Jira.\"\n",
    "        jira_client = JiraClient(email=self.jira_email, url=self.jira_url)\n",
    "        summary = \"Cas de test g√©n√©r√© automatiquement\"\n",
    "        return jira_client.create_issue(self.jira_project_key, summary, self.last_response, self.jira_issue_type)\n",
    "\n",
    "def build_ui():\n",
    "    assistant = TestCaseGenerator()\n",
    "\n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"cyan\", secondary_hue=\"indigo\")) as ui:\n",
    "        gr.Markdown(\"\"\"<h1 style='text-align: center;'>üß† G√©n√©rateur de cas de tests IA + Int√©gration Jira</h1>\"\"\")\n",
    "\n",
    "        with gr.Accordion(\"üîê Identifiants Jira\", open=True):\n",
    "            with gr.Row():\n",
    "                jira_email = gr.Textbox(label=\"Email Jira\", placeholder=\"nom@exemple.com\")\n",
    "                jira_url = gr.Textbox(label=\"URL Jira\", placeholder=\"https://ton-espace.atlassian.net\")\n",
    "            with gr.Row():\n",
    "                jira_project = gr.Textbox(label=\"Cl√© du projet Jira\", placeholder=\"ABC\")\n",
    "                jira_type = gr.Dropdown(label=\"Type de t√¢che\", choices=[\"Task\", \"Bug\", \"Story\"], value=\"Task\")\n",
    "                set_jira = gr.Button(\"‚úÖ Valider\")\n",
    "\n",
    "        with gr.Accordion(\"üìÑ Importer un document\", open=True):\n",
    "            with gr.Row():\n",
    "                file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"T√©l√©verser un fichier\")\n",
    "                preview = gr.Textbox(label=\"Aper√ßu du contenu\", lines=12)\n",
    "\n",
    "        with gr.Accordion(\"üí¨ G√©n√©ration de cas de tests\", open=True):\n",
    "            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode IA\")\n",
    "            chatbot = gr.Chatbot(label=\"Conversation IA\", show_copy_button=True)\n",
    "            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question ou demandez un cas de test\")\n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"üöÄ Envoyer\")\n",
    "                clear_btn = gr.Button(\"üßπ R√©initialiser\")\n",
    "\n",
    "        with gr.Accordion(\"üì• Export & Jira\", open=True):\n",
    "            with gr.Row():\n",
    "                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format export\")\n",
    "                download_btn = gr.Button(\"‚¨áÔ∏è T√©l√©charger\")\n",
    "                download_file = gr.File(label=\"Fichier export√©\", interactive=False)\n",
    "            with gr.Row():\n",
    "                jira_btn = gr.Button(\"üìå Cr√©er une t√¢che dans Jira\")\n",
    "                jira_result = gr.Textbox(label=\"R√©sultat Jira\")\n",
    "\n",
    "        set_jira.click(assistant.set_jira_credentials, inputs=[jira_email, jira_url, jira_project, jira_type])\n",
    "        file_input.upload(assistant.process_file, inputs=[file_input], outputs=[preview])\n",
    "        send_btn.click(assistant.generate_test_cases, inputs=[user_input, chatbot, context_mode], outputs=[chatbot])\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_test_cases, inputs=[download_format], outputs=[download_file])\n",
    "        jira_btn.click(assistant.send_to_jira, outputs=[jira_result])\n",
    "\n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7872, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24c6206f-9f5d-45a3-bd5c-327688645ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:41:49,727 - CRITICAL: Erreur de lancement : module 'gradio' has no attribute 'Box'\n",
      "2025-04-10 11:41:51,005 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "# Th√®me personnalis√© pour l'UI\n",
    "custom_theme = gr.themes.Base(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"purple\",\n",
    "    neutral_hue=\"slate\",\n",
    "    radius_size=gr.themes.sizes.radius_md,\n",
    "    font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    ").set(\n",
    "    body_background_fill=\"linear-gradient(to right, #f8f9fa, #f1f3f9)\",\n",
    "    block_background_fill=\"#ffffff\",\n",
    "    block_shadow=\"0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)\",\n",
    "    button_primary_background_fill=\"*primary_500\",\n",
    "    button_primary_background_fill_hover=\"*primary_600\",\n",
    "    button_primary_text_color=\"white\",\n",
    "    button_secondary_background_fill=\"*neutral_100\",\n",
    "    button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "    button_secondary_text_color=\"*neutral_800\",\n",
    "    input_background_fill=\"*neutral_50\",\n",
    "    input_border_color=\"*neutral_200\",\n",
    "    input_border_color_focus=\"*primary_500\",\n",
    "    input_shadow_focus=\"0 0 0 3px rgba(99, 102, 241, 0.2)\",\n",
    ")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Classe pour traiter et extraire du texte √† partir de diff√©rents formats de documents.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donn√© en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "                    \n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "                    \n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "                    \n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    \"\"\"Assistant de chat intelligent bas√© sur l'API Gemini avec support de documents.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "        self.document_name = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> Tuple[str, str]:\n",
    "        \"\"\"Traite le fichier t√©l√©charg√© pour extraire son texte.\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", \"Aucun document charg√©\"\n",
    "            \n",
    "        self.document_name = os.path.basename(file.name)\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        \n",
    "        preview = self.current_document_text[:1000]\n",
    "        if len(self.current_document_text) > 1000:\n",
    "            preview += \"\\n\\n[... Texte tronqu√© pour la pr√©visualisation]\"\n",
    "            \n",
    "        return self.document_name, preview\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "            \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:5000]}\\n\\n\"\n",
    "            \n",
    "            # Construire le contexte avec l'historique\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel √† l'API Gemini\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            # Simulation de chargement\n",
    "            for i in range(3):\n",
    "                yield history + [(message, f\"G√©n√©ration de la r√©ponse{i*'.'}\")], gr.update(interactive=False)\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                error_msg = response_data.get(\"error\", {}).get(\"message\", \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\")\n",
    "                raw_reply = f\"Erreur: {error_msg}\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            \n",
    "            return history, gr.update(interactive=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel √† l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la g√©n√©ration de la r√©ponse : {e}\"))\n",
    "            return history, gr.update(interactive=True)\n",
    "    \n",
    "    def download_response(self, file_format: str) -> Tuple[str, str]:\n",
    "        \"\"\"T√©l√©charge la r√©ponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucune r√©ponse √† t√©l√©charger\"\n",
    "            \n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        file_path = f\"response_{timestamp}.{file_format}\"\n",
    "        \n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                \n",
    "                # Diviser le texte en lignes pour √©viter le d√©passement\n",
    "                lines = self.last_response.split('\\n')\n",
    "                for line in lines:\n",
    "                    # D√©couper les lignes trop longues\n",
    "                    while len(line) > 0:\n",
    "                        chunk = line[:80]  # Environ 80 caract√®res par ligne\n",
    "                        pdf.multi_cell(190, 10, chunk)\n",
    "                        line = line[80:]\n",
    "            \n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"R√©ponse du Document Chat Assistant\", level=1)\n",
    "                doc.add_paragraph(f\"G√©n√©r√© le: {time.strftime('%d/%m/%Y √† %H:%M:%S')}\")\n",
    "                doc.add_paragraph(\"\")\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path, f\"R√©ponse t√©l√©charg√©e au format {file_format.upper()}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la cr√©ation du fichier : {e}\")\n",
    "            return None, f\"Erreur: {str(e)}\"\n",
    "\n",
    "    def reset_chat(self):\n",
    "        \"\"\"R√©initialise la conversation.\"\"\"\n",
    "        return [], \"\"\n",
    "\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    \"\"\"Cr√©e l'interface utilisateur Gradio avec un design moderne.\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    with gr.Blocks(theme=custom_theme, css=\"\"\"\n",
    "        .container { max-width: 1200px; margin: 0 auto; }\n",
    "        .header { text-align: center; margin-bottom: 1.5rem; }\n",
    "        .header h1 { background: linear-gradient(to right, #4f46e5, #7c3aed); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }\n",
    "        .doc-area { background-color: rgba(255, 255, 255, 0.7); border-radius: 0.75rem; padding: 1rem; }\n",
    "        .chat-container { background-color: white; border-radius: 0.75rem; height: 600px; overflow: hidden; display: flex; flex-direction: column; }\n",
    "        .chat-header { padding: 1rem; border-bottom: 1px solid #e5e7eb; }\n",
    "        .info-box { background-color: #eef2ff; border-left: 4px solid #4f46e5; padding: 0.75rem; margin: 0.5rem 0; border-radius: 0.25rem; }\n",
    "        .status-indicator { display: inline-block; width: 8px; height: 8px; border-radius: 50%; margin-right: 6px; }\n",
    "        .status-active { background-color: #10b981; }\n",
    "        .status-inactive { background-color: #6b7280; }\n",
    "        .file-info { display: flex; align-items: center; padding: 0.5rem; background-color: #f3f4f6; border-radius: 0.5rem; margin-top: 0.5rem; }\n",
    "        .file-icon { margin-right: 0.5rem; color: #4b5563; }\n",
    "        .download-area { background-color: #f9fafb; border-radius: 0.5rem; padding: 1rem; margin-top: 1rem; }\n",
    "    \"\"\") as demo:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                # üìÑ Document Chat Assistant\n",
    "                ### Analysez vos documents et obtenez des r√©ponses intelligentes\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Row(elem_classes=\"container\"):\n",
    "            # Colonne de gauche pour le t√©l√©chargement et les param√®tres\n",
    "            with gr.Column(scale=1, min_width=350, elem_classes=\"doc-area\"):\n",
    "                gr.Markdown(\"## üìÇ Documents\", elem_classes=\"text-xl font-semibold\")\n",
    "                \n",
    "                # Zone de t√©l√©chargement\n",
    "                with gr.Box():\n",
    "                    file_upload = gr.File(\n",
    "                        file_types=['.pdf', '.docx', '.txt'], \n",
    "                        label=\"T√©l√©verser un document\",\n",
    "                        file_count=\"single\"\n",
    "                    )\n",
    "                    \n",
    "                # Informations sur le document\n",
    "                with gr.Group():\n",
    "                    file_name = gr.Textbox(label=\"Document actuel\", interactive=False)\n",
    "                    file_preview = gr.Textbox(\n",
    "                        label=\"Aper√ßu du contenu\", \n",
    "                        lines=10,\n",
    "                        placeholder=\"Le contenu extrait du document appara√Ætra ici...\",\n",
    "                        interactive=False\n",
    "                    )\n",
    "                \n",
    "                # Options\n",
    "                with gr.Accordion(\"Options\", open=True):\n",
    "                    context_radio = gr.Radio(\n",
    "                        choices=[\"Standard\", \"Avec contexte du document\"], \n",
    "                        value=\"Standard\", \n",
    "                        label=\"Mode de conversation\",\n",
    "                        info=\"Le mode avec contexte utilise le contenu du document pour am√©liorer les r√©ponses\"\n",
    "                    )\n",
    "                    \n",
    "                # Zone de t√©l√©chargement de r√©ponse\n",
    "                with gr.Box(elem_classes=\"download-area\"):\n",
    "                    gr.Markdown(\"## üíæ Exporter la derni√®re r√©ponse\")\n",
    "                    with gr.Row():\n",
    "                        format_dropdown = gr.Dropdown(\n",
    "                            choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"], \n",
    "                            value=\"txt\", \n",
    "                            label=\"Format\",\n",
    "                            scale=1\n",
    "                        )\n",
    "                        download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\", scale=1)\n",
    "                    \n",
    "                    download_status = gr.Markdown(\"\")\n",
    "                    response_file = gr.File(label=\"Fichier g√©n√©r√©\", visible=False)\n",
    "            \n",
    "            # Colonne de droite pour le chat\n",
    "            with gr.Column(scale=2, min_width=550, elem_classes=\"chat-container\"):\n",
    "                with gr.Row(elem_classes=\"chat-header\"):\n",
    "                    gr.Markdown(\"\"\"\n",
    "                    ## üí¨ Conversation\n",
    "                    <div>\n",
    "                        <span class=\"status-indicator status-active\"></span>\n",
    "                        <span>Intelligence artificielle Gemini 2.0 Flash</span>\n",
    "                    </div>\n",
    "                    \"\"\")\n",
    "                \n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Discussion\", \n",
    "                    elem_id=\"chatbox\",\n",
    "                    height=400,\n",
    "                    bubble_full_width=False,\n",
    "                    show_copy_button=True,\n",
    "                    avatar_images=(\"https://api.dicebear.com/7.x/thumbs/svg?seed=user\", \"https://api.dicebear.com/7.x/bottts/svg?seed=assistant\")\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    msg = gr.Textbox(\n",
    "                        placeholder=\"Posez une question ou demandez de l'aide avec votre document...\",\n",
    "                        label=\"Message\",\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                        scale=10\n",
    "                    )\n",
    "                    submit_btn = gr.Button(\"Envoyer\", variant=\"primary\", scale=1)\n",
    "                \n",
    "                with gr.Row():\n",
    "                    clear_btn = gr.Button(\"üóëÔ∏è Effacer la conversation\", variant=\"secondary\")\n",
    "                    with gr.Accordion(\"Conseils d'utilisation\", open=False):\n",
    "                        gr.Markdown(\"\"\"\n",
    "                        - **Pour de meilleurs r√©sultats**, t√©l√©chargez un document et utilisez le mode \"Avec contexte du document\"\n",
    "                        - Vous pouvez poser des questions sp√©cifiques sur le contenu du document\n",
    "                        - Pour les rapports d√©taill√©s, exportez la r√©ponse au format PDF ou DOCX\n",
    "                        - Pour t√©l√©charger une r√©ponse, cliquez sur le bouton \"T√©l√©charger\"\n",
    "                        \"\"\")\n",
    "        \n",
    "        # √âv√©nements\n",
    "        file_upload.upload(\n",
    "            fn=assistant.process_document, \n",
    "            inputs=[file_upload], \n",
    "            outputs=[file_name, file_preview]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            fn=assistant.generate_response, \n",
    "            inputs=[msg, chatbot, context_radio], \n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=assistant.generate_response, \n",
    "            inputs=[msg, chatbot, context_radio], \n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        download_btn.click(\n",
    "            fn=assistant.download_response, \n",
    "            inputs=[format_dropdown], \n",
    "            outputs=[response_file, download_status]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=assistant.reset_chat, \n",
    "            inputs=[], \n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(server_name=\"0.0.0.0\", server_port=7872, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4205eda1-9f95-48ba-bad0-96df26d7cfed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Base.set() got an unexpected keyword argument 'font'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# üîß Th√®me personnalis√©\u001b[39;00m\n\u001b[32m     79\u001b[39m custom_theme = \u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthemes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprimary_hue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mindigo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43msecondary_hue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpurple\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneutral_hue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mslate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mradius_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthemes\u001b[49m\u001b[43m.\u001b[49m\u001b[43msizes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mradius_md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfont\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInter, ui-sans-serif, system-ui, sans-serif\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlinear-gradient(to right, #f8f9fa, #f1f3f9)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m#ffffff\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_shadow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_primary_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*primary_500\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_primary_background_fill_hover\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*primary_600\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_primary_text_color\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_secondary_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_100\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_secondary_background_fill_hover\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_200\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_secondary_text_color\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_800\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_50\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_border_color\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_200\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_border_color_focus\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*primary_500\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_shadow_focus\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0 0 0 3px rgba(99, 102, 241, 0.2)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# üî∑ Interface Gradio\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m gr.Blocks(theme=custom_theme, title=\u001b[33m\"\u001b[39m\u001b[33mAssistant IA avec documents\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m demo:\n",
      "\u001b[31mTypeError\u001b[39m: Base.set() got an unexpected keyword argument 'font'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "def extract_text_from_file(file):\n",
    "    \"\"\"Extraction du texte √† partir de fichiers PDF, DOCX ou TXT\"\"\"\n",
    "    text = \"\"\n",
    "    if file is None:\n",
    "        return \"Aucun fichier fourni.\"\n",
    "    ext = os.path.splitext(file.name)[-1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        with fitz.open(file.name) as doc:\n",
    "            text = \"\\n\".join(page.get_text() for page in doc)\n",
    "    elif ext == \".docx\":\n",
    "        doc = Document(file.name)\n",
    "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif ext == \".txt\":\n",
    "        text = file.read().decode(\"utf-8\")\n",
    "    else:\n",
    "        text = \"Format de fichier non pris en charge.\"\n",
    "    return text\n",
    "\n",
    "def interaction_avec_ia(message, historique, contenu_document, mode_conversation):\n",
    "    \"\"\"Traitement du message utilisateur selon le mode de conversation\"\"\"\n",
    "    if mode_conversation == \"Avec contexte (document)\":\n",
    "        contexte = contenu_document\n",
    "        reponse = f\"üß† [IA avec document] Contexte: {contexte[:100]}...\\nR√©ponse √†: {message}\"\n",
    "    else:\n",
    "        reponse = f\"üí¨ [IA sans document] R√©ponse √†: {message}\"\n",
    "    \n",
    "    historique.append((message, reponse))\n",
    "    return historique, historique\n",
    "\n",
    "def export_conversation(historique, format_fichier):\n",
    "    \"\"\"Export de la conversation au format demand√©\"\"\"\n",
    "    contenu = \"\\n\".join([f\"üë§ {msg}\\nü§ñ {rep}\" for msg, rep in historique])\n",
    "    if format_fichier == \"TXT\":\n",
    "        return contenu\n",
    "    elif format_fichier == \"PDF\":\n",
    "        import io\n",
    "        from reportlab.pdfgen import canvas\n",
    "        from reportlab.lib.pagesizes import letter\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        p = canvas.Canvas(buffer, pagesize=letter)\n",
    "        y = 750\n",
    "        for ligne in contenu.split(\"\\n\"):\n",
    "            p.drawString(40, y, ligne)\n",
    "            y -= 15\n",
    "            if y < 50:\n",
    "                p.showPage()\n",
    "                y = 750\n",
    "        p.save()\n",
    "        buffer.seek(0)\n",
    "        return (format_fichier, buffer.read())\n",
    "    elif format_fichier == \"DOCX\":\n",
    "        from docx import Document\n",
    "        doc = Document()\n",
    "        doc.add_heading(\"Historique de conversation\", 0)\n",
    "        for msg, rep in historique:\n",
    "            doc.add_paragraph(f\"üë§ {msg}\")\n",
    "            doc.add_paragraph(f\"ü§ñ {rep}\")\n",
    "        buffer = io.BytesIO()\n",
    "        doc.save(buffer)\n",
    "        buffer.seek(0)\n",
    "        return (format_fichier, buffer.read())\n",
    "    elif format_fichier == \"XLSX\":\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(historique, columns=[\"Utilisateur\", \"Assistant IA\"])\n",
    "        buffer = io.BytesIO()\n",
    "        with pd.ExcelWriter(buffer) as writer:\n",
    "            df.to_excel(writer, index=False)\n",
    "        buffer.seek(0)\n",
    "        return (format_fichier, buffer.read())\n",
    "    return None\n",
    "\n",
    "# üîß Th√®me personnalis√©\n",
    "custom_theme = gr.themes.Base(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"purple\",\n",
    "    neutral_hue=\"slate\",\n",
    "    radius_size=gr.themes.sizes.radius_md,\n",
    ").set(\n",
    "    font=\"Inter, ui-sans-serif, system-ui, sans-serif\",\n",
    "    body_background_fill=\"linear-gradient(to right, #f8f9fa, #f1f3f9)\",\n",
    "    block_background_fill=\"#ffffff\",\n",
    "    block_shadow=\"0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)\",\n",
    "    button_primary_background_fill=\"*primary_500\",\n",
    "    button_primary_background_fill_hover=\"*primary_600\",\n",
    "    button_primary_text_color=\"white\",\n",
    "    button_secondary_background_fill=\"*neutral_100\",\n",
    "    button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "    button_secondary_text_color=\"*neutral_800\",\n",
    "    input_background_fill=\"*neutral_50\",\n",
    "    input_border_color=\"*neutral_200\",\n",
    "    input_border_color_focus=\"*primary_500\",\n",
    "    input_shadow_focus=\"0 0 0 3px rgba(99, 102, 241, 0.2)\",\n",
    ")\n",
    "\n",
    "# üî∑ Interface Gradio\n",
    "with gr.Blocks(theme=custom_theme, title=\"Assistant IA avec documents\") as demo:\n",
    "    gr.Markdown(\"## ü§ñ Assistant IA ‚Äì Interagissez avec ou sans documents\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        fichier = gr.File(label=\"üìÑ Charger un fichier\", file_types=[\".pdf\", \".docx\", \".txt\"])\n",
    "        contenu_document = gr.State(\"\")\n",
    "        fichier.change(fn=extract_text_from_file, inputs=fichier, outputs=contenu_document)\n",
    "\n",
    "    mode_conversation = gr.Radio(choices=[\"Sans contexte\", \"Avec contexte (document)\"],\n",
    "                                 value=\"Sans contexte\", label=\"üéõ Mode de conversation\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"üó®Ô∏è Conversation\")\n",
    "    message = gr.Textbox(label=\"üí¨ Votre message\", placeholder=\"Posez une question...\")\n",
    "    historique = gr.State([])\n",
    "\n",
    "    envoyer = gr.Button(\"Envoyer üöÄ\")\n",
    "    envoyer.click(fn=interaction_avec_ia, \n",
    "                  inputs=[message, historique, contenu_document, mode_conversation],\n",
    "                  outputs=[chatbot, historique])\n",
    "    \n",
    "    with gr.Row():\n",
    "        format_export = gr.Dropdown(choices=[\"TXT\", \"PDF\", \"DOCX\", \"XLSX\"], value=\"TXT\", label=\"üíæ Exporter au format\")\n",
    "        bouton_export = gr.Button(\"üì• Exporter\")\n",
    "        fichier_export = gr.File(interactive=False)\n",
    "\n",
    "    bouton_export.click(fn=export_conversation,\n",
    "                        inputs=[historique, format_export],\n",
    "                        outputs=fichier_export)\n",
    "\n",
    "# üöÄ Lancement\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6968baf7-416c-426e-9cbe-51b3a2e9a749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/16.6 MB 2.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.3/16.6 MB 3.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.1/16.6 MB 4.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.1/16.6 MB 3.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.7/16.6 MB 3.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.0/16.6 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.6/16.6 MB 4.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 7.1/16.6 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.6/16.6 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.7/16.6 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.7/16.6 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.0/16.6 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.0/16.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.3/16.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.8/16.6 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.6/16.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.2/16.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.5/16.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.25.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ea1e9e2-f7f0-4aba-ba5a-3c769ba67a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\24985513.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\24985513.py:222: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:27:24,962 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:27:25,982 - INFO: HTTP Request: GET http://localhost:7874/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:27:28,052 - INFO: HTTP Request: HEAD http://localhost:7874/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:27:29,291 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://45fb6e70fc7a342e00.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:27:33,529 - INFO: HTTP Request: HEAD https://45fb6e70fc7a342e00.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://45fb6e70fc7a342e00.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2133, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        block_fn, inputs, state, explicit_call\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1787, in preprocess_data\n",
      "    raise InvalidComponentError(\n",
      "        f\"{block.__class__} Component not a valid input component.\"\n",
      "    )\n",
      "gradio.exceptions.InvalidComponentError: <class 'gradio.layouts.group.Group'> Component not a valid input component.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2133, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        block_fn, inputs, state, explicit_call\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1787, in preprocess_data\n",
      "    raise InvalidComponentError(\n",
      "        f\"{block.__class__} Component not a valid input component.\"\n",
      "    )\n",
      "gradio.exceptions.InvalidComponentError: <class 'gradio.layouts.group.Group'> Component not a valid input component.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donn√© en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier t√©l√©charg√© pour extraire son texte.\"\"\"\n",
    "        if file is None:\n",
    "            return \"Aucun fichier s√©lectionn√©.\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        preview = self.current_document_text[:1000]\n",
    "        if len(self.current_document_text) > 1000:\n",
    "            preview += \"\\n\\n[... Texte tronqu√© pour la pr√©visualisation]\"\n",
    "        return preview\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "            \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:3000]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel √† l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la g√©n√©ration de la r√©ponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"T√©l√©charge la r√©ponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None\n",
    "            \n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la cr√©ation du fichier : {e}\")\n",
    "            return None\n",
    "    \n",
    "    def clear_chat(self):\n",
    "        \"\"\"R√©initialise la conversation.\"\"\"\n",
    "        return []\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">üìÑ Document Chat Assistant</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Analysez vos documents et chattez avec leur contenu</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-file-upload\"></i> Documents\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    file_upload = gr.File(\n",
    "                        file_types=['.pdf', '.docx', '.txt'], \n",
    "                        label=\"Importez votre document\",\n",
    "                        elem_id=\"file-upload\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Accordion(\"Aper√ßu du document\", open=False):\n",
    "                        file_preview = gr.Textbox(\n",
    "                            label=\"\", \n",
    "                            lines=10,\n",
    "                            elem_id=\"file-preview\"\n",
    "                        )\n",
    "                    \n",
    "                    context_type = gr.Radio(\n",
    "                        choices=[\"Standard\", \"Avec contexte du document\"],\n",
    "                        value=\"Avec contexte du document\",\n",
    "                        label=\"Mode de conversation\",\n",
    "                        info=\"Le mode avec contexte utilise le contenu du document pour g√©n√©rer des r√©ponses.\",\n",
    "                        elem_id=\"context-type\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Group(visible=False) as export_group:\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-download\"></i> Exporter la r√©ponse\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        format_dropdown = gr.Dropdown(\n",
    "                            choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"],\n",
    "                            value=\"txt\",\n",
    "                            label=\"Format\",\n",
    "                            elem_id=\"format-dropdown\"\n",
    "                        )\n",
    "                        download_btn = gr.Button(\n",
    "                            \"T√©l√©charger\",\n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"download-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    response_file = gr.File(\n",
    "                        label=\"Fichier g√©n√©r√©\",\n",
    "                        elem_id=\"response-file\"\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-comments\"></i> Conversation\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        bubble_full_width=False,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Posez une question sur votre document...\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "                        export_toggle = gr.Button(\n",
    "                            \"Exporter la r√©ponse\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"export-toggle\"\n",
    "                        )\n",
    "        \n",
    "        # Events\n",
    "        file_upload.upload(\n",
    "            agent.process_document,\n",
    "            inputs=[file_upload],\n",
    "            outputs=[file_preview]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            agent.clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        export_toggle.click(\n",
    "            lambda visibility: gr.update(visible=not visibility),\n",
    "            inputs=[export_group],\n",
    "            outputs=[export_group]\n",
    "        )\n",
    "        \n",
    "        download_btn.click(\n",
    "            agent.download_response,\n",
    "            inputs=[format_dropdown],\n",
    "            outputs=[response_file]\n",
    "        )\n",
    "        \n",
    "        demo.load(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(\n",
    "            server_name=\"0.0.0.0\", \n",
    "            server_port=7874, \n",
    "            share=True,\n",
    "            favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21357984-1022-46f0-952e-a90397b94083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\1341683234.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\1341683234.py:222: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:33:35,649 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:33:36,715 - INFO: HTTP Request: GET http://localhost:7875/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:33:38,772 - INFO: HTTP Request: HEAD http://localhost:7875/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:33:40,027 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://93cffdc8ea4c092930.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:33:42,602 - INFO: HTTP Request: HEAD https://93cffdc8ea4c092930.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://93cffdc8ea4c092930.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2133, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        block_fn, inputs, state, explicit_call\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1787, in preprocess_data\n",
      "    raise InvalidComponentError(\n",
      "        f\"{block.__class__} Component not a valid input component.\"\n",
      "    )\n",
      "gradio.exceptions.InvalidComponentError: <class 'gradio.layouts.group.Group'> Component not a valid input component.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donn√© en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier t√©l√©charg√© pour extraire son texte.\"\"\"\n",
    "        if file is None:\n",
    "            return \"Aucun fichier s√©lectionn√©.\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        preview = self.current_document_text[:1000]\n",
    "        if len(self.current_document_text) > 1000:\n",
    "            preview += \"\\n\\n[... Texte tronqu√© pour la pr√©visualisation]\"\n",
    "        return preview\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "            \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:3000]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel √† l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la g√©n√©ration de la r√©ponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"T√©l√©charge la r√©ponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None\n",
    "            \n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la cr√©ation du fichier : {e}\")\n",
    "            return None\n",
    "    \n",
    "    def clear_chat(self):\n",
    "        \"\"\"R√©initialise la conversation.\"\"\"\n",
    "        return []\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">üìÑ Document Chat Assistant</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Analysez vos documents et chattez avec leur contenu</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-file-upload\"></i> Documents\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    file_upload = gr.File(\n",
    "                        file_types=['.pdf', '.docx', '.txt'], \n",
    "                        label=\"Importez votre document\",\n",
    "                        elem_id=\"file-upload\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Accordion(\"Aper√ßu du document\", open=False):\n",
    "                        file_preview = gr.Textbox(\n",
    "                            label=\"\", \n",
    "                            lines=10,\n",
    "                            elem_id=\"file-preview\"\n",
    "                        )\n",
    "                    \n",
    "                    context_type = gr.Radio(\n",
    "                        choices=[\"Standard\", \"Avec contexte du document\"],\n",
    "                        value=\"Avec contexte du document\",\n",
    "                        label=\"Mode de conversation\",\n",
    "                        info=\"Le mode avec contexte utilise le contenu du document pour g√©n√©rer des r√©ponses.\",\n",
    "                        elem_id=\"context-type\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Group(visible=False) as export_group:\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-download\"></i> Exporter la r√©ponse\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        format_dropdown = gr.Dropdown(\n",
    "                            choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"],\n",
    "                            value=\"txt\",\n",
    "                            label=\"Format\",\n",
    "                            elem_id=\"format-dropdown\"\n",
    "                        )\n",
    "                        download_btn = gr.Button(\n",
    "                            \"T√©l√©charger\",\n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"download-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    response_file = gr.File(\n",
    "                        label=\"Fichier g√©n√©r√©\",\n",
    "                        elem_id=\"response-file\"\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-comments\"></i> Conversation\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        bubble_full_width=False,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Posez une question sur votre document...\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "                        export_toggle = gr.Button(\n",
    "                            \"Exporter la r√©ponse\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"export-toggle\"\n",
    "                        )\n",
    "        \n",
    "        # Events\n",
    "        file_upload.upload(\n",
    "            agent.process_document,\n",
    "            inputs=[file_upload],\n",
    "            outputs=[file_preview]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            agent.clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        export_toggle.click(\n",
    "            lambda visibility: gr.update(visible=not visibility),\n",
    "            inputs=[export_group],\n",
    "            outputs=[export_group]\n",
    "        )\n",
    "        \n",
    "        download_btn.click(\n",
    "            agent.download_response,\n",
    "            inputs=[format_dropdown],\n",
    "            outputs=[response_file]\n",
    "        )\n",
    "        \n",
    "        demo.load(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(\n",
    "            server_name=\"0.0.0.0\", \n",
    "            server_port=7875, \n",
    "            share=True,\n",
    "            favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64e9a081-30c9-431d-b302-c148dfba9a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\3995981476.py:230: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\3995981476.py:230: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:42:55,979 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:42:57,240 - INFO: HTTP Request: GET http://localhost:7876/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:42:59,335 - INFO: HTTP Request: HEAD http://localhost:7876/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:43:00,561 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://27531fd8fb6f9e5f09.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:43:03,255 - INFO: HTTP Request: HEAD https://27531fd8fb6f9e5f09.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://27531fd8fb6f9e5f09.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donn√© en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier t√©l√©charg√© pour extraire son texte.\"\"\"\n",
    "        if file is None:\n",
    "            return \"Aucun fichier s√©lectionn√©.\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        preview = self.current_document_text[:1000]\n",
    "        if len(self.current_document_text) > 1000:\n",
    "            preview += \"\\n\\n[... Texte tronqu√© pour la pr√©visualisation]\"\n",
    "        return preview\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "            \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:3000]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel √† l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la g√©n√©ration de la r√©ponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"T√©l√©charge la r√©ponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None\n",
    "            \n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la cr√©ation du fichier : {e}\")\n",
    "            return None\n",
    "    \n",
    "    def clear_chat(self):\n",
    "        \"\"\"R√©initialise la conversation.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def toggle_export_panel(self, is_visible):\n",
    "        \"\"\"Bascule la visibilit√© du panneau d'exportation.\"\"\"\n",
    "        return gr.update(visible=not is_visible)\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">üìÑ Document Chat Assistant</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Analysez vos documents et chattez avec leur contenu</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        # Variable d'√©tat pour suivre la visibilit√© du panneau d'exportation\n",
    "        export_panel_visible = gr.State(False)\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-file-upload\"></i> Documents\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    file_upload = gr.File(\n",
    "                        file_types=['.pdf', '.docx', '.txt'], \n",
    "                        label=\"Importez votre document\",\n",
    "                        elem_id=\"file-upload\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Accordion(\"Aper√ßu du document\", open=False):\n",
    "                        file_preview = gr.Textbox(\n",
    "                            label=\"\", \n",
    "                            lines=10,\n",
    "                            elem_id=\"file-preview\"\n",
    "                        )\n",
    "                    \n",
    "                    context_type = gr.Radio(\n",
    "                        choices=[\"Standard\", \"Avec contexte du document\"],\n",
    "                        value=\"Avec contexte du document\",\n",
    "                        label=\"Mode de conversation\",\n",
    "                        info=\"Le mode avec contexte utilise le contenu du document pour g√©n√©rer des r√©ponses.\",\n",
    "                        elem_id=\"context-type\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Group(visible=False) as export_group:\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-download\"></i> Exporter la r√©ponse\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        format_dropdown = gr.Dropdown(\n",
    "                            choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"],\n",
    "                            value=\"txt\",\n",
    "                            label=\"Format\",\n",
    "                            elem_id=\"format-dropdown\"\n",
    "                        )\n",
    "                        download_btn = gr.Button(\n",
    "                            \"T√©l√©charger\",\n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"download-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    response_file = gr.File(\n",
    "                        label=\"Fichier g√©n√©r√©\",\n",
    "                        elem_id=\"response-file\",\n",
    "                        type=\"filepath\"\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-comments\"></i> Conversation\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        bubble_full_width=False,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Posez une question sur votre document...\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "                        export_toggle = gr.Button(\n",
    "                            \"Exporter la r√©ponse\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"export-toggle\"\n",
    "                        )\n",
    "        \n",
    "        # Events\n",
    "        file_upload.upload(\n",
    "            agent.process_document,\n",
    "            inputs=[file_upload],\n",
    "            outputs=[file_preview]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        ).then(\n",
    "            lambda: True,\n",
    "            None,\n",
    "            [export_panel_visible]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        ).then(\n",
    "            lambda: True,\n",
    "            None,\n",
    "            [export_panel_visible]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            agent.clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        ).then(\n",
    "            lambda: False,\n",
    "            None,\n",
    "            [export_panel_visible]\n",
    "        )\n",
    "        \n",
    "        export_toggle.click(\n",
    "            lambda x: (not x, gr.update(visible=not x)),\n",
    "            inputs=[export_panel_visible],\n",
    "            outputs=[export_panel_visible, export_group]\n",
    "        )\n",
    "        \n",
    "        download_btn.click(\n",
    "            agent.download_response,\n",
    "            inputs=[format_dropdown],\n",
    "            outputs=[response_file]\n",
    "        )\n",
    "        \n",
    "        # Initialisation\n",
    "        demo.load(\n",
    "            lambda: (False, gr.update(visible=False)),\n",
    "            None,\n",
    "            [export_panel_visible, export_group]\n",
    "        )\n",
    "        \n",
    "        return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(\n",
    "            server_name=\"0.0.0.0\", \n",
    "            server_port=7876, \n",
    "            share=True,\n",
    "            favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d29246c-dc9f-4dbc-81dc-79caecf8e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 14:02:38,915 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-11 14:02:39,332 - INFO: HTTP Request: GET http://localhost:7879/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-11 14:02:41,406 - INFO: HTTP Request: HEAD http://localhost:7879/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-11 14:02:43,351 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://57259245ea3083941f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 14:02:48,733 - INFO: HTTP Request: HEAD https://57259245ea3083941f.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://57259245ea3083941f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 822, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\responses.py\", line 343, in __call__\n",
      "    stat_result = await anyio.to_thread.run_sync(os.stat, self.path)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "OSError: [WinError 123] La syntaxe du nom de fichier, de r√©pertoire ou de volume est incorrecte: 'https://www.svgrepo.com/show/306500/chat.svg'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration de l'API\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class PersonalityProfile:\n",
    "    \"\"\"D√©finit le profil de personnalit√© de l'assistant.\"\"\"\n",
    "    \n",
    "    PERSONALITIES = {\n",
    "        \"amical\": {\n",
    "            \"nom\": \"L√©o\",\n",
    "            \"description\": \"un assistant virtuel amical et enjou√© qui utilise un langage informel et aime glisser des blagues dans ses r√©ponses\",\n",
    "            \"intro\": \"Salut ! Je suis L√©o, ton assistant virtuel. Je suis l√† pour t'aider avec un sourire. Qu'est-ce que je peux faire pour toi aujourd'hui ? üòä\",\n",
    "            \"ton\": \"amical, d√©contract√©, enthousiaste\",\n",
    "            \"√©mojis\": True,\n",
    "            \"expressions\": [\"Super !\", \"G√©nial !\", \"Pas de souci !\", \"Bien s√ªr !\"],\n",
    "            \"avatar\": \"üë®‚Äçüíº\"\n",
    "        },\n",
    "        \"professionnel\": {\n",
    "            \"nom\": \"Sophie\",\n",
    "            \"description\": \"une assistante virtuelle professionnelle et efficace qui s'exprime avec pr√©cision et concision\",\n",
    "            \"intro\": \"Bonjour, je suis Sophie, votre assistante virtuelle. Comment puis-je vous aider aujourd'hui ?\",\n",
    "            \"ton\": \"professionnel, concis, efficace\",\n",
    "            \"√©mojis\": False,\n",
    "            \"expressions\": [\"Certainement.\", \"Bien compris.\", \"Je vous propose...\", \"Voici les informations demand√©es.\"],\n",
    "            \"avatar\": \"üë©‚Äçüíº\"\n",
    "        },\n",
    "        \"expert\": {\n",
    "            \"nom\": \"Dr. Martin\",\n",
    "            \"description\": \"un assistant virtuel expert et analytique qui fournit des r√©ponses d√©taill√©es et document√©es\",\n",
    "            \"intro\": \"Bonjour, je suis le Dr. Martin. Je suis sp√©cialis√© dans l'analyse et la r√©solution de probl√®mes complexes. Comment puis-je vous apporter mon expertise aujourd'hui ?\",\n",
    "            \"ton\": \"expert, analytique, p√©dagogique\",\n",
    "            \"√©mojis\": False,\n",
    "            \"expressions\": [\"D'apr√®s mon analyse...\", \"Selon les donn√©es disponibles...\", \"Je recommande...\", \"Il est important de noter que...\"],\n",
    "            \"avatar\": \"üß†\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        if personality_type not in self.PERSONALITIES:\n",
    "            personality_type = \"amical\"  # Valeur par d√©faut\n",
    "        \n",
    "        self.type = personality_type\n",
    "        self.profile = self.PERSONALITIES[personality_type]\n",
    "        \n",
    "        self.nom = self.profile[\"nom\"]\n",
    "        self.description = self.profile[\"description\"]\n",
    "        self.intro = self.profile[\"intro\"]\n",
    "        self.ton = self.profile[\"ton\"]\n",
    "        self.use_emojis = self.profile[\"√©mojis\"]\n",
    "        self.expressions = self.profile[\"expressions\"]\n",
    "        self.avatar = self.profile[\"avatar\"]\n",
    "    \n",
    "    def get_random_expression(self) -> str:\n",
    "        \"\"\"Renvoie une expression al√©atoire typique de la personnalit√©.\"\"\"\n",
    "        return random.choice(self.expressions)\n",
    "    \n",
    "    def get_personality_prompt(self) -> str:\n",
    "        \"\"\"Renvoie la description de la personnalit√© pour le syst√®me.\"\"\"\n",
    "        return f\"\"\"Tu es {self.nom}, {self.description}. \n",
    "Ton ton est {self.ton}.\n",
    "{\"Tu utilises souvent des √©mojis pour exprimer des √©motions.\" if self.use_emojis else \"Tu √©vites d'utiliser des √©mojis sauf si n√©cessaire.\"}\n",
    "Assure-toi que tes r√©ponses refl√®tent cette personnalit√© de mani√®re coh√©rente.\"\"\"\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"G√®re la m√©moire √† court et long terme du chatbot.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_short_term_memory: int = 10):\n",
    "        self.short_term_memory = []\n",
    "        self.long_term_memory = {}\n",
    "        self.max_short_term_memory = max_short_term_memory\n",
    "    \n",
    "    def add_interaction(self, user_message: str, assistant_response: str):\n",
    "        \"\"\"Ajoute une interaction √† la m√©moire √† court terme.\"\"\"\n",
    "        self.short_term_memory.append({\"user\": user_message, \"assistant\": assistant_response, \"timestamp\": time.time()})\n",
    "        \n",
    "        # Limiter la taille de la m√©moire √† court terme\n",
    "        if len(self.short_term_memory) > self.max_short_term_memory:\n",
    "            self.short_term_memory.pop(0)\n",
    "    \n",
    "    def add_to_long_term_memory(self, key: str, value: str):\n",
    "        \"\"\"Ajoute une information importante √† la m√©moire √† long terme.\"\"\"\n",
    "        self.long_term_memory[key] = {\"value\": value, \"timestamp\": time.time()}\n",
    "    \n",
    "    def get_recent_conversation(self, max_entries: int = 5) -> str:\n",
    "        \"\"\"Renvoie les conversations r√©centes format√©es pour le contexte.\"\"\"\n",
    "        recent = self.short_term_memory[-max_entries:] if max_entries < len(self.short_term_memory) else self.short_term_memory\n",
    "        formatted = \"\"\n",
    "        \n",
    "        for interaction in recent:\n",
    "            formatted += f\"Utilisateur: {interaction['user']}\\n\"\n",
    "            formatted += f\"Assistant: {interaction['assistant']}\\n\\n\"\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def get_relevant_long_term_memory(self, query: str) -> Dict:\n",
    "        \"\"\"Renvoie les informations pertinentes de la m√©moire √† long terme.\"\"\"\n",
    "        # Recherche simple par mots-cl√©s\n",
    "        relevant_info = {}\n",
    "        for key, info in self.long_term_memory.items():\n",
    "            if key.lower() in query.lower():\n",
    "                relevant_info[key] = info[\"value\"]\n",
    "        \n",
    "        return relevant_info\n",
    "\n",
    "class VirtualAssistant:\n",
    "    \"\"\"Assistant virtuel avec personnalit√© utilisant l'API Gemini.\"\"\"\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        self.memory = Memory()\n",
    "        self.session_start = datetime.now()\n",
    "    \n",
    "    def switch_personality(self, personality_type: str) -> str:\n",
    "        \"\"\"Change la personnalit√© de l'assistant et renvoie le message d'introduction.\"\"\"\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        return self.personality.intro\n",
    "    \n",
    "    def generate_system_prompt(self) -> str:\n",
    "        \"\"\"G√©n√®re le prompt syst√®me avec la personnalit√© et les informations contextuelles.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        date_str = current_time.strftime(\"%d/%m/%Y\")\n",
    "        time_str = current_time.strftime(\"%H:%M\")\n",
    "        \n",
    "        system_prompt = f\"\"\"Tu es un assistant virtuel conversationnel.\n",
    "\n",
    "{self.personality.get_personality_prompt()}\n",
    "\n",
    "Informations contextuelles:\n",
    "- Date actuelle: {date_str}\n",
    "- Heure actuelle: {time_str}\n",
    "- Dur√©e de la session en cours: {str(current_time - self.session_start).split('.')[0]}\n",
    "\n",
    "R√©ponds aux questions et demandes de l'utilisateur de mani√®re conversationnelle.\"\"\"\n",
    "        \n",
    "        # Ajouter des informations de la m√©moire √† long terme si n√©cessaire\n",
    "        if self.memory.long_term_memory:\n",
    "            system_prompt += \"\\n\\nInformations sur l'utilisateur que tu connais d√©j√†:\\n\"\n",
    "            for key, value in self.memory.long_term_memory.items():\n",
    "                system_prompt += f\"- {key}: {value}\\n\"\n",
    "        \n",
    "        return system_prompt\n",
    "    \n",
    "    def process_query(self, message: str, history: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Traite une requ√™te utilisateur et g√©n√®re une r√©ponse.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            # Construire le contexte √† partir de l'historique et du prompt syst√®me\n",
    "            system_prompt = self.generate_system_prompt()\n",
    "            conversation_history = self.memory.get_recent_conversation()\n",
    "            \n",
    "            # Construire le prompt complet\n",
    "            prompt = f\"{system_prompt}\\n\\nHistorique r√©cent de la conversation:\\n{conversation_history}\\nUtilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel √† l'API Gemini\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            data = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.95,\n",
    "                    \"topK\": 40,\n",
    "                    \"maxOutputTokens\": 1024\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{API_URL}?key={GEMINI_API_KEY}\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            \n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extraire la r√©ponse\n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", {})\n",
    "                if \"parts\" in raw_reply and raw_reply[\"parts\"]:\n",
    "                    assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"\")\n",
    "                else:\n",
    "                    assistant_reply = \"Je suis d√©sol√©, mais je n'ai pas pu g√©n√©rer une r√©ponse.\"\n",
    "            else:\n",
    "                error_message = response_data.get(\"error\", {}).get(\"message\", \"Erreur inconnue\")\n",
    "                assistant_reply = f\"Je rencontre des difficult√©s techniques. D√©tails: {error_message}\"\n",
    "            \n",
    "            # M√©moriser l'interaction\n",
    "            self.memory.add_interaction(message, assistant_reply)\n",
    "            \n",
    "            # Analyser la requ√™te pour des informations personnelles\n",
    "            self._extract_personal_info(message)\n",
    "            \n",
    "            # Mettre √† jour l'historique pour l'interface\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "            \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement de la requ√™te: {e}\")\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Je suis d√©sol√©, une erreur s'est produite lors du traitement de votre demande. D√©tails techniques: {str(e)}\"})\n",
    "            return history\n",
    "    \n",
    "    def _extract_personal_info(self, message: str):\n",
    "        \"\"\"Extraction basique d'informations personnelles pour la m√©moire √† long terme.\"\"\"\n",
    "        # Exemple tr√®s simple - dans une application r√©elle, on utiliserait NLP plus avanc√©\n",
    "        \n",
    "        # V√©rifier si le message contient une pr√©sentation avec un nom\n",
    "        name_triggers = [\"je m'appelle\", \"mon nom est\", \"je suis\"]\n",
    "        for trigger in name_triggers:\n",
    "            if trigger in message.lower():\n",
    "                parts = message.lower().split(trigger)\n",
    "                if len(parts) > 1:\n",
    "                    potential_name = parts[1].strip().split()[0]\n",
    "                    # Capitaliser le nom suppos√©\n",
    "                    potential_name = potential_name.capitalize()\n",
    "                    self.memory.add_to_long_term_memory(\"nom\", potential_name)\n",
    "                    break\n",
    "        \n",
    "        # Autres extractions possibles (pr√©f√©rences, localisation, etc.)\n",
    "\n",
    "def create_interface() -> gr.Blocks:\n",
    "    \"\"\"Cr√©e l'interface utilisateur pour l'assistant virtuel.\"\"\"\n",
    "    \n",
    "    # Cr√©er l'assistant\n",
    "    assistant = VirtualAssistant()\n",
    "    \n",
    "    # CSS personnalis√©\n",
    "    css = \"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 1000px !important;\n",
    "    }\n",
    "    .chat-message-container {\n",
    "        padding: 15px;\n",
    "        border-radius: 15px;\n",
    "        margin-bottom: 10px;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #f0f7ff;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #e6f7e6;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .personality-btn {\n",
    "        padding: 8px 15px;\n",
    "        border-radius: 20px;\n",
    "        border: none;\n",
    "        margin: 5px;\n",
    "        font-weight: bold;\n",
    "        cursor: pointer;\n",
    "        transition: all 0.3s;\n",
    "    }\n",
    "    .personality-btn:hover {\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    .personality-amical {\n",
    "        background-color: #ffde7d;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-professionnel {\n",
    "        background-color: #7db9ff;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-expert {\n",
    "        background-color: #b69cff;\n",
    "        color: #333;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\"), css=css) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">‚ú® Assistant Virtuel Personnalisable</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Un assistant conversationnel avec diff√©rentes personnalit√©s</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-user-circle\"></i> Personnalit√©\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    personality_intro = gr.Markdown(assistant.personality.intro)\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        amical_btn = gr.Button(\n",
    "                            f\"L√©o {assistant.personality.PERSONALITIES['amical']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-amical\"]\n",
    "                        )\n",
    "                        pro_btn = gr.Button(\n",
    "                            f\"Sophie {assistant.personality.PERSONALITIES['professionnel']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-professionnel\"]\n",
    "                        )\n",
    "                        expert_btn = gr.Button(\n",
    "                            f\"Dr. Martin {assistant.personality.PERSONALITIES['expert']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-expert\"]\n",
    "                        )\n",
    "                \n",
    "                with gr.Accordion(\"√Ä propos de cet assistant\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        Cet assistant virtuel utilise l'API Gemini de Google pour g√©n√©rer des r√©ponses contextuelles.\n",
    "                        \n",
    "                        Chaque personnalit√© a son propre style de communication :\n",
    "                        \n",
    "                        - **L√©o** est amical et d√©contract√©, utilisant un langage informel et des √©mojis\n",
    "                        - **Sophie** est professionnelle et concise, privil√©giant l'efficacit√©\n",
    "                        - **Dr. Martin** est un expert analytique fournissant des r√©ponses d√©taill√©es et document√©es\n",
    "                        \n",
    "                        L'assistant peut se souvenir de certaines informations tout au long de la conversation.\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                with gr.Accordion(\"Capacit√©s et limitations\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        ### Capacit√©s :\n",
    "                        - R√©pondre √† des questions g√©n√©rales\n",
    "                        - Fournir des explications et d√©finitions\n",
    "                        - Proposer des id√©es et suggestions\n",
    "                        - Maintenir une conversation coh√©rente\n",
    "                        - S'adapter √† diff√©rents styles de communication\n",
    "                        \n",
    "                        ### Limitations :\n",
    "                        - Connaissance limit√©e aux √©v√©nements ant√©rieurs √† sa date d'entra√Ænement\n",
    "                        - Ne peut pas acc√©der √† Internet ou ex√©cuter du code\n",
    "                        - Ne peut pas voir ou analyser d'images\n",
    "                        - Peut parfois g√©n√©rer des informations incorrectes\n",
    "                        \"\"\",\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\",\n",
    "                        type=\"messages\",\n",
    "                        avatar_images=[None, assistant.personality.avatar]\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Comment puis-je vous aider aujourd'hui ?\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "        \n",
    "        # √âv√©nements\n",
    "        def clear_chat():\n",
    "            return []\n",
    "        \n",
    "        def switch_personality(personality_type):\n",
    "            intro_message = assistant.switch_personality(personality_type)\n",
    "            return intro_message, [], gr.update(avatar_images=[None, assistant.personality.avatar])\n",
    "        \n",
    "        # Traitement des interactions utilisateur\n",
    "        submit_btn.click(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        # Changement de personnalit√©\n",
    "        amical_btn.click(\n",
    "            lambda: switch_personality(\"amical\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        pro_btn.click(\n",
    "            lambda: switch_personality(\"professionnel\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        expert_btn.click(\n",
    "            lambda: switch_personality(\"expert\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        # Accueil\n",
    "        demo.load(\n",
    "            lambda: assistant.personality.intro,\n",
    "            outputs=[personality_intro]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_interface()\n",
    "        # D√©tection de l'environnement Hugging Face Spaces\n",
    "        if os.getenv(\"SPACE_ID\"):\n",
    "            # Configuration pour Hugging Face Spaces\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\",\n",
    "                share=False\n",
    "            )\n",
    "        else:\n",
    "            # Configuration pour un environnement local\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\", \n",
    "                server_port=7879, \n",
    "                share=True,\n",
    "                favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e614a23b-0e80-4b14-9e07-3a3e0e9fe102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 19:15:35,513 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 19:15:36,313 - INFO: HTTP Request: GET http://localhost:7880/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 19:15:38,386 - INFO: HTTP Request: HEAD http://localhost:7880/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 19:15:40,125 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://d3b1c6fec65a17cf8c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 19:15:46,308 - INFO: HTTP Request: HEAD https://d3b1c6fec65a17cf8c.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d3b1c6fec65a17cf8c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration de l'API\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class PersonalityProfile:\n",
    "    \"\"\"D√©finit le profil de personnalit√© de l'assistant.\"\"\"\n",
    "    \n",
    "    PERSONALITIES = {\n",
    "        \"amical\": {\n",
    "            \"nom\": \"L√©o\",\n",
    "            \"description\": \"un assistant virtuel amical et enjou√© qui utilise un langage informel et aime glisser des blagues dans ses r√©ponses\",\n",
    "            \"intro\": \"Salut ! Je suis L√©o, ton assistant virtuel. Je suis l√† pour t'aider avec un sourire. Qu'est-ce que je peux faire pour toi aujourd'hui ? üòä\",\n",
    "            \"ton\": \"amical, d√©contract√©, enthousiaste\",\n",
    "            \"√©mojis\": True,\n",
    "            \"expressions\": [\"Super !\", \"G√©nial !\", \"Pas de souci !\", \"Bien s√ªr !\"],\n",
    "            \"avatar\": \"üë®‚Äçüíº\"\n",
    "        },\n",
    "        \"professionnel\": {\n",
    "            \"nom\": \"Sophie\",\n",
    "            \"description\": \"une assistante virtuelle professionnelle et efficace qui s'exprime avec pr√©cision et concision\",\n",
    "            \"intro\": \"Bonjour, je suis Sophie, votre assistante virtuelle. Comment puis-je vous aider aujourd'hui ?\",\n",
    "            \"ton\": \"professionnel, concis, efficace\",\n",
    "            \"√©mojis\": False,\n",
    "            \"expressions\": [\"Certainement.\", \"Bien compris.\", \"Je vous propose...\", \"Voici les informations demand√©es.\"],\n",
    "            \"avatar\": \"üë©‚Äçüíº\"\n",
    "        },\n",
    "        \"expert\": {\n",
    "            \"nom\": \"Dr. Martin\",\n",
    "            \"description\": \"un assistant virtuel expert et analytique qui fournit des r√©ponses d√©taill√©es et document√©es\",\n",
    "            \"intro\": \"Bonjour, je suis le Dr. Martin. Je suis sp√©cialis√© dans l'analyse et la r√©solution de probl√®mes complexes. Comment puis-je vous apporter mon expertise aujourd'hui ?\",\n",
    "            \"ton\": \"expert, analytique, p√©dagogique\",\n",
    "            \"√©mojis\": False,\n",
    "            \"expressions\": [\"D'apr√®s mon analyse...\", \"Selon les donn√©es disponibles...\", \"Je recommande...\", \"Il est important de noter que...\"],\n",
    "            \"avatar\": \"üß†\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        if personality_type not in self.PERSONALITIES:\n",
    "            personality_type = \"amical\"  # Valeur par d√©faut\n",
    "        \n",
    "        self.type = personality_type\n",
    "        self.profile = self.PERSONALITIES[personality_type]\n",
    "        \n",
    "        self.nom = self.profile[\"nom\"]\n",
    "        self.description = self.profile[\"description\"]\n",
    "        self.intro = self.profile[\"intro\"]\n",
    "        self.ton = self.profile[\"ton\"]\n",
    "        self.use_emojis = self.profile[\"√©mojis\"]\n",
    "        self.expressions = self.profile[\"expressions\"]\n",
    "        self.avatar = self.profile[\"avatar\"]\n",
    "    \n",
    "    def get_random_expression(self) -> str:\n",
    "        \"\"\"Renvoie une expression al√©atoire typique de la personnalit√©.\"\"\"\n",
    "        return random.choice(self.expressions)\n",
    "    \n",
    "    def get_personality_prompt(self) -> str:\n",
    "        \"\"\"Renvoie la description de la personnalit√© pour le syst√®me.\"\"\"\n",
    "        return f\"\"\"Tu es {self.nom}, {self.description}. \n",
    "Ton ton est {self.ton}.\n",
    "{\"Tu utilises souvent des √©mojis pour exprimer des √©motions.\" if self.use_emojis else \"Tu √©vites d'utiliser des √©mojis sauf si n√©cessaire.\"}\n",
    "Assure-toi que tes r√©ponses refl√®tent cette personnalit√© de mani√®re coh√©rente.\"\"\"\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"G√®re la m√©moire √† court et long terme du chatbot.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_short_term_memory: int = 10):\n",
    "        self.short_term_memory = []\n",
    "        self.long_term_memory = {}\n",
    "        self.max_short_term_memory = max_short_term_memory\n",
    "    \n",
    "    def add_interaction(self, user_message: str, assistant_response: str):\n",
    "        \"\"\"Ajoute une interaction √† la m√©moire √† court terme.\"\"\"\n",
    "        self.short_term_memory.append({\"user\": user_message, \"assistant\": assistant_response, \"timestamp\": time.time()})\n",
    "        \n",
    "        # Limiter la taille de la m√©moire √† court terme\n",
    "        if len(self.short_term_memory) > self.max_short_term_memory:\n",
    "            self.short_term_memory.pop(0)\n",
    "    \n",
    "    def add_to_long_term_memory(self, key: str, value: str):\n",
    "        \"\"\"Ajoute une information importante √† la m√©moire √† long terme.\"\"\"\n",
    "        self.long_term_memory[key] = {\"value\": value, \"timestamp\": time.time()}\n",
    "    \n",
    "    def get_recent_conversation(self, max_entries: int = 5) -> str:\n",
    "        \"\"\"Renvoie les conversations r√©centes format√©es pour le contexte.\"\"\"\n",
    "        recent = self.short_term_memory[-max_entries:] if max_entries < len(self.short_term_memory) else self.short_term_memory\n",
    "        formatted = \"\"\n",
    "        \n",
    "        for interaction in recent:\n",
    "            formatted += f\"Utilisateur: {interaction['user']}\\n\"\n",
    "            formatted += f\"Assistant: {interaction['assistant']}\\n\\n\"\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def get_relevant_long_term_memory(self, query: str) -> Dict:\n",
    "        \"\"\"Renvoie les informations pertinentes de la m√©moire √† long terme.\"\"\"\n",
    "        # Recherche simple par mots-cl√©s\n",
    "        relevant_info = {}\n",
    "        for key, info in self.long_term_memory.items():\n",
    "            if key.lower() in query.lower():\n",
    "                relevant_info[key] = info[\"value\"]\n",
    "        \n",
    "        return relevant_info\n",
    "\n",
    "class VirtualAssistant:\n",
    "    \"\"\"Assistant virtuel avec personnalit√© utilisant l'API Gemini.\"\"\"\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        self.memory = Memory()\n",
    "        self.session_start = datetime.now()\n",
    "    \n",
    "    def switch_personality(self, personality_type: str) -> str:\n",
    "        \"\"\"Change la personnalit√© de l'assistant et renvoie le message d'introduction.\"\"\"\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        return self.personality.intro\n",
    "    \n",
    "    def generate_system_prompt(self) -> str:\n",
    "        \"\"\"G√©n√®re le prompt syst√®me avec la personnalit√© et les informations contextuelles.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        date_str = current_time.strftime(\"%d/%m/%Y\")\n",
    "        time_str = current_time.strftime(\"%H:%M\")\n",
    "        \n",
    "        system_prompt = f\"\"\"Tu es un assistant virtuel conversationnel.\n",
    "\n",
    "{self.personality.get_personality_prompt()}\n",
    "\n",
    "Informations contextuelles:\n",
    "- Date actuelle: {date_str}\n",
    "- Heure actuelle: {time_str}\n",
    "- Dur√©e de la session en cours: {str(current_time - self.session_start).split('.')[0]}\n",
    "\n",
    "R√©ponds aux questions et demandes de l'utilisateur de mani√®re conversationnelle.\"\"\"\n",
    "        \n",
    "        # Ajouter des informations de la m√©moire √† long terme si n√©cessaire\n",
    "        if self.memory.long_term_memory:\n",
    "            system_prompt += \"\\n\\nInformations sur l'utilisateur que tu connais d√©j√†:\\n\"\n",
    "            for key, value in self.memory.long_term_memory.items():\n",
    "                system_prompt += f\"- {key}: {value}\\n\"\n",
    "        \n",
    "        return system_prompt\n",
    "    \n",
    "    def process_query(self, message: str, history: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Traite une requ√™te utilisateur et g√©n√®re une r√©ponse.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            # Construire le contexte √† partir de l'historique et du prompt syst√®me\n",
    "            system_prompt = self.generate_system_prompt()\n",
    "            conversation_history = self.memory.get_recent_conversation()\n",
    "            \n",
    "            # Construire le prompt complet\n",
    "            prompt = f\"{system_prompt}\\n\\nHistorique r√©cent de la conversation:\\n{conversation_history}\\nUtilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel √† l'API Gemini\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            data = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.95,\n",
    "                    \"topK\": 40,\n",
    "                    \"maxOutputTokens\": 1024\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{API_URL}?key={GEMINI_API_KEY}\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            \n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extraire la r√©ponse\n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", {})\n",
    "                if \"parts\" in raw_reply and raw_reply[\"parts\"]:\n",
    "                    assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"\")\n",
    "                else:\n",
    "                    assistant_reply = \"Je suis d√©sol√©, mais je n'ai pas pu g√©n√©rer une r√©ponse.\"\n",
    "            else:\n",
    "                error_message = response_data.get(\"error\", {}).get(\"message\", \"Erreur inconnue\")\n",
    "                assistant_reply = f\"Je rencontre des difficult√©s techniques. D√©tails: {error_message}\"\n",
    "            \n",
    "            # M√©moriser l'interaction\n",
    "            self.memory.add_interaction(message, assistant_reply)\n",
    "            \n",
    "            # Analyser la requ√™te pour des informations personnelles\n",
    "            self._extract_personal_info(message)\n",
    "            \n",
    "            # Mettre √† jour l'historique pour l'interface\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "            \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement de la requ√™te: {e}\")\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Je suis d√©sol√©, une erreur s'est produite lors du traitement de votre demande. D√©tails techniques: {str(e)}\"})\n",
    "            return history\n",
    "    \n",
    "    def _extract_personal_info(self, message: str):\n",
    "        \"\"\"Extraction basique d'informations personnelles pour la m√©moire √† long terme.\"\"\"\n",
    "        # Exemple tr√®s simple - dans une application r√©elle, on utiliserait NLP plus avanc√©\n",
    "        \n",
    "        # V√©rifier si le message contient une pr√©sentation avec un nom\n",
    "        name_triggers = [\"je m'appelle\", \"mon nom est\", \"je suis\"]\n",
    "        for trigger in name_triggers:\n",
    "            if trigger in message.lower():\n",
    "                parts = message.lower().split(trigger)\n",
    "                if len(parts) > 1:\n",
    "                    potential_name = parts[1].strip().split()[0]\n",
    "                    # Capitaliser le nom suppos√©\n",
    "                    potential_name = potential_name.capitalize()\n",
    "                    self.memory.add_to_long_term_memory(\"nom\", potential_name)\n",
    "                    break\n",
    "        \n",
    "        # Autres extractions possibles (pr√©f√©rences, localisation, etc.)\n",
    "\n",
    "def create_interface() -> gr.Blocks:\n",
    "    \"\"\"Cr√©e l'interface utilisateur pour l'assistant virtuel.\"\"\"\n",
    "    \n",
    "    # Cr√©er l'assistant\n",
    "    assistant = VirtualAssistant()\n",
    "    \n",
    "    # CSS personnalis√©\n",
    "    css = \"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 1000px !important;\n",
    "    }\n",
    "    .chat-message-container {\n",
    "        padding: 15px;\n",
    "        border-radius: 15px;\n",
    "        margin-bottom: 10px;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #f0f7ff;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #e6f7e6;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .personality-btn {\n",
    "        padding: 8px 15px;\n",
    "        border-radius: 20px;\n",
    "        border: none;\n",
    "        margin: 5px;\n",
    "        font-weight: bold;\n",
    "        cursor: pointer;\n",
    "        transition: all 0.3s;\n",
    "    }\n",
    "    .personality-btn:hover {\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    .personality-amical {\n",
    "        background-color: #ffde7d;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-professionnel {\n",
    "        background-color: #7db9ff;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-expert {\n",
    "        background-color: #b69cff;\n",
    "        color: #333;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\"), css=css) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">‚ú® Assistant Virtuel Personnalisable</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Un assistant conversationnel avec diff√©rentes personnalit√©s</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-user-circle\"></i> Personnalit√©\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    personality_intro = gr.Markdown(assistant.personality.intro)\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        amical_btn = gr.Button(\n",
    "                            f\"L√©o {assistant.personality.PERSONALITIES['amical']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-amical\"]\n",
    "                        )\n",
    "                        pro_btn = gr.Button(\n",
    "                            f\"Sophie {assistant.personality.PERSONALITIES['professionnel']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-professionnel\"]\n",
    "                        )\n",
    "                        expert_btn = gr.Button(\n",
    "                            f\"Dr. Martin {assistant.personality.PERSONALITIES['expert']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-expert\"]\n",
    "                        )\n",
    "                \n",
    "                with gr.Accordion(\"√Ä propos de cet assistant\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        Cet assistant virtuel utilise l'API Gemini de Google pour g√©n√©rer des r√©ponses contextuelles.\n",
    "                        \n",
    "                        Chaque personnalit√© a son propre style de communication :\n",
    "                        \n",
    "                        - **L√©o** est amical et d√©contract√©, utilisant un langage informel et des √©mojis\n",
    "                        - **Sophie** est professionnelle et concise, privil√©giant l'efficacit√©\n",
    "                        - **Dr. Martin** est un expert analytique fournissant des r√©ponses d√©taill√©es et document√©es\n",
    "                        \n",
    "                        L'assistant peut se souvenir de certaines informations tout au long de la conversation.\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                with gr.Accordion(\"Capacit√©s et limitations\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        ### Capacit√©s :\n",
    "                        - R√©pondre √† des questions g√©n√©rales\n",
    "                        - Fournir des explications et d√©finitions\n",
    "                        - Proposer des id√©es et suggestions\n",
    "                        - Maintenir une conversation coh√©rente\n",
    "                        - S'adapter √† diff√©rents styles de communication\n",
    "                        \n",
    "                        ### Limitations :\n",
    "                        - Connaissance limit√©e aux √©v√©nements ant√©rieurs √† sa date d'entra√Ænement\n",
    "                        - Ne peut pas acc√©der √† Internet ou ex√©cuter du code\n",
    "                        - Ne peut pas voir ou analyser d'images\n",
    "                        - Peut parfois g√©n√©rer des informations incorrectes\n",
    "                        \"\"\",\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\",\n",
    "                        type=\"messages\",\n",
    "                        avatar_images=[None, assistant.personality.avatar]\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Comment puis-je vous aider aujourd'hui ?\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "        \n",
    "        # √âv√©nements\n",
    "        def clear_chat():\n",
    "            return []\n",
    "        \n",
    "        def switch_personality(personality_type):\n",
    "            intro_message = assistant.switch_personality(personality_type)\n",
    "            return intro_message, [], gr.update(avatar_images=[None, assistant.personality.avatar])\n",
    "        \n",
    "        # Traitement des interactions utilisateur\n",
    "        submit_btn.click(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        # Changement de personnalit√©\n",
    "        amical_btn.click(\n",
    "            lambda: switch_personality(\"amical\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        pro_btn.click(\n",
    "            lambda: switch_personality(\"professionnel\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        expert_btn.click(\n",
    "            lambda: switch_personality(\"expert\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        # Accueil\n",
    "        demo.load(\n",
    "            lambda: assistant.personality.intro,\n",
    "            outputs=[personality_intro]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_interface()\n",
    "        # D√©tection de l'environnement Hugging Face Spaces\n",
    "        if os.getenv(\"SPACE_ID\"):\n",
    "            # Configuration pour Hugging Face Spaces\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\",\n",
    "                share=False\n",
    "            )\n",
    "        else:\n",
    "            # Configuration pour un environnement local\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\", \n",
    "                server_port=7880, \n",
    "                share=True,\n",
    "                favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3266f8fa-d91e-47d2-b1d8-51ed81eebedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11760\\1311349699.py:278: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:11:58,582 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:11:59,679 - INFO: HTTP Request: GET http://localhost:7888/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:12:01,721 - INFO: HTTP Request: HEAD http://localhost:7888/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:12:04,015 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://1cc489c3559dd699d1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:12:08,003 - INFO: HTTP Request: HEAD https://1cc489c3559dd699d1.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1cc489c3559dd699d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Pr√©parer l'aper√ßu (limit√©)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqu√©]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"üìÑ Fichier : {self.document_name} | üìä {char_count} caract√®res\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse bas√©e sur le prompt et l'historique de conversation\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqu√© pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel √† l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            if \"candidates\" in result and result[\"candidates\"]:\n",
    "                raw_reply = result[\"candidates\"][0].get(\"content\", {})\n",
    "                if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                    reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "                else:\n",
    "                    reply = \"Erreur: Format de r√©ponse inattendu\"\n",
    "            else:\n",
    "                reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            self.last_response = reply\n",
    "            history.append((message, reply))\n",
    "            \n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append((message, f\"Erreur : {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"G√©n√®re un fichier t√©l√©chargeable au format sp√©cifi√©\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger. G√©n√©rez d'abord une r√©ponse.\"\n",
    "        \n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "            \n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"reponse_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traitement du texte pour FPDF (√©viter les probl√®mes d'encodage)\n",
    "                clean_text = self.last_response.encode('latin-1', 'replace').decode('latin-1')\n",
    "                pdf.multi_cell(190, 7, clean_text)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"R√©ponse g√©n√©r√©e\", level=1)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter une conversion en tableau simple\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Cr√©er des colonnes par d√©faut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_excel(filename, index=False)\n",
    "            elif file_format == \"csv\":\n",
    "                # Tenter une conversion en CSV\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Cr√©er des colonnes par d√©faut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_csv(filename, index=False)\n",
    "            \n",
    "            return (filename, display_name), \"‚úÖ Fichier g√©n√©r√© avec succ√®s\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"‚ùå Erreur lors de la g√©n√©ration du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Th√®me personnalis√© - inspir√© de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"T√©l√©versez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document charg√©\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de g√©n√©ration\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"T√©l√©charger le fichier g√©n√©r√©\", \n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents gr√¢ce √† l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **T√©l√©chargez un document**\n",
    "                   - Formats support√©s : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte √† l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme r√©f√©rence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez pr√©cis dans vos requ√™tes\n",
    "                   - Exemple : \"Peux-tu r√©sumer ce document ?\" ou \"Quels sont les points principaux abord√©s ?\"\n",
    "                \n",
    "                4. **Exportez le r√©sultat**\n",
    "                   - T√©l√©chargez les r√©ponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requ√™tes efficaces :\n",
    "                \n",
    "                - \"R√©sume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations cl√©s pr√©sent√©es dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des √©v√©nements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7888, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36fffcf4-845d-464f-8499-5a90f255d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11760\\3655890259.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:23:29,072 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:23:30,213 - INFO: HTTP Request: GET http://localhost:7889/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:23:32,283 - INFO: HTTP Request: HEAD http://localhost:7889/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:23:34,499 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://5e98bf49590ae0b23e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:23:38,697 - INFO: HTTP Request: HEAD https://5e98bf49590ae0b23e.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5e98bf49590ae0b23e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:24:53,143 - INFO: Fichier cr√©√© avec succ√®s: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\reponse_20250414_082453.docx\n",
      "2025-04-14 08:25:49,547 - INFO: Fichier cr√©√© avec succ√®s: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\reponse_20250414_082549.xlsx\n",
      "2025-04-14 08:26:28,296 - INFO: Fichier cr√©√© avec succ√®s: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\reponse_20250414_082628.pdf\n",
      "2025-04-14 08:26:50,763 - INFO: Fichier cr√©√© avec succ√®s: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\reponse_20250414_082650.txt\n"
     ]
    }
   ],
   "source": [
    " file_format == \"csv\":\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8-sig')  # BOM pour Excel\n",
    "            \n",
    "            # V√©rifier que le fichier existe bien\n",
    "            if not os.path.exists(filepath):\n",
    "                logger.error(f\"Le fichier '{filepath}' n'a pas √©t√© cr√©√©\")\n",
    "                return None, \"‚ùå Erreur: Le fichier n'a pas √©t√© cr√©√© correctement\"\n",
    "            \n",
    "            logger.info(f\"Fichier cr√©√© avec succ√®s: {filepath}\")\n",
    "            return filepath, \"‚úÖ Fichier g√©n√©r√© avec succ√®s\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\", exc_info=True)\n",
    "            return None, f\"‚ùå Erreur lors de la g√©n√©ration du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Th√®me personnalis√© - inspir√© de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"T√©l√©versez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document charg√©\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de g√©n√©ration\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"T√©l√©charger le fichier g√©n√©r√©\", \n",
    "                                    interactive=False,\n",
    "                                    type=\"filepath\"  # Important: utiliser filepath pour le t√©l√©chargement\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents gr√¢ce √† l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **T√©l√©chargez un document**\n",
    "                   - Formats support√©s : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte √† l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme r√©f√©rence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez pr√©cis dans vos requ√™tes\n",
    "                   - Exemple : \"Peux-tu r√©sumer ce document ?\" ou \"Quels sont les points principaux abord√©s ?\"\n",
    "                \n",
    "                4. **Exportez le r√©sultat**\n",
    "                   - T√©l√©chargez les r√©ponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requ√™tes efficaces :\n",
    "                \n",
    "                - \"R√©sume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations cl√©s pr√©sent√©es dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des √©v√©nements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7889, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f637d38-8681-4b49-8df6-67f61f9ea4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 13:59:04,605 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 13:59:05,222 - INFO: HTTP Request: GET http://localhost:7879/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 13:59:07,287 - INFO: HTTP Request: HEAD http://localhost:7879/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 13:59:08,832 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://b2c433ed1af1d3f4f6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 13:59:13,173 - INFO: HTTP Request: HEAD https://b2c433ed1af1d3f4f6.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b2c433ed1af1d3f4f6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration de l'API\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class PersonalityProfile:\n",
    "    \"\"\"D√©finit le profil de personnalit√© de l'assistant.\"\"\"\n",
    "    \n",
    "    PERSONALITIES = {\n",
    "        \"amical\": {\n",
    "            \"nom\": \"L√©o\",\n",
    "            \"description\": \"un assistant virtuel amical et enjou√© qui utilise un langage informel et aime glisser des blagues dans ses r√©ponses\",\n",
    "            \"intro\": \"Salut ! Je suis L√©o, ton assistant virtuel. Je suis l√† pour t'aider avec un sourire. Qu'est-ce que je peux faire pour toi aujourd'hui ? üòä\",\n",
    "            \"ton\": \"amical, d√©contract√©, enthousiaste\",\n",
    "            \"√©mojis\": True,\n",
    "            \"expressions\": [\"Super !\", \"G√©nial !\", \"Pas de souci !\", \"Bien s√ªr !\"],\n",
    "            \"avatar\": \"üë®‚Äçüíº\"\n",
    "        },\n",
    "        \"professionnel\": {\n",
    "            \"nom\": \"Sophie\",\n",
    "            \"description\": \"une assistante virtuelle professionnelle et efficace qui s'exprime avec pr√©cision et concision\",\n",
    "            \"intro\": \"Bonjour, je suis Sophie, votre assistante virtuelle. Comment puis-je vous aider aujourd'hui ?\",\n",
    "            \"ton\": \"professionnel, concis, efficace\",\n",
    "            \"√©mojis\": False,\n",
    "            \"expressions\": [\"Certainement.\", \"Bien compris.\", \"Je vous propose...\", \"Voici les informations demand√©es.\"],\n",
    "            \"avatar\": \"üë©‚Äçüíº\"\n",
    "        },\n",
    "        \"expert\": {\n",
    "            \"nom\": \"Dr. Martin\",\n",
    "            \"description\": \"un assistant virtuel expert et analytique qui fournit des r√©ponses d√©taill√©es et document√©es\",\n",
    "            \"intro\": \"Bonjour, je suis le Dr. Martin. Je suis sp√©cialis√© dans l'analyse et la r√©solution de probl√®mes complexes. Comment puis-je vous apporter mon expertise aujourd'hui ?\",\n",
    "            \"ton\": \"expert, analytique, p√©dagogique\",\n",
    "            \"√©mojis\": False,\n",
    "            \"expressions\": [\"D'apr√®s mon analyse...\", \"Selon les donn√©es disponibles...\", \"Je recommande...\", \"Il est important de noter que...\"],\n",
    "            \"avatar\": \"üß†\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        if personality_type not in self.PERSONALITIES:\n",
    "            personality_type = \"amical\"  # Valeur par d√©faut\n",
    "        \n",
    "        self.type = personality_type\n",
    "        self.profile = self.PERSONALITIES[personality_type]\n",
    "        \n",
    "        self.nom = self.profile[\"nom\"]\n",
    "        self.description = self.profile[\"description\"]\n",
    "        self.intro = self.profile[\"intro\"]\n",
    "        self.ton = self.profile[\"ton\"]\n",
    "        self.use_emojis = self.profile[\"√©mojis\"]\n",
    "        self.expressions = self.profile[\"expressions\"]\n",
    "        self.avatar = self.profile[\"avatar\"]\n",
    "    \n",
    "    def get_random_expression(self) -> str:\n",
    "        \"\"\"Renvoie une expression al√©atoire typique de la personnalit√©.\"\"\"\n",
    "        return random.choice(self.expressions)\n",
    "    \n",
    "    def get_personality_prompt(self) -> str:\n",
    "        \"\"\"Renvoie la description de la personnalit√© pour le syst√®me.\"\"\"\n",
    "        return f\"\"\"Tu es {self.nom}, {self.description}. \n",
    "Ton ton est {self.ton}.\n",
    "{\"Tu utilises souvent des √©mojis pour exprimer des √©motions.\" if self.use_emojis else \"Tu √©vites d'utiliser des √©mojis sauf si n√©cessaire.\"}\n",
    "Assure-toi que tes r√©ponses refl√®tent cette personnalit√© de mani√®re coh√©rente.\"\"\"\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"G√®re la m√©moire √† court et long terme du chatbot.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_short_term_memory: int = 10):\n",
    "        self.short_term_memory = []\n",
    "        self.long_term_memory = {}\n",
    "        self.max_short_term_memory = max_short_term_memory\n",
    "    \n",
    "    def add_interaction(self, user_message: str, assistant_response: str):\n",
    "        \"\"\"Ajoute une interaction √† la m√©moire √† court terme.\"\"\"\n",
    "        self.short_term_memory.append({\"user\": user_message, \"assistant\": assistant_response, \"timestamp\": time.time()})\n",
    "        \n",
    "        # Limiter la taille de la m√©moire √† court terme\n",
    "        if len(self.short_term_memory) > self.max_short_term_memory:\n",
    "            self.short_term_memory.pop(0)\n",
    "    \n",
    "    def add_to_long_term_memory(self, key: str, value: str):\n",
    "        \"\"\"Ajoute une information importante √† la m√©moire √† long terme.\"\"\"\n",
    "        self.long_term_memory[key] = {\"value\": value, \"timestamp\": time.time()}\n",
    "    \n",
    "    def get_recent_conversation(self, max_entries: int = 5) -> str:\n",
    "        \"\"\"Renvoie les conversations r√©centes format√©es pour le contexte.\"\"\"\n",
    "        recent = self.short_term_memory[-max_entries:] if max_entries < len(self.short_term_memory) else self.short_term_memory\n",
    "        formatted = \"\"\n",
    "        \n",
    "        for interaction in recent:\n",
    "            formatted += f\"Utilisateur: {interaction['user']}\\n\"\n",
    "            formatted += f\"Assistant: {interaction['assistant']}\\n\\n\"\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def get_relevant_long_term_memory(self, query: str) -> Dict:\n",
    "        \"\"\"Renvoie les informations pertinentes de la m√©moire √† long terme.\"\"\"\n",
    "        # Recherche simple par mots-cl√©s\n",
    "        relevant_info = {}\n",
    "        for key, info in self.long_term_memory.items():\n",
    "            if key.lower() in query.lower():\n",
    "                relevant_info[key] = info[\"value\"]\n",
    "        \n",
    "        return relevant_info\n",
    "\n",
    "class VirtualAssistant:\n",
    "    \"\"\"Assistant virtuel avec personnalit√© utilisant l'API Gemini.\"\"\"\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        self.memory = Memory()\n",
    "        self.session_start = datetime.now()\n",
    "    \n",
    "    def switch_personality(self, personality_type: str) -> str:\n",
    "        \"\"\"Change la personnalit√© de l'assistant et renvoie le message d'introduction.\"\"\"\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        return self.personality.intro\n",
    "    \n",
    "    def generate_system_prompt(self) -> str:\n",
    "        \"\"\"G√©n√®re le prompt syst√®me avec la personnalit√© et les informations contextuelles.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        date_str = current_time.strftime(\"%d/%m/%Y\")\n",
    "        time_str = current_time.strftime(\"%H:%M\")\n",
    "        \n",
    "        system_prompt = f\"\"\"Tu es un assistant virtuel conversationnel.\n",
    "\n",
    "{self.personality.get_personality_prompt()}\n",
    "\n",
    "Informations contextuelles:\n",
    "- Date actuelle: {date_str}\n",
    "- Heure actuelle: {time_str}\n",
    "- Dur√©e de la session en cours: {str(current_time - self.session_start).split('.')[0]}\n",
    "\n",
    "R√©ponds aux questions et demandes de l'utilisateur de mani√®re conversationnelle.\"\"\"\n",
    "        \n",
    "        # Ajouter des informations de la m√©moire √† long terme si n√©cessaire\n",
    "        if self.memory.long_term_memory:\n",
    "            system_prompt += \"\\n\\nInformations sur l'utilisateur que tu connais d√©j√†:\\n\"\n",
    "            for key, value in self.memory.long_term_memory.items():\n",
    "                system_prompt += f\"- {key}: {value}\\n\"\n",
    "        \n",
    "        return system_prompt\n",
    "    \n",
    "    def process_query(self, message: str, history: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Traite une requ√™te utilisateur et g√©n√®re une r√©ponse.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            # Construire le contexte √† partir de l'historique et du prompt syst√®me\n",
    "            system_prompt = self.generate_system_prompt()\n",
    "            conversation_history = self.memory.get_recent_conversation()\n",
    "            \n",
    "            # Construire le prompt complet\n",
    "            prompt = f\"{system_prompt}\\n\\nHistorique r√©cent de la conversation:\\n{conversation_history}\\nUtilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel √† l'API Gemini\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            data = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.95,\n",
    "                    \"topK\": 40,\n",
    "                    \"maxOutputTokens\": 1024\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{API_URL}?key={GEMINI_API_KEY}\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            \n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extraire la r√©ponse\n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", {})\n",
    "                if \"parts\" in raw_reply and raw_reply[\"parts\"]:\n",
    "                    assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"\")\n",
    "                else:\n",
    "                    assistant_reply = \"Je suis d√©sol√©, mais je n'ai pas pu g√©n√©rer une r√©ponse.\"\n",
    "            else:\n",
    "                error_message = response_data.get(\"error\", {}).get(\"message\", \"Erreur inconnue\")\n",
    "                assistant_reply = f\"Je rencontre des difficult√©s techniques. D√©tails: {error_message}\"\n",
    "            \n",
    "            # M√©moriser l'interaction\n",
    "            self.memory.add_interaction(message, assistant_reply)\n",
    "            \n",
    "            # Analyser la requ√™te pour des informations personnelles\n",
    "            self._extract_personal_info(message)\n",
    "            \n",
    "            # Mettre √† jour l'historique pour l'interface\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "            \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement de la requ√™te: {e}\")\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Je suis d√©sol√©, une erreur s'est produite lors du traitement de votre demande. D√©tails techniques: {str(e)}\"})\n",
    "            return history\n",
    "    \n",
    "    def _extract_personal_info(self, message: str):\n",
    "        \"\"\"Extraction basique d'informations personnelles pour la m√©moire √† long terme.\"\"\"\n",
    "        # Exemple tr√®s simple - dans une application r√©elle, on utiliserait NLP plus avanc√©\n",
    "        \n",
    "        # V√©rifier si le message contient une pr√©sentation avec un nom\n",
    "        name_triggers = [\"je m'appelle\", \"mon nom est\", \"je suis\"]\n",
    "        for trigger in name_triggers:\n",
    "            if trigger in message.lower():\n",
    "                parts = message.lower().split(trigger)\n",
    "                if len(parts) > 1:\n",
    "                    potential_name = parts[1].strip().split()[0]\n",
    "                    # Capitaliser le nom suppos√©\n",
    "                    potential_name = potential_name.capitalize()\n",
    "                    self.memory.add_to_long_term_memory(\"nom\", potential_name)\n",
    "                    break\n",
    "        \n",
    "        # Autres extractions possibles (pr√©f√©rences, localisation, etc.)\n",
    "\n",
    "def create_interface() -> gr.Blocks:\n",
    "    \"\"\"Cr√©e l'interface utilisateur pour l'assistant virtuel.\"\"\"\n",
    "    \n",
    "    # Cr√©er l'assistant\n",
    "    assistant = VirtualAssistant()\n",
    "    \n",
    "    # CSS personnalis√©\n",
    "    css = \"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 1000px !important;\n",
    "    }\n",
    "    .chat-message-container {\n",
    "        padding: 15px;\n",
    "        border-radius: 15px;\n",
    "        margin-bottom: 10px;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #f0f7ff;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #e6f7e6;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .personality-btn {\n",
    "        padding: 8px 15px;\n",
    "        border-radius: 20px;\n",
    "        border: none;\n",
    "        margin: 5px;\n",
    "        font-weight: bold;\n",
    "        cursor: pointer;\n",
    "        transition: all 0.3s;\n",
    "    }\n",
    "    .personality-btn:hover {\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    .personality-amical {\n",
    "        background-color: #ffde7d;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-professionnel {\n",
    "        background-color: #7db9ff;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-expert {\n",
    "        background-color: #b69cff;\n",
    "        color: #333;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\"), css=css) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">‚ú® Assistant Virtuel Personnalisable</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Un assistant conversationnel avec diff√©rentes personnalit√©s</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-user-circle\"></i> Personnalit√©\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    personality_intro = gr.Markdown(assistant.personality.intro)\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        amical_btn = gr.Button(\n",
    "                            f\"L√©o {assistant.personality.PERSONALITIES['amical']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-amical\"]\n",
    "                        )\n",
    "                        pro_btn = gr.Button(\n",
    "                            f\"Sophie {assistant.personality.PERSONALITIES['professionnel']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-professionnel\"]\n",
    "                        )\n",
    "                        expert_btn = gr.Button(\n",
    "                            f\"Dr. Martin {assistant.personality.PERSONALITIES['expert']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-expert\"]\n",
    "                        )\n",
    "                \n",
    "                with gr.Accordion(\"√Ä propos de cet assistant\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        Cet assistant virtuel utilise l'API Gemini de Google pour g√©n√©rer des r√©ponses contextuelles.\n",
    "                        \n",
    "                        Chaque personnalit√© a son propre style de communication :\n",
    "                        \n",
    "                        - **L√©o** est amical et d√©contract√©, utilisant un langage informel et des √©mojis\n",
    "                        - **Sophie** est professionnelle et concise, privil√©giant l'efficacit√©\n",
    "                        - **Dr. Martin** est un expert analytique fournissant des r√©ponses d√©taill√©es et document√©es\n",
    "                        \n",
    "                        L'assistant peut se souvenir de certaines informations tout au long de la conversation.\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                with gr.Accordion(\"Capacit√©s et limitations\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        ### Capacit√©s :\n",
    "                        - R√©pondre √† des questions g√©n√©rales\n",
    "                        - Fournir des explications et d√©finitions\n",
    "                        - Proposer des id√©es et suggestions\n",
    "                        - Maintenir une conversation coh√©rente\n",
    "                        - S'adapter √† diff√©rents styles de communication\n",
    "                        \n",
    "                        ### Limitations :\n",
    "                        - Connaissance limit√©e aux √©v√©nements ant√©rieurs √† sa date d'entra√Ænement\n",
    "                        - Ne peut pas acc√©der √† Internet ou ex√©cuter du code\n",
    "                        - Ne peut pas voir ou analyser d'images\n",
    "                        - Peut parfois g√©n√©rer des informations incorrectes\n",
    "                        \"\"\",\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\",\n",
    "                        type=\"messages\",\n",
    "                        avatar_images=[None, assistant.personality.avatar]\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Comment puis-je vous aider aujourd'hui ?\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "        \n",
    "        # √âv√©nements\n",
    "        def clear_chat():\n",
    "            return []\n",
    "        \n",
    "        def switch_personality(personality_type):\n",
    "            intro_message = assistant.switch_personality(personality_type)\n",
    "            return intro_message, [], gr.update(avatar_images=[None, assistant.personality.avatar])\n",
    "        \n",
    "        # Traitement des interactions utilisateur\n",
    "        submit_btn.click(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        # Changement de personnalit√©\n",
    "        amical_btn.click(\n",
    "            lambda: switch_personality(\"amical\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        pro_btn.click(\n",
    "            lambda: switch_personality(\"professionnel\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        expert_btn.click(\n",
    "            lambda: switch_personality(\"expert\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        # Accueil\n",
    "        demo.load(\n",
    "            lambda: assistant.personality.intro,\n",
    "            outputs=[personality_intro]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_interface()\n",
    "        # D√©tection de l'environnement Hugging Face Spaces\n",
    "        if os.getenv(\"SPACE_ID\"):\n",
    "            # Configuration pour Hugging Face Spaces\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\",\n",
    "                share=False\n",
    "            )\n",
    "        else:\n",
    "            # Configuration pour un environnement local\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\", \n",
    "                server_port=7879, \n",
    "                share=True,\n",
    "                favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b948d6c4-60f6-41e8-a8da-5116eff05cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\4052332530.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:12:09,849 - INFO: HTTP Request: GET http://localhost:7897/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:12:12,152 - INFO: HTTP Request: HEAD http://localhost:7897/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:12:16,185 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://def10a76e4bbe18c5f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:12:39,114 - INFO: HTTP Request: HEAD https://def10a76e4bbe18c5f.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://def10a76e4bbe18c5f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Pr√©parer l'aper√ßu (limit√©)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqu√©]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"üìÑ Fichier : {self.document_name} | üìä {char_count} caract√®res\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse bas√©e sur le prompt et l'historique de conversation\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqu√© pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel √† l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            if \"candidates\" in result and result[\"candidates\"]:\n",
    "                raw_reply = result[\"candidates\"][0].get(\"content\", {})\n",
    "                if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                    reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "                else:\n",
    "                    reply = \"Erreur: Format de r√©ponse inattendu\"\n",
    "            else:\n",
    "                reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            self.last_response = reply\n",
    "            history.append((message, reply))\n",
    "            \n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append((message, f\"Erreur : {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"G√©n√®re un fichier t√©l√©chargeable au format sp√©cifi√©\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger. G√©n√©rez d'abord une r√©ponse.\"\n",
    "        \n",
    "        try:\n",
    "            # Cr√©er un r√©pertoire temporaire si n√©cessaire\n",
    "            temp_dir = tempfile.gettempdir()\n",
    "            \n",
    "            # Cr√©er un nom de fichier avec timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            base_filename = f\"reponse_{timestamp}\"\n",
    "            filepath = os.path.join(temp_dir, f\"{base_filename}.{file_format}\")\n",
    "            \n",
    "            # Nom d'affichage pour l'interface\n",
    "            display_name = f\"{base_filename}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Division du texte en lignes pour √©viter les probl√®mes d'encodage\n",
    "                lines = self.last_response.split('\\n')\n",
    "                for line in lines:\n",
    "                    # Remplacer les caract√®res probl√©matiques\n",
    "                    clean_line = line.encode('latin-1', 'replace').decode('latin-1')\n",
    "                    pdf.multi_cell(190, 7, clean_line)\n",
    "                pdf.output(filepath)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"R√©ponse g√©n√©r√©e\", level=1)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filepath)\n",
    "            elif file_format == \"xlsx\":\n",
    "                # Meilleure conversion en tableau\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                df.to_excel(filepath, index=False)\n",
    "            elif file_format == \"csv\":\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8-sig')  # BOM pour Excel\n",
    "            \n",
    "            # V√©rifier que le fichier existe bien\n",
    "            if not os.path.exists(filepath):\n",
    "                logger.error(f\"Le fichier '{filepath}' n'a pas √©t√© cr√©√©\")\n",
    "                return None, \"‚ùå Erreur: Le fichier n'a pas √©t√© cr√©√© correctement\"\n",
    "            \n",
    "            logger.info(f\"Fichier cr√©√© avec succ√®s: {filepath}\")\n",
    "            return filepath, \"‚úÖ Fichier g√©n√©r√© avec succ√®s\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\", exc_info=True)\n",
    "            return None, f\"‚ùå Erreur lors de la g√©n√©ration du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Th√®me personnalis√© - inspir√© de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"T√©l√©versez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document charg√©\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de g√©n√©ration\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"T√©l√©charger le fichier g√©n√©r√©\", \n",
    "                                    interactive=False,\n",
    "                                    type=\"filepath\"  # Important: utiliser filepath pour le t√©l√©chargement\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents gr√¢ce √† l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **T√©l√©chargez un document**\n",
    "                   - Formats support√©s : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte √† l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme r√©f√©rence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez pr√©cis dans vos requ√™tes\n",
    "                   - Exemple : \"Peux-tu r√©sumer ce document ?\" ou \"Quels sont les points principaux abord√©s ?\"\n",
    "                \n",
    "                4. **Exportez le r√©sultat**\n",
    "                   - T√©l√©chargez les r√©ponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requ√™tes efficaces :\n",
    "                \n",
    "                - \"R√©sume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations cl√©s pr√©sent√©es dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des √©v√©nements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7897, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c049a5ba-b5ae-4d8a-93e8-5970aab401ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\907061348.py:357: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:18:57,635 - INFO: HTTP Request: GET http://localhost:7898/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:18:59,460 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:18:59,702 - INFO: HTTP Request: HEAD http://localhost:7898/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:19:05,888 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://a2500aa26104e9b212.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:19:28,224 - INFO: HTTP Request: HEAD https://a2500aa26104e9b212.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a2500aa26104e9b212.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:21:43,884 - INFO: Fichier cr√©√© avec succ√®s: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\tmpl4pdzsny.docx (37256 octets)\n",
      "2025-04-15 14:25:42,671 - INFO: Fichier cr√©√© avec succ√®s: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\tmp6wq0p56u.docx (37386 octets)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Pr√©parer l'aper√ßu (limit√©)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqu√©]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"üìÑ Fichier : {self.document_name} | üìä {char_count} caract√®res\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse bas√©e sur le prompt et l'historique de conversation\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqu√© pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel √† l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            if \"candidates\" in result and result[\"candidates\"]:\n",
    "                raw_reply = result[\"candidates\"][0].get(\"content\", {})\n",
    "                if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                    reply = raw_reply[\"parts\"][0].get(\"text\", \"R√©ponse vide\")\n",
    "                else:\n",
    "                    reply = \"Erreur: Format de r√©ponse inattendu\"\n",
    "            else:\n",
    "                reply = \"D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse.\"\n",
    "            \n",
    "            self.last_response = reply\n",
    "            history.append((message, reply))\n",
    "            \n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append((message, f\"Erreur : {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"G√©n√®re un fichier t√©l√©chargeable au format sp√©cifi√©\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger. G√©n√©rez d'abord une r√©ponse.\"\n",
    "        \n",
    "        try:\n",
    "            # Cr√©er un fichier temporaire qui ne sera pas supprim√© imm√©diatement\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            temp_file.close()\n",
    "            filepath = temp_file.name\n",
    "            \n",
    "            # Nom d'affichage pour l'interface\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"reponse_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traiter le texte pour √©viter les probl√®mes d'encodage\n",
    "                encoded_text = self.last_response.encode('ascii', 'replace').decode('ascii')\n",
    "                \n",
    "                # Diviser en paragraphes pour une meilleure mise en page\n",
    "                paragraphs = encoded_text.split('\\n\\n')\n",
    "                for paragraph in paragraphs:\n",
    "                    if paragraph.strip():\n",
    "                        pdf.multi_cell(190, 7, paragraph)\n",
    "                        pdf.ln(3)  # Espace entre paragraphes\n",
    "                \n",
    "                pdf.output(filepath)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"R√©ponse g√©n√©r√©e\", level=1)\n",
    "                \n",
    "                # Ajouter le texte en pr√©servant les paragraphes\n",
    "                paragraphs = self.last_response.split('\\n\\n')\n",
    "                for para in paragraphs:\n",
    "                    if para.strip():\n",
    "                        doc.add_paragraph(para)\n",
    "                \n",
    "                doc.save(filepath)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter de structurer les donn√©es en tableau si possible\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                \n",
    "                # D√©tection de structure tabulaire (si le texte contient des s√©parateurs coh√©rents)\n",
    "                if any('\\t' in line for line in lines) or all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                    # Tenter de cr√©er un DataFrame structur√©\n",
    "                    if all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                        # Format Markdown table\n",
    "                        clean_lines = [line.strip().strip('|') for line in lines if '|' in line]\n",
    "                        rows = [row.split('|') for row in clean_lines]\n",
    "                        rows = [[cell.strip() for cell in row] for row in rows]\n",
    "                        \n",
    "                        # Supprimer la ligne de s√©paration markdown si pr√©sente\n",
    "                        if len(rows) > 1 and all('-' in cell for cell in rows[1]):\n",
    "                            rows.pop(1)\n",
    "                        \n",
    "                        if rows:\n",
    "                            headers = rows[0] if len(rows) > 1 else [f\"Col{i+1}\" for i in range(len(rows[0]))]\n",
    "                            data = rows[1:] if len(rows) > 1 else rows\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                        else:\n",
    "                            df = pd.DataFrame({\"Contenu\": lines})\n",
    "                    else:\n",
    "                        # Format avec tabulations ou autre s√©parateur d√©tect√©\n",
    "                        df = pd.DataFrame([line.split('\\t') for line in lines])\n",
    "                else:\n",
    "                    # Format simple (une colonne)\n",
    "                    df = pd.DataFrame({\"Contenu\": lines})\n",
    "                \n",
    "                # Sauvegarder en Excel\n",
    "                df.to_excel(filepath, index=False)\n",
    "            \n",
    "            elif file_format == \"csv\":\n",
    "                # Similaire √† xlsx mais au format CSV\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                \n",
    "                # D√©tection des structures tabulaires comme pour xlsx\n",
    "                if any('\\t' in line for line in lines) or all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                    if all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                        clean_lines = [line.strip().strip('|') for line in lines if '|' in line]\n",
    "                        rows = [row.split('|') for row in clean_lines]\n",
    "                        rows = [[cell.strip() for cell in row] for row in rows]\n",
    "                        \n",
    "                        if len(rows) > 1 and all('-' in cell for cell in rows[1]):\n",
    "                            rows.pop(1)\n",
    "                        \n",
    "                        if rows:\n",
    "                            headers = rows[0] if len(rows) > 1 else [f\"Col{i+1}\" for i in range(len(rows[0]))]\n",
    "                            data = rows[1:] if len(rows) > 1 else rows\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                        else:\n",
    "                            df = pd.DataFrame({\"Contenu\": lines})\n",
    "                    else:\n",
    "                        df = pd.DataFrame([line.split('\\t') for line in lines])\n",
    "                else:\n",
    "                    df = pd.DataFrame({\"Contenu\": lines})\n",
    "                \n",
    "                # Sauvegarder en CSV avec encodage pour Excel\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # V√©rifier que le fichier existe bien\n",
    "            if not os.path.exists(filepath):\n",
    "                logger.error(f\"Le fichier '{filepath}' n'a pas √©t√© cr√©√©\")\n",
    "                return None, \"‚ùå Erreur: Le fichier n'a pas √©t√© cr√©√© correctement\"\n",
    "            \n",
    "            # V√©rifier la taille du fichier\n",
    "            file_size = os.path.getsize(filepath)\n",
    "            if file_size == 0:\n",
    "                logger.error(f\"Le fichier '{filepath}' est vide\")\n",
    "                return None, \"‚ùå Erreur: Le fichier g√©n√©r√© est vide\"\n",
    "                \n",
    "            logger.info(f\"Fichier cr√©√© avec succ√®s: {filepath} ({file_size} octets)\")\n",
    "            return filepath, f\"‚úÖ Fichier '{display_name}' g√©n√©r√© avec succ√®s ({file_size} octets)\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\", exc_info=True)\n",
    "            return None, f\"‚ùå Erreur lors de la g√©n√©ration du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Th√®me personnalis√© - inspir√© de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"T√©l√©versez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document charg√©\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de g√©n√©ration\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                # Utiliser File avec type=\"filepath\" pour le t√©l√©chargement\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"T√©l√©charger le fichier g√©n√©r√©\", \n",
    "                                    interactive=False,\n",
    "                                    type=\"filepath\",\n",
    "                                    elem_classes=\"download-btn\"\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents gr√¢ce √† l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **T√©l√©chargez un document**\n",
    "                   - Formats support√©s : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte √† l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme r√©f√©rence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez pr√©cis dans vos requ√™tes\n",
    "                   - Exemple : \"Peux-tu r√©sumer ce document ?\" ou \"Quels sont les points principaux abord√©s ?\"\n",
    "                \n",
    "                4. **Exportez le r√©sultat**\n",
    "                   - T√©l√©chargez les r√©ponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requ√™tes efficaces :\n",
    "                \n",
    "                - \"R√©sume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations cl√©s pr√©sent√©es dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des √©v√©nements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7898, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f727797a-9cf4-4913-a721-2e1d13f0b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:36:16,458 - INFO: Connexion √† LM Studio r√©ussie. Mod√®le: mistral-7b-instruct-v0.3\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\1871222720.py:388: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:36:19,580 - INFO: HTTP Request: GET http://localhost:7899/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:36:21,890 - INFO: HTTP Request: HEAD http://localhost:7899/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7899/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:37:42,348 - ERROR: Erreur API (400): {\"error\":\"Error rendering prompt with jinja template: \\\"Error: Only user and assistant roles are supported!\\n    at C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:254303\\n    at _0x1f852a.value (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:252374)\\n    at _0x5c2bc5.evaluateCallExpression (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:241449)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:251261)\\n    at _0x5c2bc5.evaluateBlock (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:240633)\\n    at _0x5c2bc5.evaluateIf (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:245045)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:249965)\\n    at _0x5c2bc5.evaluateBlock (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:240633)\\n    at _0x5c2bc5.evaluateIf (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:245045)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:249965)\\\". This is usually an issue with the model's prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration pour LM Studio local API\n",
    "LM_STUDIO_API_URL = \"http://localhost:1234/v1/chat/completions\"  # URL par d√©faut de LM Studio\n",
    "LM_STUDIO_MODEL = \"mistral-7b-instruct-v0.3\"  # Le mod√®le que vous avez t√©l√©charg√©\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non support√©\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Pr√©parer l'aper√ßu (limit√©)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqu√©]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"üìÑ Fichier : {self.document_name} | üìä {char_count} caract√®res\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"G√©n√®re une r√©ponse bas√©e sur le prompt et l'historique de conversation en utilisant LM Studio\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            # Formatter le contexte du document si activ√©\n",
    "            document_context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                document_context = f\"Je vous fournis le contexte d'un document intitul√© '{self.document_name}':\\n\\n{self.current_document_text[:3000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 3000:\n",
    "                    document_context += \"[Le document est plus long, ceci est un extrait.]\\n\\n\"\n",
    "                document_context += \"Basez vos r√©ponses sur ce document lorsque c'est pertinent.\\n\\n\"\n",
    "            \n",
    "            # Pr√©parer les messages pour l'API de chat\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"Vous √™tes un assistant documentaire utile et pr√©cis. {document_context}\"}\n",
    "            ]\n",
    "            \n",
    "            # Ajouter l'historique des conversations\n",
    "            for user_msg, assistant_msg in history:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "            \n",
    "            # Ajouter le message actuel\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            # Pr√©paration de la requ√™te pour l'API LM Studio\n",
    "            payload = {\n",
    "                \"model\": LM_STUDIO_MODEL,\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 1024,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            # Appel √† l'API LM Studio\n",
    "            response = requests.post(LM_STUDIO_API_URL, headers=headers, data=json.dumps(payload))\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                response_json = response.json()\n",
    "                reply = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "                self.last_response = reply\n",
    "                history.append((message, reply))\n",
    "            else:\n",
    "                error_msg = f\"Erreur API ({response.status_code}): {response.text}\"\n",
    "                logger.error(error_msg)\n",
    "                history.append((message, f\"Erreur: Impossible de g√©n√©rer une r√©ponse. V√©rifiez que LM Studio est en cours d'ex√©cution sur localhost:1234.\"))\n",
    "            \n",
    "            return history\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            error_msg = \"Erreur de connexion: Impossible de se connecter √† LM Studio. V√©rifiez qu'il est en cours d'ex√©cution.\"\n",
    "            logger.error(error_msg)\n",
    "            history.append((message, error_msg))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur inattendue: {e}\")\n",
    "            history.append((message, f\"Erreur: {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"G√©n√®re un fichier t√©l√©chargeable au format sp√©cifi√©\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger. G√©n√©rez d'abord une r√©ponse.\"\n",
    "        \n",
    "        try:\n",
    "            # Cr√©er un fichier temporaire qui ne sera pas supprim√© imm√©diatement\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            temp_file.close()\n",
    "            filepath = temp_file.name\n",
    "            \n",
    "            # Nom d'affichage pour l'interface\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"reponse_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traiter le texte pour √©viter les probl√®mes d'encodage\n",
    "                encoded_text = self.last_response.encode('ascii', 'replace').decode('ascii')\n",
    "                \n",
    "                # Diviser en paragraphes pour une meilleure mise en page\n",
    "                paragraphs = encoded_text.split('\\n\\n')\n",
    "                for paragraph in paragraphs:\n",
    "                    if paragraph.strip():\n",
    "                        pdf.multi_cell(190, 7, paragraph)\n",
    "                        pdf.ln(3)  # Espace entre paragraphes\n",
    "                \n",
    "                pdf.output(filepath)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"R√©ponse g√©n√©r√©e\", level=1)\n",
    "                \n",
    "                # Ajouter le texte en pr√©servant les paragraphes\n",
    "                paragraphs = self.last_response.split('\\n\\n')\n",
    "                for para in paragraphs:\n",
    "                    if para.strip():\n",
    "                        doc.add_paragraph(para)\n",
    "                \n",
    "                doc.save(filepath)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter de structurer les donn√©es en tableau si possible\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                \n",
    "                # D√©tection de structure tabulaire\n",
    "                if any('\\t' in line for line in lines) or all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                    # Tenter de cr√©er un DataFrame structur√©\n",
    "                    if all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                        # Format Markdown table\n",
    "                        clean_lines = [line.strip().strip('|') for line in lines if '|' in line]\n",
    "                        rows = [row.split('|') for row in clean_lines]\n",
    "                        rows = [[cell.strip() for cell in row] for row in rows]\n",
    "                        \n",
    "                        # Supprimer la ligne de s√©paration markdown si pr√©sente\n",
    "                        if len(rows) > 1 and all('-' in cell for cell in rows[1]):\n",
    "                            rows.pop(1)\n",
    "                        \n",
    "                        if rows:\n",
    "                            headers = rows[0] if len(rows) > 1 else [f\"Col{i+1}\" for i in range(len(rows[0]))]\n",
    "                            data = rows[1:] if len(rows) > 1 else rows\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                        else:\n",
    "                            df = pd.DataFrame({\"Contenu\": lines})\n",
    "                    else:\n",
    "                        # Format avec tabulations ou autre s√©parateur d√©tect√©\n",
    "                        df = pd.DataFrame([line.split('\\t') for line in lines])\n",
    "                else:\n",
    "                    # Format simple (une colonne)\n",
    "                    df = pd.DataFrame({\"Contenu\": lines})\n",
    "                \n",
    "                # Sauvegarder en Excel\n",
    "                df.to_excel(filepath, index=False)\n",
    "            \n",
    "            elif file_format == \"csv\":\n",
    "                # Similaire √† xlsx mais au format CSV\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                \n",
    "                # D√©tection des structures tabulaires comme pour xlsx\n",
    "                if any('\\t' in line for line in lines) or all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                    if all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                        clean_lines = [line.strip().strip('|') for line in lines if '|' in line]\n",
    "                        rows = [row.split('|') for row in clean_lines]\n",
    "                        rows = [[cell.strip() for cell in row] for row in rows]\n",
    "                        \n",
    "                        if len(rows) > 1 and all('-' in cell for cell in rows[1]):\n",
    "                            rows.pop(1)\n",
    "                        \n",
    "                        if rows:\n",
    "                            headers = rows[0] if len(rows) > 1 else [f\"Col{i+1}\" for i in range(len(rows[0]))]\n",
    "                            data = rows[1:] if len(rows) > 1 else rows\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                        else:\n",
    "                            df = pd.DataFrame({\"Contenu\": lines})\n",
    "                    else:\n",
    "                        df = pd.DataFrame([line.split('\\t') for line in lines])\n",
    "                else:\n",
    "                    df = pd.DataFrame({\"Contenu\": lines})\n",
    "                \n",
    "                # Sauvegarder en CSV avec encodage pour Excel\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # V√©rifier que le fichier existe bien\n",
    "            if not os.path.exists(filepath):\n",
    "                logger.error(f\"Le fichier '{filepath}' n'a pas √©t√© cr√©√©\")\n",
    "                return None, \"‚ùå Erreur: Le fichier n'a pas √©t√© cr√©√© correctement\"\n",
    "            \n",
    "            # V√©rifier la taille du fichier\n",
    "            file_size = os.path.getsize(filepath)\n",
    "            if file_size == 0:\n",
    "                logger.error(f\"Le fichier '{filepath}' est vide\")\n",
    "                return None, \"‚ùå Erreur: Le fichier g√©n√©r√© est vide\"\n",
    "                \n",
    "            logger.info(f\"Fichier cr√©√© avec succ√®s: {filepath} ({file_size} octets)\")\n",
    "            return filepath, f\"‚úÖ Fichier '{display_name}' g√©n√©r√© avec succ√®s ({file_size} octets)\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\", exc_info=True)\n",
    "            return None, f\"‚ùå Erreur lors de la g√©n√©ration du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Th√®me personnalis√© - inspir√© de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant (Local LLM)\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "        .model-info {padding: 5px 10px; background-color: #f3f4f6; border-radius: 4px; font-size: 0.85rem; color: #4b5563;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant (Local LLM)\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B\n",
    "            \"\"\")\n",
    "            \n",
    "        # Indicateur de statut du mod√®le local\n",
    "        with gr.Row():\n",
    "            model_status = gr.HTML(\n",
    "                f\"\"\"<div class=\"model-info\">‚úì Mod√®le local: <strong>{LM_STUDIO_MODEL}</strong> | Serveur: <strong>{LM_STUDIO_API_URL}</strong></div>\"\"\", \n",
    "                elem_classes=\"model-info\"\n",
    "            )\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"T√©l√©versez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document charg√©\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de g√©n√©ration\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                # Utiliser File avec type=\"filepath\" pour le t√©l√©chargement\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"T√©l√©charger le fichier g√©n√©r√©\", \n",
    "                                    interactive=False,\n",
    "                                    type=\"filepath\",\n",
    "                                    elem_classes=\"download-btn\"\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant (Local LLM)** est un outil qui vous permet de discuter avec vos documents en utilisant un mod√®le d'IA local (Mistral-7B).\n",
    "                \n",
    "                #### Pr√©requis :\n",
    "                \n",
    "                1. **LM Studio** doit √™tre install√© et en cours d'ex√©cution sur votre ordinateur\n",
    "                2. Le mod√®le **mistral-7b-instruct-v0.3** doit √™tre charg√© dans LM Studio\n",
    "                3. Le serveur API local doit √™tre activ√© (g√©n√©ralement sur localhost:1234)\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **T√©l√©chargez un document**\n",
    "                   - Formats support√©s : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte √† l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme r√©f√©rence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez pr√©cis dans vos requ√™tes\n",
    "                   - Exemple : \"Peux-tu r√©sumer ce document ?\" ou \"Quels sont les points principaux abord√©s ?\"\n",
    "                \n",
    "                4. **Exportez le r√©sultat**\n",
    "                   - T√©l√©chargez les r√©ponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Avantages de l'utilisation en local :\n",
    "                \n",
    "                - **Confidentialit√©** : Vos documents et conversations restent sur votre ordinateur\n",
    "                - **Sans connexion** : Fonctionne sans acc√®s √† Internet\n",
    "                - **Gratuit** : Pas de frais d'API\n",
    "                \n",
    "                #### D√©pannage :\n",
    "                \n",
    "                - Si vous obtenez une erreur de connexion, v√©rifiez que LM Studio est bien lanc√©\n",
    "                - V√©rifiez que le serveur local est activ√© dans LM Studio (bouton \"Start Server\")\n",
    "                - Si le mod√®le est lent, essayez de r√©duire la taille des documents\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des √©v√©nements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # V√©rifier la connexion √† LM Studio avant de lancer l'interface\n",
    "        try:\n",
    "            test_payload = {\n",
    "                \"model\": LM_STUDIO_MODEL,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Test de connexion\"}],\n",
    "                \"max_tokens\": 10\n",
    "            }\n",
    "            response = requests.post(\n",
    "                LM_STUDIO_API_URL, \n",
    "                headers={\"Content-Type\": \"application/json\"}, \n",
    "                data=json.dumps(test_payload),\n",
    "                timeout=5\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                logger.info(f\"Connexion √† LM Studio r√©ussie. Mod√®le: {LM_STUDIO_MODEL}\")\n",
    "            else:\n",
    "                logger.warning(f\"LM Studio est accessible mais a retourn√© une erreur: {response.status_code}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logger.warning(\"Impossible de se connecter √† LM Studio. L'application va d√©marrer, mais v√©rifiez que LM Studio est en cours d'ex√©cution.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Erreur lors du test de connexion √† LM Studio: {e}\")\n",
    "            \n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7899, share=False)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bdbb9bf-fa93-45b5-8526-4977f340cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:55:41,349 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:55:41,433 - INFO: HTTP Request: GET http://localhost:7091/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:55:43,489 - INFO: HTTP Request: HEAD http://localhost:7091/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7091/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:56:45,792 - ERROR: API Error 400: {\"error\":\"Error rendering prompt with jinja template: \\\"Error: Only user and assistant roles are supported!\\n    at C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:254303\\n    at _0x1f852a.value (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:252374)\\n    at _0x5c2bc5.evaluateCallExpression (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:241449)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:251261)\\n    at _0x5c2bc5.evaluateBlock (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:240633)\\n    at _0x5c2bc5.evaluateIf (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:245045)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:249965)\\n    at _0x5c2bc5.evaluateBlock (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:240633)\\n    at _0x5c2bc5.evaluateIf (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:245045)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:249965)\\\". This is usually an issue with the model's prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration de LM Studio\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "LM_STUDIO_CHAT_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/chat/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non support√©\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, str]:\n",
    "        if file is None:\n",
    "            return \"\", \"\"\n",
    "        try:\n",
    "            self.document_name = os.path.basename(file.name)\n",
    "            self.current_text = self.processor.extract_text_from_file(file.name)\n",
    "            preview = self.current_text[:1500] + (\"\\n\\n[...]\" if len(self.current_text) > 1500 else \"\")\n",
    "            info = f\"üìÑ {self.document_name} | {len(self.current_text)} caract√®res\"\n",
    "            return info, preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur traitement document: {e}\")\n",
    "            return \"\", f\"Erreur: {e}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Dict[str, str]], mode: str) -> List[Dict[str, str]]:\n",
    "        if history is None:\n",
    "            history = []\n",
    "        if not message:\n",
    "            return history\n",
    "        # Construire le contexte\n",
    "        context = \"\"\n",
    "        if mode == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:3000]\n",
    "            context = (\n",
    "                f\"Contexte (doc: {self.document_name}):\\n{excerpt}\"\n",
    "                + (\"\\n[...]\" if len(self.current_text) > 3000 else \"\")\n",
    "                + \"\\n\\n\"\n",
    "            )\n",
    "        system_msg = {\"role\": \"system\", \"content\": f\"Vous √™tes un assistant. {context}\"}\n",
    "        # Messages pour l'API\n",
    "        api_messages = [system_msg] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"messages\": api_messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_CHAT_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            if resp.status_code == 200:\n",
    "                data = resp.json()\n",
    "                reply = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                self.last_response = reply\n",
    "                history.append({\"role\": \"user\", \"content\": message})\n",
    "                history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            else:\n",
    "                err = f\"API Error {resp.status_code}: {resp.text}\"\n",
    "                logger.error(err)\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"Erreur: impossible de g√©n√©rer une r√©ponse. V√©rifiez LM Studio sur {LM_STUDIO_BASE_URL}\"})\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Connection error: {e}\")\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Erreur connexion √† LM Studio: {e}\"})\n",
    "        return history\n",
    "\n",
    "    def download_response(self, fmt: str) -> Tuple[str, str]:\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucune r√©ponse g√©n√©r√©e\"\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{fmt}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{fmt}\"\n",
    "        try:\n",
    "            if fmt == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif fmt == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", size=11)\n",
    "                for p in self.last_response.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif fmt == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"R√©ponse\", level=1)\n",
    "                for p in self.last_response.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif fmt in [\"xlsx\", \"csv\"]:\n",
    "                lines = [l for l in self.last_response.split(\"\\n\") if l.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                if fmt == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {fmt}\"\n",
    "            size = os.path.getsize(path)\n",
    "            logger.info(f\"Fichier g√©n√©r√©: {path} ({size} octets)\")\n",
    "            return path, f\"Fichier '{name}' g√©n√©r√© ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export: {e}\")\n",
    "            return None, f\"Erreur export: {e}\"\n",
    "\n",
    "# Construction de l'interface Gradio\n",
    "assistant = DocumentChatAssistant()\n",
    "with gr.Blocks(title=\"Document Chat Assistant\") as demo:\n",
    "    gr.Markdown(\"# üí¨ Chat Assistant Local (Mistral-7B)\")\n",
    "    gr.HTML(f\"<p>Serveur LM Studio: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Chat avec documents\"):\n",
    "            file_input = gr.File(label=\"T√©l√©versez un document (.pdf, .docx, .txt)\")\n",
    "            file_info = gr.Textbox(interactive=False, label=\"Infos fichier\")\n",
    "            preview = gr.Textbox(lines=10, interactive=False, label=\"Aper√ßu du document\")\n",
    "            mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode\")\n",
    "            chatbot = gr.Chatbot(type=\"messages\", label=\"Conversation\")\n",
    "            user_input = gr.Textbox(lines=2, placeholder=\"Votre message...\", label=\"Message\")\n",
    "            send = gr.Button(\"Envoyer\")\n",
    "            fmt = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format export\")\n",
    "            dl_btn = gr.Button(\"T√©l√©charger\")\n",
    "            dl_file = gr.File(interactive=False, type=\"filepath\", label=\"Fichier export√©\")\n",
    "            dl_status = gr.Textbox(interactive=False, label=\"Statut\")\n",
    "\n",
    "            file_input.upload(\n",
    "                assistant.process_document,\n",
    "                inputs=[file_input],\n",
    "                outputs=[file_info, preview]\n",
    "            )\n",
    "            send.click(\n",
    "                assistant.generate_response,\n",
    "                inputs=[user_input, chatbot, mode],\n",
    "                outputs=[chatbot]\n",
    "            ).then(\n",
    "                lambda: \"\",\n",
    "                outputs=[user_input]\n",
    "            )\n",
    "            user_input.submit(\n",
    "                assistant.generate_response,\n",
    "                inputs=[user_input, chatbot, mode],\n",
    "                outputs=[chatbot]\n",
    "            ).then(\n",
    "                lambda: \"\",\n",
    "                outputs=[user_input]\n",
    "            )\n",
    "            dl_btn.click(\n",
    "                assistant.download_response,\n",
    "                inputs=[fmt],\n",
    "                outputs=[dl_file, dl_status]\n",
    "            )\n",
    "        with gr.TabItem(\"Aide\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## Guide d'utilisation\n",
    "            1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "            2. V√©rifiez que le serveur local (127.0.0.1:1234) est d√©marr√©\n",
    "            3. T√©l√©versez un document et posez vos questions\n",
    "            4. Exportez la r√©ponse si besoin\n",
    "            \"\"\")\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7091)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a788a070-3c5d-44ce-bb77-a1447adab695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:05:20,253 - INFO: HTTP Request: GET http://localhost:7902/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 15:05:22,005 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 15:05:22,317 - INFO: HTTP Request: HEAD http://localhost:7902/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7902/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:06:37,728 - ERROR: Connection error: HTTPConnectionPool(host='127.0.0.1', port=1234): Read timed out. (read timeout=30)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# LM Studio configuration (via .env or defaults)\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "LM_STUDIO_CHAT_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/chat/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non support√©\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, gr.update, str]:\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        try:\n",
    "            self.document_name = os.path.basename(file.name)\n",
    "            self.current_text = self.processor.extract_text_from_file(file.name)\n",
    "            preview = self.current_text[:1500]\n",
    "            if len(self.current_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqu√©]\"\n",
    "            info = f\"üìÑ Fichier : {self.document_name} | üìä {len(self.current_text)} caract√®res\"\n",
    "            return info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {e}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Dict[str, str]], context_type: str) -> List[Dict[str, str]]:\n",
    "        # history: list of {'role': 'user'/'assistant', 'content': ...}\n",
    "        if history is None:\n",
    "            history = []\n",
    "        if not message:\n",
    "            return history\n",
    "        # Build context prefix\n",
    "        prefix = \"\"\n",
    "        if context_type == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:2000]\n",
    "            prefix = f\"Contexte du document '{self.document_name}':\\n{excerpt}\"\n",
    "            if len(self.current_text) > 2000:\n",
    "                prefix += \"\\n[... Reste du document tronqu√© ...]\"\n",
    "            prefix += \"\\n\\n\"\n",
    "        # New user message\n",
    "        user_content = prefix + message\n",
    "        # Prepare messages list without system role\n",
    "        api_messages = history.copy()\n",
    "        api_messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"messages\": api_messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_CHAT_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            if resp.status_code == 200:\n",
    "                data = resp.json()\n",
    "                reply = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                logger.error(f\"API Error {resp.status_code}: {resp.text}\")\n",
    "                reply = f\"Erreur: impossible de g√©n√©rer une r√©ponse. V√©rifiez LM Studio sur {LM_STUDIO_BASE_URL}\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Connection error: {e}\")\n",
    "            reply = f\"Erreur connexion √† LM Studio: {e}\"\n",
    "        # Append assistant reply\n",
    "        history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        self.last_response = reply\n",
    "        return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger.\"\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", size=11)\n",
    "                for p in self.last_response.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"R√©ponse g√©n√©r√©e\", level=1)\n",
    "                for p in self.last_response.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                lines = [l for l in self.last_response.split(\"\\n\") if l.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                if file_format == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {file_format}\"\n",
    "            size = os.path.getsize(path)\n",
    "            logger.info(f\"Fichier g√©n√©r√©: {path} ({size} octets)\")\n",
    "            return path, f\"‚úÖ Fichier '{name}' g√©n√©r√© ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"‚ùå Erreur export : {e}\"\n",
    "\n",
    "# Build UI\n",
    "assistant = DocumentChatAssistant()\n",
    "\n",
    "def build_ui():\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B (LM Studio)\n",
    "            \"\"\")\n",
    "\n",
    "        gr.HTML(f\"<p>Serveur local: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"T√©l√©versez un document\")\n",
    "                            file_info = gr.Textbox(label=\"Informations du fichier\", interactive=False)\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(label=\"Contenu du document\", lines=12, interactive=False)\n",
    "\n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode de g√©n√©ration\")\n",
    "\n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format d'export\")\n",
    "                                download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(label=\"Statut\", interactive=False)\n",
    "                                download_file = gr.File(label=\"T√©l√©charger le fichier g√©n√©r√©\", interactive=False, type=\"filepath\", elem_classes=\"download-btn\")\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(label=\"Conversation\", height=500, type=\"messages\")\n",
    "                            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question sur votre document...\", lines=2)\n",
    "                            with gr.Row():\n",
    "                                send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "\n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "\n",
    "                1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "                2. V√©rifiez que le serveur local (127.0.0.1:1234) est d√©marr√©\n",
    "                3. T√©l√©versez un document et posez vos questions\n",
    "                4. Exportez la r√©ponse si besoin\n",
    "                \"\"\")\n",
    "\n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(assistant.process_document, inputs=[file_input], outputs=[file_info, preview_accordion, preview])\n",
    "        send_btn.click(assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot]).then(lambda: \"\", outputs=[user_input])\n",
    "        user_input.submit(assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot]).then(lambda: \"\", outputs=[user_input])\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_response, inputs=[download_format], outputs=[download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7902)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c0f35f-6036-42d2-8c97-f38603d66e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\1164165499.py:224: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:17:50,145 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:17:51,133 - INFO: HTTP Request: GET http://localhost:7199/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:17:53,200 - INFO: HTTP Request: HEAD http://localhost:7199/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7199/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:18:28,030 - INFO: Envoi requ√™te √† http://127.0.0.1:1234/v1/completions\n",
      "2025-04-15 16:19:26,915 - INFO: R√©ponse code 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# LM Studio configuration (via .env or defaults)\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "# Use completions endpoint to avoid Jinja template issues\n",
    "LM_STUDIO_COMPLETIONS_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non support√©\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.history: List[Tuple[str, str]] = []  # list of (user, assistant)\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, gr.update, str]:\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        try:\n",
    "            path = file.name if hasattr(file, 'name') else file\n",
    "            self.document_name = os.path.basename(path)\n",
    "            self.current_text = self.processor.extract_text_from_file(path)\n",
    "            preview = self.current_text[:1500] + (\"\\n\\n[...]\" if len(self.current_text) > 1500 else \"\")\n",
    "            size_kb = os.path.getsize(path) / 1024\n",
    "            info = f\"üìÑ {self.document_name} ({size_kb:.1f} KB, {len(self.current_text)} caract√®res)\"\n",
    "            return info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return f\"Erreur: {e}\", gr.update(visible=False), \"\"\n",
    "\n",
    "    def generate_response(self, message: str, chat_history: List[Tuple[str, str]], context_type: str) -> Tuple[List[Tuple[str, str]], str]:\n",
    "        if not message.strip():\n",
    "            return chat_history, \"\"\n",
    "        # Build prompt\n",
    "        prompt = \"\"\n",
    "        if context_type == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:2000]\n",
    "            prompt += f\"Contexte du document '{self.document_name}':\\n{excerpt}\\n[...\\n]\\n\\n\"\n",
    "        # Append last messages\n",
    "        for u, a in chat_history[-3:]:  # keep last 3 exchanges\n",
    "            prompt += f\"Utilisateur: {u}\\nAssistant: {a}\\n\"\n",
    "        prompt += f\"Utilisateur: {message}\\nAssistant:\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"stop\": [\"Utilisateur:\", \"Assistant:\"]\n",
    "        }\n",
    "        try:\n",
    "            logger.info(f\"Envoi requ√™te √† {LM_STUDIO_COMPLETIONS_ENDPOINT}\")\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_COMPLETIONS_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=120\n",
    "            )\n",
    "            logger.info(f\"R√©ponse code {resp.status_code}\")\n",
    "            if resp.status_code != 200:\n",
    "                logger.error(f\"API Error {resp.status_code}: {resp.text}\")\n",
    "                reply = \"Erreur API, v√©rifiez que LM Studio est en cours d'ex√©cution.\"\n",
    "            else:\n",
    "                data = resp.json()\n",
    "                reply = data.get('choices', [{}])[0].get('text', '').strip()\n",
    "                if not reply:\n",
    "                    reply = \"Aucune r√©ponse re√ßue.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur requ√™te: {e}\")\n",
    "            reply = f\"Erreur de connexion ou timeout: {e}\"\n",
    "\n",
    "        chat_history.append((message, reply))\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    def download_response(self, file_format: str) -> Tuple[str, str]:\n",
    "        if not self.history:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger.\"\n",
    "        content = self.history[-1][1]\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", 11)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"R√©ponse g√©n√©r√©e\", 1)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                df = pd.DataFrame({\"Contenu\": content.split(\"\\n\")})\n",
    "                if file_format == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {file_format}\"\n",
    "            size = os.path.getsize(path)\n",
    "            return path, f\"‚úÖ Fichier '{name}' g√©n√©r√© ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export: {e}\")\n",
    "            return None, f\"Erreur export: {e}\"\n",
    "\n",
    "# Build UI\n",
    "assistant = DocumentChatAssistant()\n",
    "\n",
    "def build_ui():\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B (LM Studio)\n",
    "            \"\"\")\n",
    "\n",
    "        gr.HTML(f\"<p>Serveur local: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"T√©l√©versez un document\")\n",
    "                            file_info = gr.Textbox(label=\"Informations du fichier\", interactive=False)\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(label=\"Contenu du document\", lines=12, interactive=False)\n",
    "\n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode de g√©n√©ration\")\n",
    "\n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format d'export\")\n",
    "                                download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(label=\"Statut\", interactive=False)\n",
    "                                download_file = gr.File(label=\"T√©l√©charger le fichier g√©n√©r√©\", interactive=False, type=\"filepath\", elem_classes=\"download-btn\")\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n",
    "                            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question sur votre document...\", lines=2)\n",
    "                            with gr.Row():\n",
    "                                send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "\n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "\n",
    "                1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "                2. V√©rifiez que le serveur local (127.0.0.1:1234) est d√©marr√©\n",
    "                3. T√©l√©versez un document et posez vos questions\n",
    "                4. Exportez la r√©ponse si besoin\n",
    "                \"\"\")\n",
    "\n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(assistant.process_document, inputs=[file_input], outputs=[file_info, preview_accordion, preview])\n",
    "        send_btn.click(assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        user_input.submit(assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "        download_btn.click(assistant.download_response, inputs=[download_format], outputs=[download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7199)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0135f167-cd2e-4b4c-a25e-cd02c6a321c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\2357147622.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:26:35,398 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:26:36,369 - INFO: HTTP Request: GET http://localhost:7192/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:26:38,425 - INFO: HTTP Request: HEAD http://localhost:7192/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7192/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:27:14,105 - INFO: Envoi requ√™te √† http://127.0.0.1:1234/v1/completions\n",
      "2025-04-15 16:28:03,041 - INFO: R√©ponse code 200\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1663, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\2357147622.py\", line 255, in handle_download\n",
      "    return gr.File.update(value=path), status\n",
      "           ^^^^^^^^^^^^^^\n",
      "AttributeError: type object 'File' has no attribute 'update'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# LM Studio configuration\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "LM_STUDIO_COMPLETIONS_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non support√©\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.history: List[Tuple[str, str]] = []\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, gr.update, str]:\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        try:\n",
    "            path = file.name if hasattr(file, 'name') else file\n",
    "            self.document_name = os.path.basename(path)\n",
    "            self.current_text = self.processor.extract_text_from_file(path)\n",
    "            preview = self.current_text[:1500] + (\"\\n\\n[...]\" if len(self.current_text) > 1500 else \"\")\n",
    "            size_kb = os.path.getsize(path) / 1024\n",
    "            info = f\"üìÑ {self.document_name} ({size_kb:.1f} KB, {len(self.current_text)} caract√®res)\"\n",
    "            return info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return f\"Erreur: {e}\", gr.update(visible=False), \"\"\n",
    "\n",
    "    def generate_response(self, message: str, chat_history: List[Tuple[str, str]], context_type: str) -> Tuple[List[Tuple[str, str]], str]:\n",
    "        if not message.strip():\n",
    "            return chat_history, \"\"\n",
    "        # Build prompt\n",
    "        prompt = \"\"\n",
    "        if context_type == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:2000]\n",
    "            prompt += f\"Contexte du document '{self.document_name}':\\n{excerpt}\\n[...\\n]\\n\\n\"\n",
    "        # Append last messages\n",
    "        for u, a in chat_history[-3:]:\n",
    "            prompt += f\"Utilisateur: {u}\\nAssistant: {a}\\n\"\n",
    "        prompt += f\"Utilisateur: {message}\\nAssistant:\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"stop\": [\"Utilisateur:\", \"Assistant:\"]\n",
    "        }\n",
    "        try:\n",
    "            logger.info(f\"Envoi requ√™te √† {LM_STUDIO_COMPLETIONS_ENDPOINT}\")\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_COMPLETIONS_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=120\n",
    "            )\n",
    "            logger.info(f\"R√©ponse code {resp.status_code}\")\n",
    "            if resp.status_code != 200:\n",
    "                logger.error(f\"API Error {resp.status_code}: {resp.text}\")\n",
    "                reply = \"Erreur API, v√©rifiez que LM Studio est en cours d'ex√©cution.\"\n",
    "            else:\n",
    "                data = resp.json()\n",
    "                reply = data.get('choices', [{}])[0].get('text', '').strip() or \"Aucune r√©ponse re√ßue.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur requ√™te: {e}\")\n",
    "            reply = f\"Erreur de connexion ou timeout: {e}\"\n",
    "\n",
    "        # Update both chat history and internal history\n",
    "        chat_history.append((message, reply))\n",
    "        self.history = chat_history\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    def download_response(self, file_format: str) -> Tuple[str, str]:\n",
    "        if not self.history:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger.\"\n",
    "        content = self.history[-1][1]\n",
    "        if not content.strip():\n",
    "            return None, \"La r√©ponse est vide.\"\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", 11)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"R√©ponse g√©n√©r√©e\", 1)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                df = pd.DataFrame({\"Contenu\": content.split(\"\\n\")})\n",
    "                if file_format == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {file_format}\"\n",
    "            size = os.path.getsize(path)\n",
    "            return path, f\"‚úÖ Fichier '{name}' g√©n√©r√© ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export: {e}\")\n",
    "            return None, f\"Erreur export: {e}\"\n",
    "\n",
    "# Build UI\n",
    "demo_assistant = DocumentChatAssistant()\n",
    "\n",
    "def build_ui():\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B (LM Studio)\n",
    "            \"\"\")\n",
    "\n",
    "        gr.HTML(f\"<p>Serveur local: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"T√©l√©versez un document\")\n",
    "                            file_info = gr.Textbox(label=\"Informations du fichier\", interactive=False)\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(label=\"Contenu du document\", lines=12, interactive=False)\n",
    "\n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode de g√©n√©ration\")\n",
    "\n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format d'export\")\n",
    "                                download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(label=\"Statut\", interactive=False)\n",
    "                                download_file = gr.File(label=\"T√©l√©charger le fichier g√©n√©r√©\", interactive=False, type=\"filepath\", elem_classes=\"download-btn\")\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n",
    "                            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question sur votre document...\", lines=2)\n",
    "                            with gr.Row():\n",
    "                                send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "\n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "\n",
    "                1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "                2. V√©rifiez que le serveur local (127.0.0.1:1234) est d√©marr√©\n",
    "                3. T√©l√©versez un document et posez vos questions\n",
    "                4. Exportez la r√©ponse si besoin\n",
    "                \"\"\")\n",
    "\n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(demo_assistant.process_document, inputs=[file_input], outputs=[file_info, preview_accordion, preview])\n",
    "        send_btn.click(demo_assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        user_input.submit(demo_assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "        # Wrap download to update file component\n",
    "        def handle_download(fmt):\n",
    "            path, status = demo_assistant.download_response(fmt)\n",
    "            return gr.File.update(value=path), status\n",
    "        download_btn.click(handle_download, inputs=[download_format], outputs=[download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_ui().launch(server_name=\"0.0.0.0\", server_port=7192)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b46bd1bd-a439-47fc-9e0f-664cd07a3839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\1582274602.py:206: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:38:09,241 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:38:10,394 - INFO: HTTP Request: GET http://localhost:7193/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:38:12,428 - INFO: HTTP Request: HEAD http://localhost:7193/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7193/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:38:50,298 - INFO: Envoi requ√™te √† http://127.0.0.1:1234/v1/completions\n",
      "2025-04-15 16:39:42,683 - INFO: R√©ponse code 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# LM Studio configuration\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "LM_STUDIO_COMPLETIONS_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non support√©\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.history: List[Tuple[str, str]] = []\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, gr.update, str]:\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier s√©lectionn√©\"\n",
    "        try:\n",
    "            path = file.name if hasattr(file, 'name') else file\n",
    "            self.document_name = os.path.basename(path)\n",
    "            self.current_text = self.processor.extract_text_from_file(path)\n",
    "            preview = self.current_text[:1500] + (\"\\n\\n[...]\" if len(self.current_text) > 1500 else \"\")\n",
    "            size_kb = os.path.getsize(path) / 1024\n",
    "            info = f\"üìÑ {self.document_name} ({size_kb:.1f} KB, {len(self.current_text)} caract√®res)\"\n",
    "            return info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return f\"Erreur: {e}\", gr.update(visible=False), \"\"\n",
    "\n",
    "    def generate_response(self, message: str, chat_history: List[Tuple[str, str]], context_type: str) -> Tuple[List[Tuple[str, str]], str]:\n",
    "        if not message.strip():\n",
    "            return chat_history, \"\"\n",
    "        # Build prompt\n",
    "        prompt = \"\"\n",
    "        if context_type == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:2000]\n",
    "            prompt += f\"Contexte du document '{self.document_name}':\\n{excerpt}\\n[...\\n]\\n\\n\"\n",
    "        # Append last messages\n",
    "        for u, a in chat_history[-3:]:\n",
    "            prompt += f\"Utilisateur: {u}\\nAssistant: {a}\\n\"\n",
    "        prompt += f\"Utilisateur: {message}\\nAssistant:\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"stop\": [\"Utilisateur:\", \"Assistant:\"]\n",
    "        }\n",
    "        try:\n",
    "            logger.info(f\"Envoi requ√™te √† {LM_STUDIO_COMPLETIONS_ENDPOINT}\")\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_COMPLETIONS_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=120\n",
    "            )\n",
    "            logger.info(f\"R√©ponse code {resp.status_code}\")\n",
    "            if resp.status_code != 200:\n",
    "                logger.error(f\"API Error {resp.status_code}: {resp.text}\")\n",
    "                reply = \"Erreur API, v√©rifiez que LM Studio est en cours d'ex√©cution.\"\n",
    "            else:\n",
    "                data = resp.json()\n",
    "                reply = data.get('choices', [{}])[0].get('text', '').strip() or \"Aucune r√©ponse re√ßue.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur requ√™te: {e}\")\n",
    "            reply = f\"Erreur de connexion ou timeout: {e}\"\n",
    "\n",
    "        # Update both chat history and internal history\n",
    "        chat_history.append((message, reply))\n",
    "        self.history = chat_history\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    def download_response(self, file_format: str) -> Tuple[str, str]:\n",
    "        if not self.history:\n",
    "            return None, \"Aucun contenu √† t√©l√©charger.\"\n",
    "        content = self.history[-1][1]\n",
    "        if not content.strip():\n",
    "            return None, \"La r√©ponse est vide.\"\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", 11)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"R√©ponse g√©n√©r√©e\", 1)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                df = pd.DataFrame({\"Contenu\": content.split(\"\\n\")})\n",
    "                if file_format == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {file_format}\"\n",
    "            size = os.path.getsize(path)\n",
    "            return path, f\"‚úÖ Fichier '{name}' g√©n√©r√© ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export: {e}\")\n",
    "            return None, f\"Erreur export: {e}\"\n",
    "\n",
    "# Build UI\n",
    "demo_assistant = DocumentChatAssistant()\n",
    "\n",
    "def build_ui():\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üí¨ Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B (LM Studio)\n",
    "            \"\"\")\n",
    "\n",
    "        gr.HTML(f\"<p>Serveur local: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üìÑ Document source\")\n",
    "                            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"T√©l√©versez un document\")\n",
    "                            file_info = gr.Textbox(label=\"Informations du fichier\", interactive=False)\n",
    "                            with gr.Accordion(\"Aper√ßu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(label=\"Contenu du document\", lines=12, interactive=False)\n",
    "\n",
    "                            gr.Markdown(\"### ‚öôÔ∏è Options\")\n",
    "                            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode de g√©n√©ration\")\n",
    "\n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format d'export\")\n",
    "                                download_btn = gr.Button(\"üì• T√©l√©charger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(label=\"Statut\", interactive=False)\n",
    "                                download_file = gr.File(label=\"T√©l√©charger le fichier g√©n√©r√©\", interactive=False, type=\"filepath\", elem_classes=\"download-btn\")\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### üí¨ Conversation\")\n",
    "                            chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n",
    "                            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question sur votre document...\", lines=2)\n",
    "                            with gr.Row():\n",
    "                                send_btn = gr.Button(\"üí¨ Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "\n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### üìñ Guide d'utilisation\n",
    "\n",
    "                1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "                2. V√©rifiez que le serveur local (127.0.0.1:1234) est d√©marr√©\n",
    "                3. T√©l√©versez un document et posez vos questions\n",
    "                4. Exportez la r√©ponse si besoin\n",
    "                \"\"\")\n",
    "\n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>¬© 2025 Document Chat Assistant | Propuls√© par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(demo_assistant.process_document, inputs=[file_input], outputs=[file_info, preview_accordion, preview])\n",
    "        send_btn.click(demo_assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        user_input.submit(demo_assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "\n",
    "        def handle_download(fmt):\n",
    "            path, status = demo_assistant.download_response(fmt)\n",
    "            return gr.update(value=path), status\n",
    "\n",
    "        download_btn.click(handle_download, inputs=[download_format], outputs=[download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_ui().launch(server_name=\"0.0.0.0\", server_port=7193)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae24db8c-8ae2-4416-b406-3bc7456ef7c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflask\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flask, request, render_template_string\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from flask import Flask, request, render_template_string\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# URL du serveur LM Studio local\n",
    "API_URL = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "\n",
    "# Initialisation de l'application Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Fonction pour appeler l'API LM Studio\n",
    "def generate_response(prompt, max_tokens=256):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"mistral-7b-instruct-v0.3\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, data=json.dumps(payload))\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            return f\"Erreur lors de l'appel API: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Erreur de connexion au serveur LM Studio: {str(e)}\"\n",
    "\n",
    "# Route principale : formulaire et g√©n√©ration\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    story_text = \"\"\n",
    "    format_choice = \"gherkin\"\n",
    "    generated_test = None\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        # R√©cup√©rer la user story et le format depuis le formulaire\n",
    "        story_text = request.form.get(\"story\", \"\").strip()\n",
    "        format_choice = request.form.get(\"format\", \"gherkin\")\n",
    "        if story_text:\n",
    "            # Construire le prompt en fonction du format choisi\n",
    "            if format_choice == \"gherkin\":\n",
    "                prompt = (\n",
    "                    f\"Voici une user story : \\\"{story_text}\\\"\\n\"\n",
    "                    \"En tant qu'assistant de test, g√©n√®re un sc√©nario de test au format Gherkin \"\n",
    "                    \"(Given/When/Then) en fran√ßais.\"\n",
    "                )\n",
    "            else:  # format \"action\"\n",
    "                prompt = (\n",
    "                    f\"Voici une user story : \\\"{story_text}\\\"\\n\"\n",
    "                    \"En tant qu'assistant de test, g√©n√®re un cas de test d√©taillant les actions \"\n",
    "                    \"√† effectuer et les r√©sultats attendus pour chaque action, en fran√ßais.\"\n",
    "                )\n",
    "            # Appel au mod√®le local pour g√©n√©rer la suite de texte\n",
    "            generated_test = generate_response(prompt, max_tokens=256)\n",
    "\n",
    "    # Template HTML simple (int√©gr√© sous forme de cha√Æne)\n",
    "    html_template = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "      <meta charset=\"UTF-8\">\n",
    "      <title>G√©n√©ration de cas de test</title>\n",
    "      <style>\n",
    "        body { \n",
    "          font-family: Arial, sans-serif; \n",
    "          max-width: 800px; \n",
    "          margin: 0 auto; \n",
    "          padding: 20px; \n",
    "        }\n",
    "        textarea { width: 100%; }\n",
    "        pre { \n",
    "          background-color: #f5f5f5; \n",
    "          padding: 15px; \n",
    "          border-radius: 5px; \n",
    "          white-space: pre-wrap; \n",
    "        }\n",
    "        button { \n",
    "          background-color: #4CAF50; \n",
    "          color: white; \n",
    "          padding: 10px 15px; \n",
    "          border: none; \n",
    "          border-radius: 4px; \n",
    "          cursor: pointer; \n",
    "        }\n",
    "        button:hover { background-color: #45a049; }\n",
    "      </style>\n",
    "    </head>\n",
    "    <body>\n",
    "      <h1>G√©n√©rer des cas de test √† partir d'une User Story</h1>\n",
    "      <form method=\"post\">\n",
    "        <p>\n",
    "          <textarea name=\"story\" rows=\"6\" cols=\"60\" placeholder=\"Entrez la user story ici...\">{{ story|e }}</textarea>\n",
    "        </p>\n",
    "        <p>\n",
    "          Format de test : \n",
    "          <label><input type=\"radio\" name=\"format\" value=\"gherkin\" {% if format_choice != 'action' %}checked{% endif %}> Gherkin (Given/When/Then)</label>\n",
    "          <label><input type=\"radio\" name=\"format\" value=\"action\" {% if format_choice == 'action' %}checked{% endif %}> Actions / R√©sultats attendus</label>\n",
    "        </p>\n",
    "        <p><button type=\"submit\">G√©n√©rer</button></p>\n",
    "      </form>\n",
    "      {% if generated_test %}\n",
    "        <h2>Cas de test g√©n√©r√© :</h2>\n",
    "        <pre>{{ generated_test }}</pre>\n",
    "      {% endif %}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    # Rendre la page HTML avec les variables (story_text, format_choice, generated_test)\n",
    "    return render_template_string(html_template, story=story_text, format_choice=format_choice, generated_test=generated_test)\n",
    "\n",
    "# D√©marrer le serveur Flask si ce script est ex√©cut√© directement\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c36cc80a-ef62-40f6-bfb7-1d8f4b53edef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15827f1f-2c12-47ee-a490-7deacf08df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f2b738-b288-41fe-963e-96affcc7b6d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (24040424.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m ensurepip --upgrade\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m ensurepip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9b9499b-1e71-4be2-b8e4-24c9cc4c07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0f31d92-b5f5-49d2-9b78-e8631cb94899",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2372076907.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m pip install flask requests\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb57c3ad-ba0c-43d6-9c18-be1a4abc3a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0014d772-234d-42dc-bf93-ce7eeaad91e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2247650335.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython get-pip.py\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python get-pip.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e94d07-0fcc-41c1-abac-3263618668bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4783864c-0aae-4446-a981-065d09e12235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08687e9-1ce6-4768-a041-53485632c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efe408-3ed0-4438-968e-1083c7714652",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f300466-7d89-4b3d-9104-47169aab3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5121f4dc-676d-480a-8d74-0868084757a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d19a15-b973-4c5f-acd1-2b8edfc7a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31042d11-872e-4a4a-924f-9606760d282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3754c351-c743-4b8a-b252-b68e1506e0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28086cdf-b359-4705-840b-3425a08ae979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ffe83-a249-4299-b167-54fca0f06c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
