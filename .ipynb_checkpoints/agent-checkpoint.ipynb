{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7cd14fd-0fe0-4480-af12-932cb1fa96b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08393b45-0653-4364-818b-c02f661614de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for os\n"
     ]
    }
   ],
   "source": [
    "pip install os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff160d5-0c4d-42f0-bbed-ddc2e28c77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9ddccd-b6b8-4567-bd08-54dd3c03f46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 633.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 633.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 633.8 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 425.8 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 425.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 476.9 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 476.9 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 539.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.8 MB 583.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 647.7 kB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "Successfully installed pip-25.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1fb9994-6e92-4758-ad3f-644a22469b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8df34c3-b25f-4007-bb08-4fe53f9696b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mammoth in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: cobble<0.2,>=0.1.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mammoth) (0.1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mammoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbdb50a0-a888-4d12-8ec0-c02b318f8f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "606105ae-9772-4a7a-ab11-4fd1b1bb0678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d127d44b-19e7-4db0-9612-33f8bbbf6c42",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (1722728746.py, line 90)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpdf.output(file_path)\u001b[39m\n                         ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donné en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier téléchargé pour extraire son texte.\"\"\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        return self.current_document_text[:1000] + \"\\n\\n[... Texte tronqué pour la prévisualisation]\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:300]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel à l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la génération de la réponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"Télécharge la réponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf43f87-5840-446b-830e-7e5cbfcaad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5496\\41155677.py:138: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Chat\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:8033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 05:08:15,675 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:08:16,417 - INFO: HTTP Request: GET http://localhost:8033/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:08:18,478 - INFO: HTTP Request: HEAD http://localhost:8033/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:08:20,601 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://6a6b014a475eaac76c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 05:08:25,012 - INFO: HTTP Request: HEAD https://6a6b014a475eaac76c.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6a6b014a475eaac76c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donné en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier téléchargé pour extraire son texte.\"\"\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        return self.current_document_text[:1000] + \"\\n\\n[... Texte tronqué pour la prévisualisation]\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:300]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel à l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la génération de la réponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"Télécharge la réponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la création du fichier : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# 📄 Document Chat Assistant\", elem_classes=\"text-2xl font-bold text-blue-600 mb-4\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=300):\n",
    "                gr.Markdown(\"## Document Upload\", elem_classes=\"text-xl font-semibold\")\n",
    "                file_upload = gr.File(file_types=['.pdf', '.docx', '.txt'], label=\"Téléverser un document\")\n",
    "                file_preview = gr.Textbox(label=\"Aperçu du document\", lines=10)\n",
    "                context_radio = gr.Radio(choices=[\"Standard\", \"Avec contexte du document\"], value=\"Standard\", label=\"Mode de conversation\")\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                gr.Markdown(\"## Conversation\", elem_classes=\"text-xl font-semibold\")\n",
    "                chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
    "                msg = gr.Textbox(label=\"Votre message\")\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Envoyer\")\n",
    "                    clear_btn = gr.Button(\"Réinitialiser\")\n",
    "                    format_dropdown = gr.Dropdown(choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format de téléchargement\")\n",
    "                    download_btn = gr.Button(\"Télécharger la réponse\")\n",
    "                    response_file = gr.File()\n",
    "        \n",
    "        file_upload.upload(agent.process_document, inputs=[file_upload], outputs=[file_preview])\n",
    "        submit_btn.click(agent.generate_response, inputs=[msg, chatbot, context_radio], outputs=[chatbot])\n",
    "        download_btn.click(agent.download_response, inputs=[format_dropdown], outputs=[response_file])\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(server_name=\"0.0.0.0\", server_port=8033, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "086bc0d4-8668-4950-a82f-0e00e344ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5496\\3114139170.py:138: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Chat\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 05:44:37,679 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:44:38,561 - INFO: HTTP Request: GET http://localhost:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:44:40,610 - INFO: HTTP Request: HEAD http://localhost:7860/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-05 05:44:42,896 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://c759d952ff2df04b83.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 05:44:47,919 - INFO: HTTP Request: HEAD https://c759d952ff2df04b83.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c759d952ff2df04b83.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donné en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier téléchargé pour extraire son texte.\"\"\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        return self.current_document_text[:1000] + \"\\n\\n[... Texte tronqué pour la prévisualisation]\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:300]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel à l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la génération de la réponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"Télécharge la réponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la création du fichier : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# 📄 Document Chat Assistant\", elem_classes=\"text-2xl font-bold text-blue-600 mb-4\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=300):\n",
    "                gr.Markdown(\"## Document Upload\", elem_classes=\"text-xl font-semibold\")\n",
    "                file_upload = gr.File(file_types=['.pdf', '.docx', '.txt'], label=\"Téléverser un document\")\n",
    "                file_preview = gr.Textbox(label=\"Aperçu du document\", lines=10)\n",
    "                context_radio = gr.Radio(choices=[\"Standard\", \"Avec contexte du document\"], value=\"Standard\", label=\"Mode de conversation\")\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                gr.Markdown(\"## Conversation\", elem_classes=\"text-xl font-semibold\")\n",
    "                chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
    "                msg = gr.Textbox(label=\"Votre message\")\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Envoyer\")\n",
    "                    clear_btn = gr.Button(\"Réinitialiser\")\n",
    "                    format_dropdown = gr.Dropdown(choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format de téléchargement\")\n",
    "                    download_btn = gr.Button(\"Télécharger la réponse\")\n",
    "                    response_file = gr.File()\n",
    "        \n",
    "        file_upload.upload(agent.process_document, inputs=[file_upload], outputs=[file_preview])\n",
    "        submit_btn.click(agent.generate_response, inputs=[msg, chatbot, context_radio], outputs=[chatbot])\n",
    "        download_btn.click(agent.download_response, inputs=[format_dropdown], outputs=[response_file])\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(server_name=\"0.0.0.0\", server_port= 7860, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf38caa-b93e-4040-a9e3-3f04e5afbef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\1654830701.py:144: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"💬 Conversation\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:58:55,817 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 10:58:56,978 - INFO: HTTP Request: GET http://localhost:7865/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 10:58:59,011 - INFO: HTTP Request: HEAD http://localhost:7865/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 10:59:00,431 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://62117fb62850c0f2da.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:59:02,943 - INFO: HTTP Request: HEAD https://62117fb62850c0f2da.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://62117fb62850c0f2da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Chargement des variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "JIRA_USER_EMAIL = os.getenv(\"JIRA_USER_EMAIL\")\n",
    "JIRA_URL = os.getenv(\"JIRA_URL\")  # ex: https://ton-espace.atlassian.net\n",
    "JIRA_PROJECT_KEY = os.getenv(\"JIRA_PROJECT_KEY\")\n",
    "\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    return \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])[:max_chars]\n",
    "            elif extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    return mammoth.extract_raw_text(file).value[:max_chars]\n",
    "            elif extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'extraction : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "class JiraClient:\n",
    "    def __init__(self):\n",
    "        self.auth = (JIRA_USER_EMAIL, JIRA_API_TOKEN)\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_issue(self, summary: str, description: str) -> str:\n",
    "        try:\n",
    "            payload = {\n",
    "                \"fields\": {\n",
    "                    \"project\": {\"key\": JIRA_PROJECT_KEY},\n",
    "                    \"summary\": summary,\n",
    "                    \"description\": description,\n",
    "                    \"issuetype\": {\"name\": \"Task\"}\n",
    "                }\n",
    "            }\n",
    "            response = requests.post(f\"{JIRA_URL}/rest/api/3/issue\", json=payload, headers=self.headers, auth=self.auth)\n",
    "            response.raise_for_status()\n",
    "            return f\"Tâche créée avec succès : {response.json()['key']}\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de création de tâche : {e}\")\n",
    "            return f\"Erreur Jira : {e}\"\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.jira = JiraClient()\n",
    "        self.document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "\n",
    "    def process_file(self, file) -> str:\n",
    "        self.document_text = self.processor.extract_text_from_file(file.name)\n",
    "        return self.document_text[:1000] + \"\\n\\n[... Texte tronqué]\"\n",
    "\n",
    "    def generate_test_cases(self, prompt: str, chat_history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        try:\n",
    "            context = f\"Contexte:\\n{self.document_text[:300]}\\n\\n\" if context_type == \"Avec contexte\" and self.document_text else \"\"\n",
    "            for user_msg, assistant_msg in chat_history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {prompt}\\nAssistant: \"\n",
    "\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "\n",
    "            reply = result.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Erreur de réponse\")\n",
    "            self.last_response = reply\n",
    "            chat_history.append((prompt, reply))\n",
    "            return chat_history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            chat_history.append((prompt, f\"Erreur : {e}\"))\n",
    "            return chat_history\n",
    "\n",
    "    def download_test_cases(self, file_format: str) -> str:\n",
    "        filename = f\"test_cases.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                df = pd.DataFrame([line.split(\";\") for line in lines])\n",
    "                df.to_excel(filename, index=False, header=False)\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def send_to_jira(self) -> str:\n",
    "        summary = \"Cas de test généré automatiquement\"\n",
    "        return self.jira.create_issue(summary, self.last_response)\n",
    "\n",
    "def build_ui():\n",
    "    assistant = TestCaseGenerator()\n",
    "\n",
    "    with gr.Blocks(theme=gr.themes.Base()) as ui:\n",
    "        gr.Markdown(\"# 🤖 Générateur de cas de tests IA + Intégration Jira\", elem_classes=\"text-2xl font-bold mb-4\")\n",
    "\n",
    "        with gr.Row():\n",
    "            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"📄 Téléverser un document\")\n",
    "            preview = gr.Textbox(label=\"📃 Aperçu\", lines=10)\n",
    "        \n",
    "        context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode IA\")\n",
    "        chatbot = gr.Chatbot(label=\"💬 Conversation\")\n",
    "        user_input = gr.Textbox(label=\"Entrez votre message\")\n",
    "\n",
    "        with gr.Row():\n",
    "            send_btn = gr.Button(\"Envoyer\")\n",
    "            clear_btn = gr.Button(\"Réinitialiser\")\n",
    "\n",
    "        with gr.Row():\n",
    "            download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format export\")\n",
    "            download_btn = gr.Button(\"Télécharger\")\n",
    "            download_file = gr.File()\n",
    "\n",
    "        with gr.Row():\n",
    "            jira_btn = gr.Button(\"📌 Créer une tâche dans Jira\")\n",
    "            jira_result = gr.Textbox(label=\"Résultat Jira\")\n",
    "\n",
    "        file_input.upload(assistant.process_file, inputs=[file_input], outputs=[preview])\n",
    "        send_btn.click(assistant.generate_test_cases, inputs=[user_input, chatbot, context_mode], outputs=[chatbot])\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_test_cases, inputs=[download_format], outputs=[download_file])\n",
    "        jira_btn.click(assistant.send_to_jira, outputs=[jira_result])\n",
    "\n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7865, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e70a7a-f379-4ad3-9af6-1bdb0cd4e182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\2417973827.py:161: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"💬 Conversation\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 12:57:06,536 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 12:57:06,898 - INFO: HTTP Request: GET http://localhost:7863/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 12:57:08,945 - INFO: HTTP Request: HEAD http://localhost:7863/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-09 12:57:10,531 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://af4e23ccca0b2fc1ef.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 12:57:15,138 - INFO: HTTP Request: HEAD https://af4e23ccca0b2fc1ef.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://af4e23ccca0b2fc1ef.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    return \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])[:max_chars]\n",
    "            elif extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    return mammoth.extract_raw_text(file).value[:max_chars]\n",
    "            elif extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'extraction : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "class JiraClient:\n",
    "    def __init__(self, email: str, url: str):\n",
    "        self.email = email\n",
    "        self.url = url\n",
    "        self.auth = (self.email, JIRA_API_TOKEN)\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_issue(self, project_key: str, summary: str, description: str, issue_type: str = \"Task\") -> str:\n",
    "        try:\n",
    "            payload = {\n",
    "                \"fields\": {\n",
    "                    \"project\": {\"key\": project_key},\n",
    "                    \"summary\": summary,\n",
    "                    \"description\": description,\n",
    "                    \"issuetype\": {\"name\": issue_type}\n",
    "                }\n",
    "            }\n",
    "            response = requests.post(f\"{self.url}/rest/api/3/issue\", json=payload, headers=self.headers, auth=self.auth)\n",
    "            response.raise_for_status()\n",
    "            return f\"Tâche créée avec succès : {response.json()['key']}\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de création de tâche : {e}\")\n",
    "            return f\"Erreur Jira : {e}\"\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "        self.jira_email = \"\"\n",
    "        self.jira_url = \"\"\n",
    "        self.jira_project_key = \"\"\n",
    "        self.jira_issue_type = \"Task\"\n",
    "\n",
    "    def set_jira_credentials(self, email: str, url: str, project_key: str, issue_type: str):\n",
    "        self.jira_email = email\n",
    "        self.jira_url = url\n",
    "        self.jira_project_key = project_key\n",
    "        self.jira_issue_type = issue_type\n",
    "\n",
    "    def process_file(self, file) -> str:\n",
    "        self.document_text = self.processor.extract_text_from_file(file.name)\n",
    "        return self.document_text[:1000] + \"\\n\\n[... Texte tronqué]\"\n",
    "\n",
    "    def generate_test_cases(self, prompt: str, chat_history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        try:\n",
    "            context = f\"Contexte:\\n{self.document_text[:300]}\\n\\n\" if context_type == \"Avec contexte\" and self.document_text else \"\"\n",
    "            for user_msg, assistant_msg in chat_history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {prompt}\\nAssistant: \"\n",
    "\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "\n",
    "            reply = result.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Erreur de réponse\")\n",
    "            self.last_response = reply\n",
    "            chat_history.append((prompt, reply))\n",
    "            return chat_history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            chat_history.append((prompt, f\"Erreur : {e}\"))\n",
    "            return chat_history\n",
    "\n",
    "    def download_test_cases(self, file_format: str):\n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "\n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                df = pd.DataFrame([line.split(\";\") for line in lines])\n",
    "                df.to_excel(filename, index=False, header=False)\n",
    "\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def send_to_jira(self) -> str:\n",
    "        if not all([self.jira_email, self.jira_url, self.jira_project_key]):\n",
    "            return \"Veuillez remplir les identifiants Jira.\"\n",
    "        jira_client = JiraClient(email=self.jira_email, url=self.jira_url)\n",
    "        summary = \"Cas de test généré automatiquement\"\n",
    "        return jira_client.create_issue(self.jira_project_key, summary, self.last_response, self.jira_issue_type)\n",
    "\n",
    "def build_ui():\n",
    "    assistant = TestCaseGenerator()\n",
    "\n",
    "    with gr.Blocks(theme=gr.themes.Base()) as ui:\n",
    "        gr.Markdown(\"# 🧠 Générateur de cas de tests IA + Intégration Jira\", elem_classes=\"text-2xl font-bold mb-4\")\n",
    "\n",
    "        with gr.Row():\n",
    "            jira_email = gr.Textbox(label=\"Email User\")\n",
    "            jira_url = gr.Textbox(label=\"URL Jira (ex: https://ton-espace.atlassian.net)\")\n",
    "            jira_project = gr.Textbox(label=\"Clé du projet Jira\")\n",
    "            jira_type = gr.Dropdown(label=\"Type de tâche\", choices=[\"Task\", \"Bug\", \"Story\"], value=\"Task\")\n",
    "            set_jira = gr.Button(\"Valider les identifiants\")\n",
    "\n",
    "        with gr.Row():\n",
    "            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"📄 Téléverser un document\")\n",
    "            preview = gr.Textbox(label=\"📜 Aperçu\", lines=10)\n",
    "\n",
    "        context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode IA\")\n",
    "        chatbot = gr.Chatbot(label=\"💬 Conversation\")\n",
    "        user_input = gr.Textbox(label=\"Entrez votre message\")\n",
    "\n",
    "        with gr.Row():\n",
    "            send_btn = gr.Button(\"Envoyer\")\n",
    "            clear_btn = gr.Button(\"Réinitialiser\")\n",
    "\n",
    "        with gr.Row():\n",
    "            download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format export\")\n",
    "            download_btn = gr.Button(\"Télécharger\")\n",
    "            download_file = gr.File(label=\"Fichier exporté\", interactive=False)\n",
    "\n",
    "        with gr.Row():\n",
    "            jira_btn = gr.Button(\"📌 Créer une tâche dans Jira\")\n",
    "            jira_result = gr.Textbox(label=\"Résultat Jira\")\n",
    "\n",
    "        set_jira.click(assistant.set_jira_credentials, inputs=[jira_email, jira_url, jira_project, jira_type])\n",
    "        file_input.upload(assistant.process_file, inputs=[file_input], outputs=[preview])\n",
    "        send_btn.click(assistant.generate_test_cases, inputs=[user_input, chatbot, context_mode], outputs=[chatbot])\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_test_cases, inputs=[download_format], outputs=[download_file])\n",
    "        jira_btn.click(assistant.send_to_jira, outputs=[jira_result])\n",
    "\n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7863, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2b0573-5437-4a82-9e2a-dd45a3d35f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\3105611391.py:347: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 07:15:53,395 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 07:15:53,947 - INFO: HTTP Request: GET http://localhost:7866/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 07:15:56,013 - INFO: HTTP Request: HEAD http://localhost:7866/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 07:15:58,112 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://2837ce49d40679e548.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 07:16:12,951 - INFO: HTTP Request: HEAD https://2837ce49d40679e548.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2837ce49d40679e548.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Variables d'environnement\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte de différents formats de fichiers\"\"\"\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    return \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])[:max_chars]\n",
    "            elif extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    return mammoth.extract_raw_text(file).value[:max_chars]\n",
    "            elif extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'extraction : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "class JiraClient:\n",
    "    def __init__(self, email: str, url: str):\n",
    "        self.email = email\n",
    "        self.url = url\n",
    "        self.auth = (self.email, JIRA_API_TOKEN)\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_issue(self, project_key: str, summary: str, description: str, issue_type: str = \"Task\") -> dict:\n",
    "        \"\"\"Crée une issue dans Jira et renvoie le résultat et le statut\"\"\"\n",
    "        try:\n",
    "            payload = {\n",
    "                \"fields\": {\n",
    "                    \"project\": {\"key\": project_key},\n",
    "                    \"summary\": summary,\n",
    "                    \"description\": description,\n",
    "                    \"issuetype\": {\"name\": issue_type}\n",
    "                }\n",
    "            }\n",
    "            response = requests.post(f\"{self.url}/rest/api/3/issue\", json=payload, headers=self.headers, auth=self.auth)\n",
    "            response.raise_for_status()\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"Tâche créée avec succès : {response.json()['key']}\",\n",
    "                \"key\": response.json()['key']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de création de tâche : {e}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"message\": f\"Erreur Jira : {str(e)}\"\n",
    "            }\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "        self.jira_email = \"\"\n",
    "        self.jira_url = \"\"\n",
    "        self.jira_project_key = \"\"\n",
    "        self.jira_issue_type = \"Task\"\n",
    "        self.processing = False\n",
    "\n",
    "    def set_jira_credentials(self, email: str, url: str, project_key: str, issue_type: str):\n",
    "        \"\"\"Configure les identifiants Jira\"\"\"\n",
    "        self.jira_email = email\n",
    "        self.jira_url = url\n",
    "        self.jira_project_key = project_key\n",
    "        self.jira_issue_type = issue_type\n",
    "        \n",
    "        # Vérification basique\n",
    "        valid = all([email, url, project_key])\n",
    "        message = \"✅ Identifiants Jira enregistrés\" if valid else \"❌ Veuillez compléter tous les champs requis\"\n",
    "        return message\n",
    "\n",
    "    def process_file(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.document_text = self.processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Préparer l'aperçu (limité)\n",
    "            preview = self.document_text[:1500]\n",
    "            if len(self.document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqué]\"\n",
    "                \n",
    "            char_count = len(self.document_text)\n",
    "            file_info = f\"📄 Fichier : {self.document_name} | 📊 {char_count} caractères\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_test_cases(self, prompt: str, chat_history: List[Tuple[str, str]], context_type: str, generation_type: str):\n",
    "        \"\"\"Génère des cas de tests en fonction du prompt et de l'historique\"\"\"\n",
    "        if not prompt:\n",
    "            chat_history.append((\"\", \"Veuillez entrer une requête.\"))\n",
    "            return chat_history, gr.update(value=\"\")\n",
    "        \n",
    "        try:\n",
    "            self.processing = True\n",
    "            \n",
    "            # Construction du contexte\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.document_text[:2000]}\\n\\n\"\n",
    "                if len(self.document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqué pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Construction de l'instruction basée sur le type de génération\n",
    "            system_instruction = \"Tu es un expert en test logiciel. \"\n",
    "            if generation_type == \"Cas de test fonctionnels\":\n",
    "                system_instruction += \"Génère des cas de test fonctionnels détaillés avec préconditions, étapes et résultats attendus.\"\n",
    "            elif generation_type == \"Tests d'acceptation\":\n",
    "                system_instruction += \"Génère des tests d'acceptation au format Gherkin (Given-When-Then).\"\n",
    "            elif generation_type == \"Scénarios de test BDD\":\n",
    "                system_instruction += \"Crée des scénarios de tests comportementaux détaillés.\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in chat_history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{system_instruction}\\n\\n{context}Utilisateur: {prompt}\\nAssistant: \"\n",
    "            \n",
    "            # Appel à l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            reply = result.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Erreur de réponse\")\n",
    "            self.last_response = reply\n",
    "            chat_history.append((prompt, reply))\n",
    "            \n",
    "            self.processing = False\n",
    "            return chat_history, gr.update(value=\"\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            chat_history.append((prompt, f\"Erreur : {str(e)}\"))\n",
    "            self.processing = False\n",
    "            return chat_history, gr.update(value=\"\")\n",
    "\n",
    "    def download_test_cases(self, file_format: str):\n",
    "        \"\"\"Génère un fichier téléchargeable au format spécifié\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu à télécharger. Générez d'abord des cas de test.\"\n",
    "        \n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "            \n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"cas_de_test_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traitement du texte pour FPDF (éviter les problèmes d'encodage)\n",
    "                clean_text = self.last_response.encode('latin-1', 'replace').decode('latin-1')\n",
    "                pdf.multi_cell(190, 7, clean_text)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"Cas de Tests Générés\", level=1)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter une conversion en tableau simple\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Créer des colonnes par défaut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_excel(filename, index=False)\n",
    "            \n",
    "            return (filename, display_name), \"✅ Fichier généré avec succès\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"❌ Erreur lors de la génération du fichier : {str(e)}\"\n",
    "\n",
    "    def send_to_jira(self, title: str):\n",
    "        \"\"\"Envoie les cas de test à Jira\"\"\"\n",
    "        if not self.last_response:\n",
    "            return \"❌ Aucun contenu à envoyer à Jira. Générez d'abord des cas de test.\"\n",
    "        \n",
    "        if not all([self.jira_email, self.jira_url, self.jira_project_key]):\n",
    "            return \"❌ Veuillez configurer vos identifiants Jira avant de créer une tâche.\"\n",
    "        \n",
    "        if not title:\n",
    "            title = f\"Cas de tests générés ({datetime.now().strftime('%d/%m/%Y')})\"\n",
    "        \n",
    "        jira_client = JiraClient(email=self.jira_email, url=self.jira_url)\n",
    "        result = jira_client.create_issue(self.jira_project_key, title, self.last_response, self.jira_issue_type)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            return f\"✅ {result['message']}\"\n",
    "        else:\n",
    "            return f\"❌ {result['message']}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = TestCaseGenerator()\n",
    "    \n",
    "    # Thème personnalisé - CORRIGÉ pour supprimer la propriété checkbox_text_color_selected\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"indigo\",\n",
    "        secondary_hue=\"blue\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"TestCaseGenius\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 🧪 TestCaseGenius\n",
    "            ### Générateur intelligent de cas de tests avec intégration Jira\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Générateur de tests\", id=\"generator\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Remplacement de gr.Box par gr.Group avec div CSS\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"Téléversez un document de spécification (optionnel)\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document chargé\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de génération\"\n",
    "                                )\n",
    "                                generation_type = gr.Radio(\n",
    "                                    [\"Cas de test fonctionnels\", \"Tests d'acceptation\", \"Scénarios de test BDD\"],\n",
    "                                    value=\"Cas de test fonctionnels\",\n",
    "                                    label=\"Type de tests\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"Télécharger le fichier généré\", \n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Remplacement de gr.Box par gr.Group avec div CSS\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Générateur de cas de tests\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Demandez à l'IA de générer des cas de tests\",\n",
    "                                    placeholder=\"Exemple: Génère des cas de test pour une fonctionnalité de connexion\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"💬 Générer les cas de test\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Intégration Jira\", id=\"jira\", elem_classes=\"tab-nav\"):\n",
    "                # Remplacement de gr.Box par gr.Group avec div CSS\n",
    "                with gr.Group(elem_classes=\"container\"):\n",
    "                    gr.Markdown(\"### 🔐 Paramètres de connexion Jira\")\n",
    "                    with gr.Row():\n",
    "                        jira_email = gr.Textbox(\n",
    "                            label=\"Email Jira\",\n",
    "                            placeholder=\"votre.email@entreprise.com\"\n",
    "                        )\n",
    "                        jira_url = gr.Textbox(\n",
    "                            label=\"URL Jira\",\n",
    "                            placeholder=\"https://votre-espace.atlassian.net\"\n",
    "                        )\n",
    "                    with gr.Row():\n",
    "                        jira_project = gr.Textbox(\n",
    "                            label=\"Clé du projet Jira\",\n",
    "                            placeholder=\"Ex: PROJ\"\n",
    "                        )\n",
    "                        jira_type = gr.Dropdown(\n",
    "                            label=\"Type de tâche\", \n",
    "                            choices=[\"Task\", \"Bug\", \"Story\", \"Test Case\", \"Epic\"],\n",
    "                            value=\"Task\"\n",
    "                        )\n",
    "                    set_jira = gr.Button(\"💾 Enregistrer les paramètres\", variant=\"primary\")\n",
    "                    jira_creds_status = gr.Textbox(\n",
    "                        label=\"Statut\",\n",
    "                        interactive=False\n",
    "                    )\n",
    "                    \n",
    "                    gr.Markdown(\"### 📌 Création de tâche Jira\")\n",
    "                    jira_title = gr.Textbox(\n",
    "                        label=\"Titre de la tâche\",\n",
    "                        placeholder=\"Titre descriptif pour votre tâche Jira\"\n",
    "                    )\n",
    "                    jira_btn = gr.Button(\"🚀 Créer une tâche dans Jira\", variant=\"primary\")\n",
    "                    jira_result = gr.Textbox(\n",
    "                        label=\"Résultat\",\n",
    "                        interactive=False\n",
    "                    )\n",
    "                \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "                \n",
    "                **TestCaseGenius** est un outil qui utilise l'IA pour générer des cas de tests à partir de vos spécifications.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **Téléchargez un document** (optionnel) \n",
    "                   - Formats supportés : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte à l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : génération sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme référence\n",
    "                   - **Type de tests** : choisissez le format de test souhaité\n",
    "                \n",
    "                3. **Demandez à l'IA**\n",
    "                   - Soyez précis dans vos requêtes\n",
    "                   - Exemple : \"Génère des cas de test pour la fonctionnalité de réinitialisation de mot de passe\"\n",
    "                \n",
    "                4. **Exportez le résultat**\n",
    "                   - Téléchargez au format TXT, PDF, DOCX ou XLSX\n",
    "                   - Envoyez directement vers Jira (configuration requise)\n",
    "                \n",
    "                5. **Configuration Jira**\n",
    "                   - Renseignez vos identifiants dans l'onglet \"Intégration Jira\"\n",
    "                   - La variable d'environnement JIRA_API_TOKEN doit être configurée\n",
    "                \n",
    "                #### Exemples de requêtes efficaces :\n",
    "                \n",
    "                - \"Crée 5 cas de test pour valider un formulaire d'inscription\"\n",
    "                - \"Génère des tests de non-régression pour une API de paiement\"\n",
    "                - \"Écris des tests d'acceptation pour la fonctionnalité de recherche avancée\"\n",
    "                - \"Développe des scénarios BDD pour un panier d'achat e-commerce\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 TestCaseGenius | Propulsé par IA | Créé pour les équipes QA</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des événements\n",
    "        file_input.upload(\n",
    "            assistant.process_file, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_test_cases, \n",
    "            inputs=[user_input, chatbot, context_mode, generation_type], \n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_test_cases, \n",
    "            inputs=[user_input, chatbot, context_mode, generation_type], \n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_test_cases, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "        set_jira.click(\n",
    "            assistant.set_jira_credentials, \n",
    "            inputs=[jira_email, jira_url, jira_project, jira_type], \n",
    "            outputs=[jira_creds_status]\n",
    "        )\n",
    "        \n",
    "        jira_btn.click(\n",
    "            assistant.send_to_jira, \n",
    "            inputs=[jira_title], \n",
    "            outputs=[jira_result]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7866, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef33d76-9e79-4e6f-a94d-feed75d9f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\4248759264.py:278: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:29:13,625 - INFO: HTTP Request: GET http://localhost:7869/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:29:14,836 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:29:15,715 - INFO: HTTP Request: HEAD http://localhost:7869/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:29:17,800 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://dd3272df300909736f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:29:22,511 - INFO: HTTP Request: HEAD https://dd3272df300909736f.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dd3272df300909736f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Préparer l'aperçu (limité)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqué]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"📄 Fichier : {self.document_name} | 📊 {char_count} caractères\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse basée sur le prompt et l'historique de conversation\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqué pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel à l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            if \"candidates\" in result and result[\"candidates\"]:\n",
    "                raw_reply = result[\"candidates\"][0].get(\"content\", {})\n",
    "                if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                    reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "                else:\n",
    "                    reply = \"Erreur: Format de réponse inattendu\"\n",
    "            else:\n",
    "                reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            self.last_response = reply\n",
    "            history.append((message, reply))\n",
    "            \n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append((message, f\"Erreur : {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"Génère un fichier téléchargeable au format spécifié\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu à télécharger. Générez d'abord une réponse.\"\n",
    "        \n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "            \n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"reponse_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traitement du texte pour FPDF (éviter les problèmes d'encodage)\n",
    "                clean_text = self.last_response.encode('latin-1', 'replace').decode('latin-1')\n",
    "                pdf.multi_cell(190, 7, clean_text)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"Réponse générée\", level=1)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter une conversion en tableau simple\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Créer des colonnes par défaut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_excel(filename, index=False)\n",
    "            elif file_format == \"csv\":\n",
    "                # Tenter une conversion en CSV\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Créer des colonnes par défaut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_csv(filename, index=False)\n",
    "            \n",
    "            return (filename, display_name), \"✅ Fichier généré avec succès\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"❌ Erreur lors de la génération du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Thème personnalisé - inspiré de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"Téléversez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document chargé\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de génération\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"Télécharger le fichier généré\", \n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents grâce à l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **Téléchargez un document**\n",
    "                   - Formats supportés : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte à l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme référence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez précis dans vos requêtes\n",
    "                   - Exemple : \"Peux-tu résumer ce document ?\" ou \"Quels sont les points principaux abordés ?\"\n",
    "                \n",
    "                4. **Exportez le résultat**\n",
    "                   - Téléchargez les réponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requêtes efficaces :\n",
    "                \n",
    "                - \"Résume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations clés présentées dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des événements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7869, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5b915a-6fb8-494e-a311-bafdfc668395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:53:01,188 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:53:01,824 - INFO: HTTP Request: GET http://localhost:7870/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 10:53:04,156 - INFO: HTTP Request: HEAD http://localhost:7870/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from openpyxl import Workbook\n",
    "from gemini_api import generate_test_cases_with_gemini\n",
    "\n",
    "def generate_test_cases(input_text, file_format, project_name, jira_link):\n",
    "    # Générer les cas de test à l'aide de l'API Gemini\n",
    "    test_cases = generate_test_cases_with_gemini(input_text, project_name, jira_link)\n",
    "    \n",
    "    # Déterminer le dossier de sortie\n",
    "    output_dir = Path(\"generated_files\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Générer le nom du fichier avec horodatage\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if file_format == \"Excel\":\n",
    "        output_file_path = output_dir / f\"{project_name}_test_cases_{timestamp}.xlsx\"\n",
    "        df = pd.DataFrame(test_cases)\n",
    "        df.to_excel(output_file_path, index=False)\n",
    "        return output_file_path\n",
    "\n",
    "    elif file_format == \"CSV\":\n",
    "        output_file_path = output_dir / f\"{project_name}_test_cases_{timestamp}.csv\"\n",
    "        df = pd.DataFrame(test_cases)\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "        return output_file_path\n",
    "\n",
    "    elif file_format == \"JSON\":\n",
    "        output_file_path = output_dir / f\"{project_name}_test_cases_{timestamp}.json\"\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(test_cases, f, indent=4, ensure_ascii=False)\n",
    "        return output_file_path\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Format de fichier non supporté.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31d45c99-35d9-4a3e-a98d-914163891652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:09:25,979 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:09:27,045 - INFO: HTTP Request: GET http://localhost:7871/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:09:29,107 - INFO: HTTP Request: HEAD http://localhost:7871/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Logger config\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join([p.extract_text() or \"\" for p in reader.pages])\n",
    "                return text[:max_chars]\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    result = mammoth.extract_raw_text(f)\n",
    "                return result.value[:max_chars]\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    return f.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "\n",
    "    def process_document(self, file):\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        try:\n",
    "            self.document_name = os.path.basename(file.name)\n",
    "            self.current_document_text = self.processor.extract_text_from_file(file.name)\n",
    "\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqué]\"\n",
    "\n",
    "            return (\n",
    "                f\"📄 {self.document_name} | {len(self.current_document_text)} caractères\",\n",
    "                gr.update(visible=True),\n",
    "                preview\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur : {e}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Dict[str, str]], context_type: str):\n",
    "        if not message:\n",
    "            history.append({\"role\": \"assistant\", \"content\": \"Veuillez entrer un message.\"})\n",
    "            return history\n",
    "\n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context += f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Document tronqué]\\n\\n\"\n",
    "\n",
    "            for msg in history:\n",
    "                role = \"Utilisateur\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "                context += f\"{role}: {msg['content']}\\n\"\n",
    "\n",
    "            prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=payload, headers=headers)\n",
    "            data = response.json()\n",
    "\n",
    "            reply = \"Réponse indisponible.\"\n",
    "            if \"candidates\" in data and data[\"candidates\"]:\n",
    "                candidate = data[\"candidates\"][0]\n",
    "                if \"content\" in candidate and \"parts\" in candidate[\"content\"]:\n",
    "                    reply = candidate[\"content\"][\"parts\"][0].get(\"text\", reply)\n",
    "\n",
    "            self.last_response = reply\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Erreur : {e}\"})\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucune réponse disponible à exporter.\"\n",
    "\n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "\n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                clean_text = self.last_response.encode(\"latin-1\", \"replace\").decode(\"latin-1\")\n",
    "                pdf.multi_cell(190, 8, clean_text)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"Réponse générée\", 0)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                lines = [line.strip() for line in self.last_response.splitlines() if line.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                if file_format == \"xlsx\":\n",
    "                    df.to_excel(filename, index=False)\n",
    "                else:\n",
    "                    df.to_csv(filename, index=False)\n",
    "\n",
    "            return (filename, os.path.basename(filename)), \"✅ Fichier généré avec succès\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"❌ Erreur : {e}\"\n",
    "\n",
    "\n",
    "def build_ui():\n",
    "    assistant = DocumentChatAssistant()\n",
    "    theme = gr.themes.Soft(primary_hue=\"blue\").set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_text_color=\"white\",\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme) as ui:\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\"# 💬 Document Chat Assistant\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.Tab(\"📄 Chat avec documents\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"])\n",
    "                        file_info = gr.Textbox(label=\"Fichier\", interactive=False)\n",
    "                        with gr.Accordion(\"Aperçu\", visible=False) as preview_accordion:\n",
    "                            preview = gr.Textbox(label=\"Aperçu\", lines=10, interactive=False)\n",
    "                        context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode\")\n",
    "\n",
    "                        with gr.Row():\n",
    "                            download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"pdf\")\n",
    "                            download_btn = gr.Button(\"Télécharger\")\n",
    "                        download_file = gr.File()\n",
    "                        download_status = gr.Textbox(visible=True, interactive=False)\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        chatbot = gr.Chatbot(label=\"Conversation\", height=400, type=\"messages\")\n",
    "                        user_input = gr.Textbox(label=\"Votre question\")\n",
    "                        with gr.Row():\n",
    "                            send_btn = gr.Button(\"💬 Envoyer\")\n",
    "                            clear_btn = gr.Button(\"🧹 Effacer\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(assistant.process_document, [file_input], [file_info, preview_accordion, preview])\n",
    "        send_btn.click(assistant.generate_response, [user_input, chatbot, context_mode], [chatbot]).then(\n",
    "            lambda: \"\", None, [user_input]\n",
    "        )\n",
    "        user_input.submit(assistant.generate_response, [user_input, chatbot, context_mode], [chatbot]).then(\n",
    "            lambda: \"\", None, [user_input]\n",
    "        )\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_response, [download_format], [download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7871)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "842cc5c5-e932-4eb4-999d-a1a42341af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\301940393.py:165: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation IA\", show_copy_button=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:34:01,115 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:34:02,238 - INFO: HTTP Request: GET http://localhost:7872/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:34:04,296 - INFO: HTTP Request: HEAD http://localhost:7872/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 11:34:05,542 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://5186faeea363d25555.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:34:08,432 - INFO: HTTP Request: HEAD https://5186faeea363d25555.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5186faeea363d25555.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    return \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])[:max_chars]\n",
    "            elif extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    return mammoth.extract_raw_text(file).value[:max_chars]\n",
    "            elif extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'extraction : {e}\")\n",
    "            return f\"Erreur : {e}\"\n",
    "\n",
    "class JiraClient:\n",
    "    def __init__(self, email: str, url: str):\n",
    "        self.email = email\n",
    "        self.url = url\n",
    "        self.auth = (self.email, JIRA_API_TOKEN)\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_issue(self, project_key: str, summary: str, description: str, issue_type: str = \"Task\") -> str:\n",
    "        try:\n",
    "            payload = {\n",
    "                \"fields\": {\n",
    "                    \"project\": {\"key\": project_key},\n",
    "                    \"summary\": summary,\n",
    "                    \"description\": description,\n",
    "                    \"issuetype\": {\"name\": issue_type}\n",
    "                }\n",
    "            }\n",
    "            response = requests.post(f\"{self.url}/rest/api/3/issue\", json=payload, headers=self.headers, auth=self.auth)\n",
    "            response.raise_for_status()\n",
    "            return f\"Tâche créée avec succès : {response.json()['key']}\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de création de tâche : {e}\")\n",
    "            return f\"Erreur Jira : {e}\"\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "        self.jira_email = \"\"\n",
    "        self.jira_url = \"\"\n",
    "        self.jira_project_key = \"\"\n",
    "        self.jira_issue_type = \"Task\"\n",
    "\n",
    "    def set_jira_credentials(self, email: str, url: str, project_key: str, issue_type: str):\n",
    "        self.jira_email = email\n",
    "        self.jira_url = url\n",
    "        self.jira_project_key = project_key\n",
    "        self.jira_issue_type = issue_type\n",
    "\n",
    "    def process_file(self, file) -> str:\n",
    "        self.document_text = self.processor.extract_text_from_file(file.name)\n",
    "        return self.document_text[:1000] + \"\\n\\n[... Texte tronqué]\"\n",
    "\n",
    "    def generate_test_cases(self, prompt: str, chat_history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        try:\n",
    "            context = f\"Contexte:\\n{self.document_text[:300]}\\n\\n\" if context_type == \"Avec contexte\" and self.document_text else \"\"\n",
    "            for user_msg, assistant_msg in chat_history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {prompt}\\nAssistant: \"\n",
    "\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "\n",
    "            reply = result.get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Erreur de réponse\")\n",
    "            self.last_response = reply\n",
    "            chat_history.append((prompt, reply))\n",
    "            return chat_history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            chat_history.append((prompt, f\"Erreur : {e}\"))\n",
    "            return chat_history\n",
    "\n",
    "    def download_test_cases(self, file_format: str):\n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "\n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                df = pd.DataFrame([line.split(\";\") for line in lines])\n",
    "                df.to_excel(filename, index=False, header=False)\n",
    "\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def send_to_jira(self) -> str:\n",
    "        if not all([self.jira_email, self.jira_url, self.jira_project_key]):\n",
    "            return \"Veuillez remplir les identifiants Jira.\"\n",
    "        jira_client = JiraClient(email=self.jira_email, url=self.jira_url)\n",
    "        summary = \"Cas de test généré automatiquement\"\n",
    "        return jira_client.create_issue(self.jira_project_key, summary, self.last_response, self.jira_issue_type)\n",
    "\n",
    "def build_ui():\n",
    "    assistant = TestCaseGenerator()\n",
    "\n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"cyan\", secondary_hue=\"indigo\")) as ui:\n",
    "        gr.Markdown(\"\"\"<h1 style='text-align: center;'>🧠 Générateur de cas de tests IA + Intégration Jira</h1>\"\"\")\n",
    "\n",
    "        with gr.Accordion(\"🔐 Identifiants Jira\", open=True):\n",
    "            with gr.Row():\n",
    "                jira_email = gr.Textbox(label=\"Email Jira\", placeholder=\"nom@exemple.com\")\n",
    "                jira_url = gr.Textbox(label=\"URL Jira\", placeholder=\"https://ton-espace.atlassian.net\")\n",
    "            with gr.Row():\n",
    "                jira_project = gr.Textbox(label=\"Clé du projet Jira\", placeholder=\"ABC\")\n",
    "                jira_type = gr.Dropdown(label=\"Type de tâche\", choices=[\"Task\", \"Bug\", \"Story\"], value=\"Task\")\n",
    "                set_jira = gr.Button(\"✅ Valider\")\n",
    "\n",
    "        with gr.Accordion(\"📄 Importer un document\", open=True):\n",
    "            with gr.Row():\n",
    "                file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"Téléverser un fichier\")\n",
    "                preview = gr.Textbox(label=\"Aperçu du contenu\", lines=12)\n",
    "\n",
    "        with gr.Accordion(\"💬 Génération de cas de tests\", open=True):\n",
    "            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode IA\")\n",
    "            chatbot = gr.Chatbot(label=\"Conversation IA\", show_copy_button=True)\n",
    "            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question ou demandez un cas de test\")\n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"🚀 Envoyer\")\n",
    "                clear_btn = gr.Button(\"🧹 Réinitialiser\")\n",
    "\n",
    "        with gr.Accordion(\"📥 Export & Jira\", open=True):\n",
    "            with gr.Row():\n",
    "                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\"], value=\"txt\", label=\"Format export\")\n",
    "                download_btn = gr.Button(\"⬇️ Télécharger\")\n",
    "                download_file = gr.File(label=\"Fichier exporté\", interactive=False)\n",
    "            with gr.Row():\n",
    "                jira_btn = gr.Button(\"📌 Créer une tâche dans Jira\")\n",
    "                jira_result = gr.Textbox(label=\"Résultat Jira\")\n",
    "\n",
    "        set_jira.click(assistant.set_jira_credentials, inputs=[jira_email, jira_url, jira_project, jira_type])\n",
    "        file_input.upload(assistant.process_file, inputs=[file_input], outputs=[preview])\n",
    "        send_btn.click(assistant.generate_test_cases, inputs=[user_input, chatbot, context_mode], outputs=[chatbot])\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_test_cases, inputs=[download_format], outputs=[download_file])\n",
    "        jira_btn.click(assistant.send_to_jira, outputs=[jira_result])\n",
    "\n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7872, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24c6206f-9f5d-45a3-bd5c-327688645ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:41:49,727 - CRITICAL: Erreur de lancement : module 'gradio' has no attribute 'Box'\n",
      "2025-04-10 11:41:51,005 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "# Thème personnalisé pour l'UI\n",
    "custom_theme = gr.themes.Base(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"purple\",\n",
    "    neutral_hue=\"slate\",\n",
    "    radius_size=gr.themes.sizes.radius_md,\n",
    "    font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    ").set(\n",
    "    body_background_fill=\"linear-gradient(to right, #f8f9fa, #f1f3f9)\",\n",
    "    block_background_fill=\"#ffffff\",\n",
    "    block_shadow=\"0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)\",\n",
    "    button_primary_background_fill=\"*primary_500\",\n",
    "    button_primary_background_fill_hover=\"*primary_600\",\n",
    "    button_primary_text_color=\"white\",\n",
    "    button_secondary_background_fill=\"*neutral_100\",\n",
    "    button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "    button_secondary_text_color=\"*neutral_800\",\n",
    "    input_background_fill=\"*neutral_50\",\n",
    "    input_border_color=\"*neutral_200\",\n",
    "    input_border_color_focus=\"*primary_500\",\n",
    "    input_shadow_focus=\"0 0 0 3px rgba(99, 102, 241, 0.2)\",\n",
    ")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Classe pour traiter et extraire du texte à partir de différents formats de documents.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donné en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "                    \n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "                    \n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "                    \n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    \"\"\"Assistant de chat intelligent basé sur l'API Gemini avec support de documents.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "        self.document_name = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> Tuple[str, str]:\n",
    "        \"\"\"Traite le fichier téléchargé pour extraire son texte.\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", \"Aucun document chargé\"\n",
    "            \n",
    "        self.document_name = os.path.basename(file.name)\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        \n",
    "        preview = self.current_document_text[:1000]\n",
    "        if len(self.current_document_text) > 1000:\n",
    "            preview += \"\\n\\n[... Texte tronqué pour la prévisualisation]\"\n",
    "            \n",
    "        return self.document_name, preview\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "            \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:5000]}\\n\\n\"\n",
    "            \n",
    "            # Construire le contexte avec l'historique\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel à l'API Gemini\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            # Simulation de chargement\n",
    "            for i in range(3):\n",
    "                yield history + [(message, f\"Génération de la réponse{i*'.'}\")], gr.update(interactive=False)\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                error_msg = response_data.get(\"error\", {}).get(\"message\", \"Désolé, je n'ai pas pu générer de réponse.\")\n",
    "                raw_reply = f\"Erreur: {error_msg}\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            \n",
    "            return history, gr.update(interactive=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel à l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la génération de la réponse : {e}\"))\n",
    "            return history, gr.update(interactive=True)\n",
    "    \n",
    "    def download_response(self, file_format: str) -> Tuple[str, str]:\n",
    "        \"\"\"Télécharge la réponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucune réponse à télécharger\"\n",
    "            \n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        file_path = f\"response_{timestamp}.{file_format}\"\n",
    "        \n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                \n",
    "                # Diviser le texte en lignes pour éviter le dépassement\n",
    "                lines = self.last_response.split('\\n')\n",
    "                for line in lines:\n",
    "                    # Découper les lignes trop longues\n",
    "                    while len(line) > 0:\n",
    "                        chunk = line[:80]  # Environ 80 caractères par ligne\n",
    "                        pdf.multi_cell(190, 10, chunk)\n",
    "                        line = line[80:]\n",
    "            \n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"Réponse du Document Chat Assistant\", level=1)\n",
    "                doc.add_paragraph(f\"Généré le: {time.strftime('%d/%m/%Y à %H:%M:%S')}\")\n",
    "                doc.add_paragraph(\"\")\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path, f\"Réponse téléchargée au format {file_format.upper()}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la création du fichier : {e}\")\n",
    "            return None, f\"Erreur: {str(e)}\"\n",
    "\n",
    "    def reset_chat(self):\n",
    "        \"\"\"Réinitialise la conversation.\"\"\"\n",
    "        return [], \"\"\n",
    "\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    \"\"\"Crée l'interface utilisateur Gradio avec un design moderne.\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    with gr.Blocks(theme=custom_theme, css=\"\"\"\n",
    "        .container { max-width: 1200px; margin: 0 auto; }\n",
    "        .header { text-align: center; margin-bottom: 1.5rem; }\n",
    "        .header h1 { background: linear-gradient(to right, #4f46e5, #7c3aed); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }\n",
    "        .doc-area { background-color: rgba(255, 255, 255, 0.7); border-radius: 0.75rem; padding: 1rem; }\n",
    "        .chat-container { background-color: white; border-radius: 0.75rem; height: 600px; overflow: hidden; display: flex; flex-direction: column; }\n",
    "        .chat-header { padding: 1rem; border-bottom: 1px solid #e5e7eb; }\n",
    "        .info-box { background-color: #eef2ff; border-left: 4px solid #4f46e5; padding: 0.75rem; margin: 0.5rem 0; border-radius: 0.25rem; }\n",
    "        .status-indicator { display: inline-block; width: 8px; height: 8px; border-radius: 50%; margin-right: 6px; }\n",
    "        .status-active { background-color: #10b981; }\n",
    "        .status-inactive { background-color: #6b7280; }\n",
    "        .file-info { display: flex; align-items: center; padding: 0.5rem; background-color: #f3f4f6; border-radius: 0.5rem; margin-top: 0.5rem; }\n",
    "        .file-icon { margin-right: 0.5rem; color: #4b5563; }\n",
    "        .download-area { background-color: #f9fafb; border-radius: 0.5rem; padding: 1rem; margin-top: 1rem; }\n",
    "    \"\"\") as demo:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                # 📄 Document Chat Assistant\n",
    "                ### Analysez vos documents et obtenez des réponses intelligentes\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Row(elem_classes=\"container\"):\n",
    "            # Colonne de gauche pour le téléchargement et les paramètres\n",
    "            with gr.Column(scale=1, min_width=350, elem_classes=\"doc-area\"):\n",
    "                gr.Markdown(\"## 📂 Documents\", elem_classes=\"text-xl font-semibold\")\n",
    "                \n",
    "                # Zone de téléchargement\n",
    "                with gr.Box():\n",
    "                    file_upload = gr.File(\n",
    "                        file_types=['.pdf', '.docx', '.txt'], \n",
    "                        label=\"Téléverser un document\",\n",
    "                        file_count=\"single\"\n",
    "                    )\n",
    "                    \n",
    "                # Informations sur le document\n",
    "                with gr.Group():\n",
    "                    file_name = gr.Textbox(label=\"Document actuel\", interactive=False)\n",
    "                    file_preview = gr.Textbox(\n",
    "                        label=\"Aperçu du contenu\", \n",
    "                        lines=10,\n",
    "                        placeholder=\"Le contenu extrait du document apparaîtra ici...\",\n",
    "                        interactive=False\n",
    "                    )\n",
    "                \n",
    "                # Options\n",
    "                with gr.Accordion(\"Options\", open=True):\n",
    "                    context_radio = gr.Radio(\n",
    "                        choices=[\"Standard\", \"Avec contexte du document\"], \n",
    "                        value=\"Standard\", \n",
    "                        label=\"Mode de conversation\",\n",
    "                        info=\"Le mode avec contexte utilise le contenu du document pour améliorer les réponses\"\n",
    "                    )\n",
    "                    \n",
    "                # Zone de téléchargement de réponse\n",
    "                with gr.Box(elem_classes=\"download-area\"):\n",
    "                    gr.Markdown(\"## 💾 Exporter la dernière réponse\")\n",
    "                    with gr.Row():\n",
    "                        format_dropdown = gr.Dropdown(\n",
    "                            choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"], \n",
    "                            value=\"txt\", \n",
    "                            label=\"Format\",\n",
    "                            scale=1\n",
    "                        )\n",
    "                        download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\", scale=1)\n",
    "                    \n",
    "                    download_status = gr.Markdown(\"\")\n",
    "                    response_file = gr.File(label=\"Fichier généré\", visible=False)\n",
    "            \n",
    "            # Colonne de droite pour le chat\n",
    "            with gr.Column(scale=2, min_width=550, elem_classes=\"chat-container\"):\n",
    "                with gr.Row(elem_classes=\"chat-header\"):\n",
    "                    gr.Markdown(\"\"\"\n",
    "                    ## 💬 Conversation\n",
    "                    <div>\n",
    "                        <span class=\"status-indicator status-active\"></span>\n",
    "                        <span>Intelligence artificielle Gemini 2.0 Flash</span>\n",
    "                    </div>\n",
    "                    \"\"\")\n",
    "                \n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Discussion\", \n",
    "                    elem_id=\"chatbox\",\n",
    "                    height=400,\n",
    "                    bubble_full_width=False,\n",
    "                    show_copy_button=True,\n",
    "                    avatar_images=(\"https://api.dicebear.com/7.x/thumbs/svg?seed=user\", \"https://api.dicebear.com/7.x/bottts/svg?seed=assistant\")\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    msg = gr.Textbox(\n",
    "                        placeholder=\"Posez une question ou demandez de l'aide avec votre document...\",\n",
    "                        label=\"Message\",\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                        scale=10\n",
    "                    )\n",
    "                    submit_btn = gr.Button(\"Envoyer\", variant=\"primary\", scale=1)\n",
    "                \n",
    "                with gr.Row():\n",
    "                    clear_btn = gr.Button(\"🗑️ Effacer la conversation\", variant=\"secondary\")\n",
    "                    with gr.Accordion(\"Conseils d'utilisation\", open=False):\n",
    "                        gr.Markdown(\"\"\"\n",
    "                        - **Pour de meilleurs résultats**, téléchargez un document et utilisez le mode \"Avec contexte du document\"\n",
    "                        - Vous pouvez poser des questions spécifiques sur le contenu du document\n",
    "                        - Pour les rapports détaillés, exportez la réponse au format PDF ou DOCX\n",
    "                        - Pour télécharger une réponse, cliquez sur le bouton \"Télécharger\"\n",
    "                        \"\"\")\n",
    "        \n",
    "        # Événements\n",
    "        file_upload.upload(\n",
    "            fn=assistant.process_document, \n",
    "            inputs=[file_upload], \n",
    "            outputs=[file_name, file_preview]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            fn=assistant.generate_response, \n",
    "            inputs=[msg, chatbot, context_radio], \n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=assistant.generate_response, \n",
    "            inputs=[msg, chatbot, context_radio], \n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        download_btn.click(\n",
    "            fn=assistant.download_response, \n",
    "            inputs=[format_dropdown], \n",
    "            outputs=[response_file, download_status]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=assistant.reset_chat, \n",
    "            inputs=[], \n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(server_name=\"0.0.0.0\", server_port=7872, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4205eda1-9f95-48ba-bad0-96df26d7cfed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Base.set() got an unexpected keyword argument 'font'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# 🔧 Thème personnalisé\u001b[39;00m\n\u001b[32m     79\u001b[39m custom_theme = \u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthemes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprimary_hue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mindigo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43msecondary_hue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpurple\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneutral_hue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mslate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mradius_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthemes\u001b[49m\u001b[43m.\u001b[49m\u001b[43msizes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mradius_md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfont\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInter, ui-sans-serif, system-ui, sans-serif\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlinear-gradient(to right, #f8f9fa, #f1f3f9)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m#ffffff\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_shadow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_primary_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*primary_500\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_primary_background_fill_hover\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*primary_600\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_primary_text_color\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_secondary_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_100\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_secondary_background_fill_hover\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_200\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbutton_secondary_text_color\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_800\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_background_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_50\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_border_color\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*neutral_200\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_border_color_focus\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*primary_500\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_shadow_focus\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0 0 0 3px rgba(99, 102, 241, 0.2)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# 🔷 Interface Gradio\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m gr.Blocks(theme=custom_theme, title=\u001b[33m\"\u001b[39m\u001b[33mAssistant IA avec documents\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m demo:\n",
      "\u001b[31mTypeError\u001b[39m: Base.set() got an unexpected keyword argument 'font'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "def extract_text_from_file(file):\n",
    "    \"\"\"Extraction du texte à partir de fichiers PDF, DOCX ou TXT\"\"\"\n",
    "    text = \"\"\n",
    "    if file is None:\n",
    "        return \"Aucun fichier fourni.\"\n",
    "    ext = os.path.splitext(file.name)[-1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        with fitz.open(file.name) as doc:\n",
    "            text = \"\\n\".join(page.get_text() for page in doc)\n",
    "    elif ext == \".docx\":\n",
    "        doc = Document(file.name)\n",
    "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif ext == \".txt\":\n",
    "        text = file.read().decode(\"utf-8\")\n",
    "    else:\n",
    "        text = \"Format de fichier non pris en charge.\"\n",
    "    return text\n",
    "\n",
    "def interaction_avec_ia(message, historique, contenu_document, mode_conversation):\n",
    "    \"\"\"Traitement du message utilisateur selon le mode de conversation\"\"\"\n",
    "    if mode_conversation == \"Avec contexte (document)\":\n",
    "        contexte = contenu_document\n",
    "        reponse = f\"🧠 [IA avec document] Contexte: {contexte[:100]}...\\nRéponse à: {message}\"\n",
    "    else:\n",
    "        reponse = f\"💬 [IA sans document] Réponse à: {message}\"\n",
    "    \n",
    "    historique.append((message, reponse))\n",
    "    return historique, historique\n",
    "\n",
    "def export_conversation(historique, format_fichier):\n",
    "    \"\"\"Export de la conversation au format demandé\"\"\"\n",
    "    contenu = \"\\n\".join([f\"👤 {msg}\\n🤖 {rep}\" for msg, rep in historique])\n",
    "    if format_fichier == \"TXT\":\n",
    "        return contenu\n",
    "    elif format_fichier == \"PDF\":\n",
    "        import io\n",
    "        from reportlab.pdfgen import canvas\n",
    "        from reportlab.lib.pagesizes import letter\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        p = canvas.Canvas(buffer, pagesize=letter)\n",
    "        y = 750\n",
    "        for ligne in contenu.split(\"\\n\"):\n",
    "            p.drawString(40, y, ligne)\n",
    "            y -= 15\n",
    "            if y < 50:\n",
    "                p.showPage()\n",
    "                y = 750\n",
    "        p.save()\n",
    "        buffer.seek(0)\n",
    "        return (format_fichier, buffer.read())\n",
    "    elif format_fichier == \"DOCX\":\n",
    "        from docx import Document\n",
    "        doc = Document()\n",
    "        doc.add_heading(\"Historique de conversation\", 0)\n",
    "        for msg, rep in historique:\n",
    "            doc.add_paragraph(f\"👤 {msg}\")\n",
    "            doc.add_paragraph(f\"🤖 {rep}\")\n",
    "        buffer = io.BytesIO()\n",
    "        doc.save(buffer)\n",
    "        buffer.seek(0)\n",
    "        return (format_fichier, buffer.read())\n",
    "    elif format_fichier == \"XLSX\":\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(historique, columns=[\"Utilisateur\", \"Assistant IA\"])\n",
    "        buffer = io.BytesIO()\n",
    "        with pd.ExcelWriter(buffer) as writer:\n",
    "            df.to_excel(writer, index=False)\n",
    "        buffer.seek(0)\n",
    "        return (format_fichier, buffer.read())\n",
    "    return None\n",
    "\n",
    "# 🔧 Thème personnalisé\n",
    "custom_theme = gr.themes.Base(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"purple\",\n",
    "    neutral_hue=\"slate\",\n",
    "    radius_size=gr.themes.sizes.radius_md,\n",
    ").set(\n",
    "    font=\"Inter, ui-sans-serif, system-ui, sans-serif\",\n",
    "    body_background_fill=\"linear-gradient(to right, #f8f9fa, #f1f3f9)\",\n",
    "    block_background_fill=\"#ffffff\",\n",
    "    block_shadow=\"0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)\",\n",
    "    button_primary_background_fill=\"*primary_500\",\n",
    "    button_primary_background_fill_hover=\"*primary_600\",\n",
    "    button_primary_text_color=\"white\",\n",
    "    button_secondary_background_fill=\"*neutral_100\",\n",
    "    button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "    button_secondary_text_color=\"*neutral_800\",\n",
    "    input_background_fill=\"*neutral_50\",\n",
    "    input_border_color=\"*neutral_200\",\n",
    "    input_border_color_focus=\"*primary_500\",\n",
    "    input_shadow_focus=\"0 0 0 3px rgba(99, 102, 241, 0.2)\",\n",
    ")\n",
    "\n",
    "# 🔷 Interface Gradio\n",
    "with gr.Blocks(theme=custom_theme, title=\"Assistant IA avec documents\") as demo:\n",
    "    gr.Markdown(\"## 🤖 Assistant IA – Interagissez avec ou sans documents\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        fichier = gr.File(label=\"📄 Charger un fichier\", file_types=[\".pdf\", \".docx\", \".txt\"])\n",
    "        contenu_document = gr.State(\"\")\n",
    "        fichier.change(fn=extract_text_from_file, inputs=fichier, outputs=contenu_document)\n",
    "\n",
    "    mode_conversation = gr.Radio(choices=[\"Sans contexte\", \"Avec contexte (document)\"],\n",
    "                                 value=\"Sans contexte\", label=\"🎛 Mode de conversation\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"🗨️ Conversation\")\n",
    "    message = gr.Textbox(label=\"💬 Votre message\", placeholder=\"Posez une question...\")\n",
    "    historique = gr.State([])\n",
    "\n",
    "    envoyer = gr.Button(\"Envoyer 🚀\")\n",
    "    envoyer.click(fn=interaction_avec_ia, \n",
    "                  inputs=[message, historique, contenu_document, mode_conversation],\n",
    "                  outputs=[chatbot, historique])\n",
    "    \n",
    "    with gr.Row():\n",
    "        format_export = gr.Dropdown(choices=[\"TXT\", \"PDF\", \"DOCX\", \"XLSX\"], value=\"TXT\", label=\"💾 Exporter au format\")\n",
    "        bouton_export = gr.Button(\"📥 Exporter\")\n",
    "        fichier_export = gr.File(interactive=False)\n",
    "\n",
    "    bouton_export.click(fn=export_conversation,\n",
    "                        inputs=[historique, format_export],\n",
    "                        outputs=fichier_export)\n",
    "\n",
    "# 🚀 Lancement\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6968baf7-416c-426e-9cbe-51b3a2e9a749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/16.6 MB 2.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.3/16.6 MB 3.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.1/16.6 MB 4.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.1/16.6 MB 3.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.7/16.6 MB 3.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.0/16.6 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.6/16.6 MB 4.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 7.1/16.6 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.6/16.6 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.7/16.6 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.7/16.6 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.0/16.6 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.0/16.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.3/16.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.8/16.6 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.6/16.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.2/16.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.5/16.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.25.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ea1e9e2-f7f0-4aba-ba5a-3c769ba67a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\24985513.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\24985513.py:222: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:27:24,962 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:27:25,982 - INFO: HTTP Request: GET http://localhost:7874/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:27:28,052 - INFO: HTTP Request: HEAD http://localhost:7874/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:27:29,291 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://45fb6e70fc7a342e00.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:27:33,529 - INFO: HTTP Request: HEAD https://45fb6e70fc7a342e00.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://45fb6e70fc7a342e00.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2133, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        block_fn, inputs, state, explicit_call\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1787, in preprocess_data\n",
      "    raise InvalidComponentError(\n",
      "        f\"{block.__class__} Component not a valid input component.\"\n",
      "    )\n",
      "gradio.exceptions.InvalidComponentError: <class 'gradio.layouts.group.Group'> Component not a valid input component.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2133, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        block_fn, inputs, state, explicit_call\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1787, in preprocess_data\n",
      "    raise InvalidComponentError(\n",
      "        f\"{block.__class__} Component not a valid input component.\"\n",
      "    )\n",
      "gradio.exceptions.InvalidComponentError: <class 'gradio.layouts.group.Group'> Component not a valid input component.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donné en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier téléchargé pour extraire son texte.\"\"\"\n",
    "        if file is None:\n",
    "            return \"Aucun fichier sélectionné.\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        preview = self.current_document_text[:1000]\n",
    "        if len(self.current_document_text) > 1000:\n",
    "            preview += \"\\n\\n[... Texte tronqué pour la prévisualisation]\"\n",
    "        return preview\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "            \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:3000]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel à l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la génération de la réponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"Télécharge la réponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None\n",
    "            \n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la création du fichier : {e}\")\n",
    "            return None\n",
    "    \n",
    "    def clear_chat(self):\n",
    "        \"\"\"Réinitialise la conversation.\"\"\"\n",
    "        return []\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">📄 Document Chat Assistant</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Analysez vos documents et chattez avec leur contenu</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-file-upload\"></i> Documents\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    file_upload = gr.File(\n",
    "                        file_types=['.pdf', '.docx', '.txt'], \n",
    "                        label=\"Importez votre document\",\n",
    "                        elem_id=\"file-upload\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Accordion(\"Aperçu du document\", open=False):\n",
    "                        file_preview = gr.Textbox(\n",
    "                            label=\"\", \n",
    "                            lines=10,\n",
    "                            elem_id=\"file-preview\"\n",
    "                        )\n",
    "                    \n",
    "                    context_type = gr.Radio(\n",
    "                        choices=[\"Standard\", \"Avec contexte du document\"],\n",
    "                        value=\"Avec contexte du document\",\n",
    "                        label=\"Mode de conversation\",\n",
    "                        info=\"Le mode avec contexte utilise le contenu du document pour générer des réponses.\",\n",
    "                        elem_id=\"context-type\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Group(visible=False) as export_group:\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-download\"></i> Exporter la réponse\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        format_dropdown = gr.Dropdown(\n",
    "                            choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"],\n",
    "                            value=\"txt\",\n",
    "                            label=\"Format\",\n",
    "                            elem_id=\"format-dropdown\"\n",
    "                        )\n",
    "                        download_btn = gr.Button(\n",
    "                            \"Télécharger\",\n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"download-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    response_file = gr.File(\n",
    "                        label=\"Fichier généré\",\n",
    "                        elem_id=\"response-file\"\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-comments\"></i> Conversation\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        bubble_full_width=False,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Posez une question sur votre document...\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "                        export_toggle = gr.Button(\n",
    "                            \"Exporter la réponse\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"export-toggle\"\n",
    "                        )\n",
    "        \n",
    "        # Events\n",
    "        file_upload.upload(\n",
    "            agent.process_document,\n",
    "            inputs=[file_upload],\n",
    "            outputs=[file_preview]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            agent.clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        export_toggle.click(\n",
    "            lambda visibility: gr.update(visible=not visibility),\n",
    "            inputs=[export_group],\n",
    "            outputs=[export_group]\n",
    "        )\n",
    "        \n",
    "        download_btn.click(\n",
    "            agent.download_response,\n",
    "            inputs=[format_dropdown],\n",
    "            outputs=[response_file]\n",
    "        )\n",
    "        \n",
    "        demo.load(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(\n",
    "            server_name=\"0.0.0.0\", \n",
    "            server_port=7874, \n",
    "            share=True,\n",
    "            favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21357984-1022-46f0-952e-a90397b94083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\1341683234.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\1341683234.py:222: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:33:35,649 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:33:36,715 - INFO: HTTP Request: GET http://localhost:7875/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:33:38,772 - INFO: HTTP Request: HEAD http://localhost:7875/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:33:40,027 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://93cffdc8ea4c092930.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:33:42,602 - INFO: HTTP Request: HEAD https://93cffdc8ea4c092930.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://93cffdc8ea4c092930.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2133, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        block_fn, inputs, state, explicit_call\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1787, in preprocess_data\n",
      "    raise InvalidComponentError(\n",
      "        f\"{block.__class__} Component not a valid input component.\"\n",
      "    )\n",
      "gradio.exceptions.InvalidComponentError: <class 'gradio.layouts.group.Group'> Component not a valid input component.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donné en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier téléchargé pour extraire son texte.\"\"\"\n",
    "        if file is None:\n",
    "            return \"Aucun fichier sélectionné.\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        preview = self.current_document_text[:1000]\n",
    "        if len(self.current_document_text) > 1000:\n",
    "            preview += \"\\n\\n[... Texte tronqué pour la prévisualisation]\"\n",
    "        return preview\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "            \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:3000]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel à l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la génération de la réponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"Télécharge la réponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None\n",
    "            \n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la création du fichier : {e}\")\n",
    "            return None\n",
    "    \n",
    "    def clear_chat(self):\n",
    "        \"\"\"Réinitialise la conversation.\"\"\"\n",
    "        return []\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">📄 Document Chat Assistant</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Analysez vos documents et chattez avec leur contenu</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-file-upload\"></i> Documents\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    file_upload = gr.File(\n",
    "                        file_types=['.pdf', '.docx', '.txt'], \n",
    "                        label=\"Importez votre document\",\n",
    "                        elem_id=\"file-upload\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Accordion(\"Aperçu du document\", open=False):\n",
    "                        file_preview = gr.Textbox(\n",
    "                            label=\"\", \n",
    "                            lines=10,\n",
    "                            elem_id=\"file-preview\"\n",
    "                        )\n",
    "                    \n",
    "                    context_type = gr.Radio(\n",
    "                        choices=[\"Standard\", \"Avec contexte du document\"],\n",
    "                        value=\"Avec contexte du document\",\n",
    "                        label=\"Mode de conversation\",\n",
    "                        info=\"Le mode avec contexte utilise le contenu du document pour générer des réponses.\",\n",
    "                        elem_id=\"context-type\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Group(visible=False) as export_group:\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-download\"></i> Exporter la réponse\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        format_dropdown = gr.Dropdown(\n",
    "                            choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"],\n",
    "                            value=\"txt\",\n",
    "                            label=\"Format\",\n",
    "                            elem_id=\"format-dropdown\"\n",
    "                        )\n",
    "                        download_btn = gr.Button(\n",
    "                            \"Télécharger\",\n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"download-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    response_file = gr.File(\n",
    "                        label=\"Fichier généré\",\n",
    "                        elem_id=\"response-file\"\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-comments\"></i> Conversation\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        bubble_full_width=False,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Posez une question sur votre document...\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "                        export_toggle = gr.Button(\n",
    "                            \"Exporter la réponse\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"export-toggle\"\n",
    "                        )\n",
    "        \n",
    "        # Events\n",
    "        file_upload.upload(\n",
    "            agent.process_document,\n",
    "            inputs=[file_upload],\n",
    "            outputs=[file_preview]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            agent.clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        export_toggle.click(\n",
    "            lambda visibility: gr.update(visible=not visibility),\n",
    "            inputs=[export_group],\n",
    "            outputs=[export_group]\n",
    "        )\n",
    "        \n",
    "        download_btn.click(\n",
    "            agent.download_response,\n",
    "            inputs=[format_dropdown],\n",
    "            outputs=[response_file]\n",
    "        )\n",
    "        \n",
    "        demo.load(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        )\n",
    "        \n",
    "        return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(\n",
    "            server_name=\"0.0.0.0\", \n",
    "            server_port=7875, \n",
    "            share=True,\n",
    "            favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64e9a081-30c9-431d-b302-c148dfba9a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\3995981476.py:230: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20848\\3995981476.py:230: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:42:55,979 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:42:57,240 - INFO: HTTP Request: GET http://localhost:7876/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:42:59,335 - INFO: HTTP Request: HEAD http://localhost:7876/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-10 12:43:00,561 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://27531fd8fb6f9e5f09.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:43:03,255 - INFO: HTTP Request: HEAD https://27531fd8fb6f9e5f09.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://27531fd8fb6f9e5f09.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier donné en fonction de son extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class FreeChatGPT:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file) -> str:\n",
    "        \"\"\"Traite le fichier téléchargé pour extraire son texte.\"\"\"\n",
    "        if file is None:\n",
    "            return \"Aucun fichier sélectionné.\"\n",
    "        self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "        preview = self.current_document_text[:1000]\n",
    "        if len(self.current_document_text) > 1000:\n",
    "            preview += \"\\n\\n[... Texte tronqué pour la prévisualisation]\"\n",
    "        return preview\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse en fonction du message de l'utilisateur et de l'historique.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "            \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte du document\" and self.current_document_text:\n",
    "                context += f\"Contexte du document:\\n{self.current_document_text[:3000]}\\n\\n\"\n",
    "            \n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            context += f\"Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": context}]}]}\n",
    "            \n",
    "            response = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", \"\")\n",
    "            else:\n",
    "                raw_reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "            else:\n",
    "                assistant_reply = raw_reply\n",
    "            \n",
    "            self.last_response = assistant_reply\n",
    "            history.append((message, assistant_reply))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur d'appel à l'API : {e}\")\n",
    "            history.append((message, f\"Erreur lors de la génération de la réponse : {e}\"))\n",
    "            return history\n",
    "    \n",
    "    def download_response(self, file_format: str) -> str:\n",
    "        \"\"\"Télécharge la réponse sous forme de fichier TXT, PDF, DOCX ou XLSX.\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None\n",
    "            \n",
    "        file_path = f\"response.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=12)\n",
    "                pdf.multi_cell(190, 10, self.last_response)\n",
    "                pdf.output(file_path)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(file_path)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                data = [line.split(\";\") for line in lines]\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_excel(file_path, index=False, header=False)\n",
    "            \n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la création du fichier : {e}\")\n",
    "            return None\n",
    "    \n",
    "    def clear_chat(self):\n",
    "        \"\"\"Réinitialise la conversation.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def toggle_export_panel(self, is_visible):\n",
    "        \"\"\"Bascule la visibilité du panneau d'exportation.\"\"\"\n",
    "        return gr.update(visible=not is_visible)\n",
    "\n",
    "def create_gradio_interface() -> gr.Blocks:\n",
    "    agent = FreeChatGPT()\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">📄 Document Chat Assistant</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Analysez vos documents et chattez avec leur contenu</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        # Variable d'état pour suivre la visibilité du panneau d'exportation\n",
    "        export_panel_visible = gr.State(False)\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-file-upload\"></i> Documents\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    file_upload = gr.File(\n",
    "                        file_types=['.pdf', '.docx', '.txt'], \n",
    "                        label=\"Importez votre document\",\n",
    "                        elem_id=\"file-upload\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Accordion(\"Aperçu du document\", open=False):\n",
    "                        file_preview = gr.Textbox(\n",
    "                            label=\"\", \n",
    "                            lines=10,\n",
    "                            elem_id=\"file-preview\"\n",
    "                        )\n",
    "                    \n",
    "                    context_type = gr.Radio(\n",
    "                        choices=[\"Standard\", \"Avec contexte du document\"],\n",
    "                        value=\"Avec contexte du document\",\n",
    "                        label=\"Mode de conversation\",\n",
    "                        info=\"Le mode avec contexte utilise le contenu du document pour générer des réponses.\",\n",
    "                        elem_id=\"context-type\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Group(visible=False) as export_group:\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-download\"></i> Exporter la réponse\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        format_dropdown = gr.Dropdown(\n",
    "                            choices=[\"txt\", \"pdf\", \"docx\", \"xlsx\"],\n",
    "                            value=\"txt\",\n",
    "                            label=\"Format\",\n",
    "                            elem_id=\"format-dropdown\"\n",
    "                        )\n",
    "                        download_btn = gr.Button(\n",
    "                            \"Télécharger\",\n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"download-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    response_file = gr.File(\n",
    "                        label=\"Fichier généré\",\n",
    "                        elem_id=\"response-file\",\n",
    "                        type=\"filepath\"\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-comments\"></i> Conversation\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        bubble_full_width=False,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Posez une question sur votre document...\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "                        export_toggle = gr.Button(\n",
    "                            \"Exporter la réponse\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"export-toggle\"\n",
    "                        )\n",
    "        \n",
    "        # Events\n",
    "        file_upload.upload(\n",
    "            agent.process_document,\n",
    "            inputs=[file_upload],\n",
    "            outputs=[file_preview]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        ).then(\n",
    "            lambda: True,\n",
    "            None,\n",
    "            [export_panel_visible]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            agent.generate_response,\n",
    "            inputs=[msg, chatbot, context_type],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=True),\n",
    "            None,\n",
    "            [export_group]\n",
    "        ).then(\n",
    "            lambda: True,\n",
    "            None,\n",
    "            [export_panel_visible]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            agent.clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(visible=False),\n",
    "            None,\n",
    "            [export_group]\n",
    "        ).then(\n",
    "            lambda: False,\n",
    "            None,\n",
    "            [export_panel_visible]\n",
    "        )\n",
    "        \n",
    "        export_toggle.click(\n",
    "            lambda x: (not x, gr.update(visible=not x)),\n",
    "            inputs=[export_panel_visible],\n",
    "            outputs=[export_panel_visible, export_group]\n",
    "        )\n",
    "        \n",
    "        download_btn.click(\n",
    "            agent.download_response,\n",
    "            inputs=[format_dropdown],\n",
    "            outputs=[response_file]\n",
    "        )\n",
    "        \n",
    "        # Initialisation\n",
    "        demo.load(\n",
    "            lambda: (False, gr.update(visible=False)),\n",
    "            None,\n",
    "            [export_panel_visible, export_group]\n",
    "        )\n",
    "        \n",
    "        return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_gradio_interface()\n",
    "        interface.launch(\n",
    "            server_name=\"0.0.0.0\", \n",
    "            server_port=7876, \n",
    "            share=True,\n",
    "            favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d29246c-dc9f-4dbc-81dc-79caecf8e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 14:02:38,915 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-11 14:02:39,332 - INFO: HTTP Request: GET http://localhost:7879/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-11 14:02:41,406 - INFO: HTTP Request: HEAD http://localhost:7879/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-11 14:02:43,351 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://57259245ea3083941f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 14:02:48,733 - INFO: HTTP Request: HEAD https://57259245ea3083941f.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://57259245ea3083941f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 822, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\starlette\\responses.py\", line 343, in __call__\n",
      "    stat_result = await anyio.to_thread.run_sync(os.stat, self.path)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "OSError: [WinError 123] La syntaxe du nom de fichier, de répertoire ou de volume est incorrecte: 'https://www.svgrepo.com/show/306500/chat.svg'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration de l'API\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class PersonalityProfile:\n",
    "    \"\"\"Définit le profil de personnalité de l'assistant.\"\"\"\n",
    "    \n",
    "    PERSONALITIES = {\n",
    "        \"amical\": {\n",
    "            \"nom\": \"Léo\",\n",
    "            \"description\": \"un assistant virtuel amical et enjoué qui utilise un langage informel et aime glisser des blagues dans ses réponses\",\n",
    "            \"intro\": \"Salut ! Je suis Léo, ton assistant virtuel. Je suis là pour t'aider avec un sourire. Qu'est-ce que je peux faire pour toi aujourd'hui ? 😊\",\n",
    "            \"ton\": \"amical, décontracté, enthousiaste\",\n",
    "            \"émojis\": True,\n",
    "            \"expressions\": [\"Super !\", \"Génial !\", \"Pas de souci !\", \"Bien sûr !\"],\n",
    "            \"avatar\": \"👨‍💼\"\n",
    "        },\n",
    "        \"professionnel\": {\n",
    "            \"nom\": \"Sophie\",\n",
    "            \"description\": \"une assistante virtuelle professionnelle et efficace qui s'exprime avec précision et concision\",\n",
    "            \"intro\": \"Bonjour, je suis Sophie, votre assistante virtuelle. Comment puis-je vous aider aujourd'hui ?\",\n",
    "            \"ton\": \"professionnel, concis, efficace\",\n",
    "            \"émojis\": False,\n",
    "            \"expressions\": [\"Certainement.\", \"Bien compris.\", \"Je vous propose...\", \"Voici les informations demandées.\"],\n",
    "            \"avatar\": \"👩‍💼\"\n",
    "        },\n",
    "        \"expert\": {\n",
    "            \"nom\": \"Dr. Martin\",\n",
    "            \"description\": \"un assistant virtuel expert et analytique qui fournit des réponses détaillées et documentées\",\n",
    "            \"intro\": \"Bonjour, je suis le Dr. Martin. Je suis spécialisé dans l'analyse et la résolution de problèmes complexes. Comment puis-je vous apporter mon expertise aujourd'hui ?\",\n",
    "            \"ton\": \"expert, analytique, pédagogique\",\n",
    "            \"émojis\": False,\n",
    "            \"expressions\": [\"D'après mon analyse...\", \"Selon les données disponibles...\", \"Je recommande...\", \"Il est important de noter que...\"],\n",
    "            \"avatar\": \"🧠\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        if personality_type not in self.PERSONALITIES:\n",
    "            personality_type = \"amical\"  # Valeur par défaut\n",
    "        \n",
    "        self.type = personality_type\n",
    "        self.profile = self.PERSONALITIES[personality_type]\n",
    "        \n",
    "        self.nom = self.profile[\"nom\"]\n",
    "        self.description = self.profile[\"description\"]\n",
    "        self.intro = self.profile[\"intro\"]\n",
    "        self.ton = self.profile[\"ton\"]\n",
    "        self.use_emojis = self.profile[\"émojis\"]\n",
    "        self.expressions = self.profile[\"expressions\"]\n",
    "        self.avatar = self.profile[\"avatar\"]\n",
    "    \n",
    "    def get_random_expression(self) -> str:\n",
    "        \"\"\"Renvoie une expression aléatoire typique de la personnalité.\"\"\"\n",
    "        return random.choice(self.expressions)\n",
    "    \n",
    "    def get_personality_prompt(self) -> str:\n",
    "        \"\"\"Renvoie la description de la personnalité pour le système.\"\"\"\n",
    "        return f\"\"\"Tu es {self.nom}, {self.description}. \n",
    "Ton ton est {self.ton}.\n",
    "{\"Tu utilises souvent des émojis pour exprimer des émotions.\" if self.use_emojis else \"Tu évites d'utiliser des émojis sauf si nécessaire.\"}\n",
    "Assure-toi que tes réponses reflètent cette personnalité de manière cohérente.\"\"\"\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"Gère la mémoire à court et long terme du chatbot.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_short_term_memory: int = 10):\n",
    "        self.short_term_memory = []\n",
    "        self.long_term_memory = {}\n",
    "        self.max_short_term_memory = max_short_term_memory\n",
    "    \n",
    "    def add_interaction(self, user_message: str, assistant_response: str):\n",
    "        \"\"\"Ajoute une interaction à la mémoire à court terme.\"\"\"\n",
    "        self.short_term_memory.append({\"user\": user_message, \"assistant\": assistant_response, \"timestamp\": time.time()})\n",
    "        \n",
    "        # Limiter la taille de la mémoire à court terme\n",
    "        if len(self.short_term_memory) > self.max_short_term_memory:\n",
    "            self.short_term_memory.pop(0)\n",
    "    \n",
    "    def add_to_long_term_memory(self, key: str, value: str):\n",
    "        \"\"\"Ajoute une information importante à la mémoire à long terme.\"\"\"\n",
    "        self.long_term_memory[key] = {\"value\": value, \"timestamp\": time.time()}\n",
    "    \n",
    "    def get_recent_conversation(self, max_entries: int = 5) -> str:\n",
    "        \"\"\"Renvoie les conversations récentes formatées pour le contexte.\"\"\"\n",
    "        recent = self.short_term_memory[-max_entries:] if max_entries < len(self.short_term_memory) else self.short_term_memory\n",
    "        formatted = \"\"\n",
    "        \n",
    "        for interaction in recent:\n",
    "            formatted += f\"Utilisateur: {interaction['user']}\\n\"\n",
    "            formatted += f\"Assistant: {interaction['assistant']}\\n\\n\"\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def get_relevant_long_term_memory(self, query: str) -> Dict:\n",
    "        \"\"\"Renvoie les informations pertinentes de la mémoire à long terme.\"\"\"\n",
    "        # Recherche simple par mots-clés\n",
    "        relevant_info = {}\n",
    "        for key, info in self.long_term_memory.items():\n",
    "            if key.lower() in query.lower():\n",
    "                relevant_info[key] = info[\"value\"]\n",
    "        \n",
    "        return relevant_info\n",
    "\n",
    "class VirtualAssistant:\n",
    "    \"\"\"Assistant virtuel avec personnalité utilisant l'API Gemini.\"\"\"\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        self.memory = Memory()\n",
    "        self.session_start = datetime.now()\n",
    "    \n",
    "    def switch_personality(self, personality_type: str) -> str:\n",
    "        \"\"\"Change la personnalité de l'assistant et renvoie le message d'introduction.\"\"\"\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        return self.personality.intro\n",
    "    \n",
    "    def generate_system_prompt(self) -> str:\n",
    "        \"\"\"Génère le prompt système avec la personnalité et les informations contextuelles.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        date_str = current_time.strftime(\"%d/%m/%Y\")\n",
    "        time_str = current_time.strftime(\"%H:%M\")\n",
    "        \n",
    "        system_prompt = f\"\"\"Tu es un assistant virtuel conversationnel.\n",
    "\n",
    "{self.personality.get_personality_prompt()}\n",
    "\n",
    "Informations contextuelles:\n",
    "- Date actuelle: {date_str}\n",
    "- Heure actuelle: {time_str}\n",
    "- Durée de la session en cours: {str(current_time - self.session_start).split('.')[0]}\n",
    "\n",
    "Réponds aux questions et demandes de l'utilisateur de manière conversationnelle.\"\"\"\n",
    "        \n",
    "        # Ajouter des informations de la mémoire à long terme si nécessaire\n",
    "        if self.memory.long_term_memory:\n",
    "            system_prompt += \"\\n\\nInformations sur l'utilisateur que tu connais déjà:\\n\"\n",
    "            for key, value in self.memory.long_term_memory.items():\n",
    "                system_prompt += f\"- {key}: {value}\\n\"\n",
    "        \n",
    "        return system_prompt\n",
    "    \n",
    "    def process_query(self, message: str, history: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Traite une requête utilisateur et génère une réponse.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            # Construire le contexte à partir de l'historique et du prompt système\n",
    "            system_prompt = self.generate_system_prompt()\n",
    "            conversation_history = self.memory.get_recent_conversation()\n",
    "            \n",
    "            # Construire le prompt complet\n",
    "            prompt = f\"{system_prompt}\\n\\nHistorique récent de la conversation:\\n{conversation_history}\\nUtilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel à l'API Gemini\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            data = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.95,\n",
    "                    \"topK\": 40,\n",
    "                    \"maxOutputTokens\": 1024\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{API_URL}?key={GEMINI_API_KEY}\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            \n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extraire la réponse\n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", {})\n",
    "                if \"parts\" in raw_reply and raw_reply[\"parts\"]:\n",
    "                    assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"\")\n",
    "                else:\n",
    "                    assistant_reply = \"Je suis désolé, mais je n'ai pas pu générer une réponse.\"\n",
    "            else:\n",
    "                error_message = response_data.get(\"error\", {}).get(\"message\", \"Erreur inconnue\")\n",
    "                assistant_reply = f\"Je rencontre des difficultés techniques. Détails: {error_message}\"\n",
    "            \n",
    "            # Mémoriser l'interaction\n",
    "            self.memory.add_interaction(message, assistant_reply)\n",
    "            \n",
    "            # Analyser la requête pour des informations personnelles\n",
    "            self._extract_personal_info(message)\n",
    "            \n",
    "            # Mettre à jour l'historique pour l'interface\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "            \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement de la requête: {e}\")\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Je suis désolé, une erreur s'est produite lors du traitement de votre demande. Détails techniques: {str(e)}\"})\n",
    "            return history\n",
    "    \n",
    "    def _extract_personal_info(self, message: str):\n",
    "        \"\"\"Extraction basique d'informations personnelles pour la mémoire à long terme.\"\"\"\n",
    "        # Exemple très simple - dans une application réelle, on utiliserait NLP plus avancé\n",
    "        \n",
    "        # Vérifier si le message contient une présentation avec un nom\n",
    "        name_triggers = [\"je m'appelle\", \"mon nom est\", \"je suis\"]\n",
    "        for trigger in name_triggers:\n",
    "            if trigger in message.lower():\n",
    "                parts = message.lower().split(trigger)\n",
    "                if len(parts) > 1:\n",
    "                    potential_name = parts[1].strip().split()[0]\n",
    "                    # Capitaliser le nom supposé\n",
    "                    potential_name = potential_name.capitalize()\n",
    "                    self.memory.add_to_long_term_memory(\"nom\", potential_name)\n",
    "                    break\n",
    "        \n",
    "        # Autres extractions possibles (préférences, localisation, etc.)\n",
    "\n",
    "def create_interface() -> gr.Blocks:\n",
    "    \"\"\"Crée l'interface utilisateur pour l'assistant virtuel.\"\"\"\n",
    "    \n",
    "    # Créer l'assistant\n",
    "    assistant = VirtualAssistant()\n",
    "    \n",
    "    # CSS personnalisé\n",
    "    css = \"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 1000px !important;\n",
    "    }\n",
    "    .chat-message-container {\n",
    "        padding: 15px;\n",
    "        border-radius: 15px;\n",
    "        margin-bottom: 10px;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #f0f7ff;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #e6f7e6;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .personality-btn {\n",
    "        padding: 8px 15px;\n",
    "        border-radius: 20px;\n",
    "        border: none;\n",
    "        margin: 5px;\n",
    "        font-weight: bold;\n",
    "        cursor: pointer;\n",
    "        transition: all 0.3s;\n",
    "    }\n",
    "    .personality-btn:hover {\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    .personality-amical {\n",
    "        background-color: #ffde7d;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-professionnel {\n",
    "        background-color: #7db9ff;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-expert {\n",
    "        background-color: #b69cff;\n",
    "        color: #333;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\"), css=css) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">✨ Assistant Virtuel Personnalisable</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Un assistant conversationnel avec différentes personnalités</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-user-circle\"></i> Personnalité\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    personality_intro = gr.Markdown(assistant.personality.intro)\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        amical_btn = gr.Button(\n",
    "                            f\"Léo {assistant.personality.PERSONALITIES['amical']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-amical\"]\n",
    "                        )\n",
    "                        pro_btn = gr.Button(\n",
    "                            f\"Sophie {assistant.personality.PERSONALITIES['professionnel']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-professionnel\"]\n",
    "                        )\n",
    "                        expert_btn = gr.Button(\n",
    "                            f\"Dr. Martin {assistant.personality.PERSONALITIES['expert']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-expert\"]\n",
    "                        )\n",
    "                \n",
    "                with gr.Accordion(\"À propos de cet assistant\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        Cet assistant virtuel utilise l'API Gemini de Google pour générer des réponses contextuelles.\n",
    "                        \n",
    "                        Chaque personnalité a son propre style de communication :\n",
    "                        \n",
    "                        - **Léo** est amical et décontracté, utilisant un langage informel et des émojis\n",
    "                        - **Sophie** est professionnelle et concise, privilégiant l'efficacité\n",
    "                        - **Dr. Martin** est un expert analytique fournissant des réponses détaillées et documentées\n",
    "                        \n",
    "                        L'assistant peut se souvenir de certaines informations tout au long de la conversation.\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                with gr.Accordion(\"Capacités et limitations\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        ### Capacités :\n",
    "                        - Répondre à des questions générales\n",
    "                        - Fournir des explications et définitions\n",
    "                        - Proposer des idées et suggestions\n",
    "                        - Maintenir une conversation cohérente\n",
    "                        - S'adapter à différents styles de communication\n",
    "                        \n",
    "                        ### Limitations :\n",
    "                        - Connaissance limitée aux événements antérieurs à sa date d'entraînement\n",
    "                        - Ne peut pas accéder à Internet ou exécuter du code\n",
    "                        - Ne peut pas voir ou analyser d'images\n",
    "                        - Peut parfois générer des informations incorrectes\n",
    "                        \"\"\",\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\",\n",
    "                        type=\"messages\",\n",
    "                        avatar_images=[None, assistant.personality.avatar]\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Comment puis-je vous aider aujourd'hui ?\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "        \n",
    "        # Événements\n",
    "        def clear_chat():\n",
    "            return []\n",
    "        \n",
    "        def switch_personality(personality_type):\n",
    "            intro_message = assistant.switch_personality(personality_type)\n",
    "            return intro_message, [], gr.update(avatar_images=[None, assistant.personality.avatar])\n",
    "        \n",
    "        # Traitement des interactions utilisateur\n",
    "        submit_btn.click(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        # Changement de personnalité\n",
    "        amical_btn.click(\n",
    "            lambda: switch_personality(\"amical\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        pro_btn.click(\n",
    "            lambda: switch_personality(\"professionnel\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        expert_btn.click(\n",
    "            lambda: switch_personality(\"expert\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        # Accueil\n",
    "        demo.load(\n",
    "            lambda: assistant.personality.intro,\n",
    "            outputs=[personality_intro]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_interface()\n",
    "        # Détection de l'environnement Hugging Face Spaces\n",
    "        if os.getenv(\"SPACE_ID\"):\n",
    "            # Configuration pour Hugging Face Spaces\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\",\n",
    "                share=False\n",
    "            )\n",
    "        else:\n",
    "            # Configuration pour un environnement local\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\", \n",
    "                server_port=7879, \n",
    "                share=True,\n",
    "                favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e614a23b-0e80-4b14-9e07-3a3e0e9fe102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 19:15:35,513 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 19:15:36,313 - INFO: HTTP Request: GET http://localhost:7880/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 19:15:38,386 - INFO: HTTP Request: HEAD http://localhost:7880/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 19:15:40,125 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://d3b1c6fec65a17cf8c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 19:15:46,308 - INFO: HTTP Request: HEAD https://d3b1c6fec65a17cf8c.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d3b1c6fec65a17cf8c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration de l'API\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class PersonalityProfile:\n",
    "    \"\"\"Définit le profil de personnalité de l'assistant.\"\"\"\n",
    "    \n",
    "    PERSONALITIES = {\n",
    "        \"amical\": {\n",
    "            \"nom\": \"Léo\",\n",
    "            \"description\": \"un assistant virtuel amical et enjoué qui utilise un langage informel et aime glisser des blagues dans ses réponses\",\n",
    "            \"intro\": \"Salut ! Je suis Léo, ton assistant virtuel. Je suis là pour t'aider avec un sourire. Qu'est-ce que je peux faire pour toi aujourd'hui ? 😊\",\n",
    "            \"ton\": \"amical, décontracté, enthousiaste\",\n",
    "            \"émojis\": True,\n",
    "            \"expressions\": [\"Super !\", \"Génial !\", \"Pas de souci !\", \"Bien sûr !\"],\n",
    "            \"avatar\": \"👨‍💼\"\n",
    "        },\n",
    "        \"professionnel\": {\n",
    "            \"nom\": \"Sophie\",\n",
    "            \"description\": \"une assistante virtuelle professionnelle et efficace qui s'exprime avec précision et concision\",\n",
    "            \"intro\": \"Bonjour, je suis Sophie, votre assistante virtuelle. Comment puis-je vous aider aujourd'hui ?\",\n",
    "            \"ton\": \"professionnel, concis, efficace\",\n",
    "            \"émojis\": False,\n",
    "            \"expressions\": [\"Certainement.\", \"Bien compris.\", \"Je vous propose...\", \"Voici les informations demandées.\"],\n",
    "            \"avatar\": \"👩‍💼\"\n",
    "        },\n",
    "        \"expert\": {\n",
    "            \"nom\": \"Dr. Martin\",\n",
    "            \"description\": \"un assistant virtuel expert et analytique qui fournit des réponses détaillées et documentées\",\n",
    "            \"intro\": \"Bonjour, je suis le Dr. Martin. Je suis spécialisé dans l'analyse et la résolution de problèmes complexes. Comment puis-je vous apporter mon expertise aujourd'hui ?\",\n",
    "            \"ton\": \"expert, analytique, pédagogique\",\n",
    "            \"émojis\": False,\n",
    "            \"expressions\": [\"D'après mon analyse...\", \"Selon les données disponibles...\", \"Je recommande...\", \"Il est important de noter que...\"],\n",
    "            \"avatar\": \"🧠\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        if personality_type not in self.PERSONALITIES:\n",
    "            personality_type = \"amical\"  # Valeur par défaut\n",
    "        \n",
    "        self.type = personality_type\n",
    "        self.profile = self.PERSONALITIES[personality_type]\n",
    "        \n",
    "        self.nom = self.profile[\"nom\"]\n",
    "        self.description = self.profile[\"description\"]\n",
    "        self.intro = self.profile[\"intro\"]\n",
    "        self.ton = self.profile[\"ton\"]\n",
    "        self.use_emojis = self.profile[\"émojis\"]\n",
    "        self.expressions = self.profile[\"expressions\"]\n",
    "        self.avatar = self.profile[\"avatar\"]\n",
    "    \n",
    "    def get_random_expression(self) -> str:\n",
    "        \"\"\"Renvoie une expression aléatoire typique de la personnalité.\"\"\"\n",
    "        return random.choice(self.expressions)\n",
    "    \n",
    "    def get_personality_prompt(self) -> str:\n",
    "        \"\"\"Renvoie la description de la personnalité pour le système.\"\"\"\n",
    "        return f\"\"\"Tu es {self.nom}, {self.description}. \n",
    "Ton ton est {self.ton}.\n",
    "{\"Tu utilises souvent des émojis pour exprimer des émotions.\" if self.use_emojis else \"Tu évites d'utiliser des émojis sauf si nécessaire.\"}\n",
    "Assure-toi que tes réponses reflètent cette personnalité de manière cohérente.\"\"\"\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"Gère la mémoire à court et long terme du chatbot.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_short_term_memory: int = 10):\n",
    "        self.short_term_memory = []\n",
    "        self.long_term_memory = {}\n",
    "        self.max_short_term_memory = max_short_term_memory\n",
    "    \n",
    "    def add_interaction(self, user_message: str, assistant_response: str):\n",
    "        \"\"\"Ajoute une interaction à la mémoire à court terme.\"\"\"\n",
    "        self.short_term_memory.append({\"user\": user_message, \"assistant\": assistant_response, \"timestamp\": time.time()})\n",
    "        \n",
    "        # Limiter la taille de la mémoire à court terme\n",
    "        if len(self.short_term_memory) > self.max_short_term_memory:\n",
    "            self.short_term_memory.pop(0)\n",
    "    \n",
    "    def add_to_long_term_memory(self, key: str, value: str):\n",
    "        \"\"\"Ajoute une information importante à la mémoire à long terme.\"\"\"\n",
    "        self.long_term_memory[key] = {\"value\": value, \"timestamp\": time.time()}\n",
    "    \n",
    "    def get_recent_conversation(self, max_entries: int = 5) -> str:\n",
    "        \"\"\"Renvoie les conversations récentes formatées pour le contexte.\"\"\"\n",
    "        recent = self.short_term_memory[-max_entries:] if max_entries < len(self.short_term_memory) else self.short_term_memory\n",
    "        formatted = \"\"\n",
    "        \n",
    "        for interaction in recent:\n",
    "            formatted += f\"Utilisateur: {interaction['user']}\\n\"\n",
    "            formatted += f\"Assistant: {interaction['assistant']}\\n\\n\"\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def get_relevant_long_term_memory(self, query: str) -> Dict:\n",
    "        \"\"\"Renvoie les informations pertinentes de la mémoire à long terme.\"\"\"\n",
    "        # Recherche simple par mots-clés\n",
    "        relevant_info = {}\n",
    "        for key, info in self.long_term_memory.items():\n",
    "            if key.lower() in query.lower():\n",
    "                relevant_info[key] = info[\"value\"]\n",
    "        \n",
    "        return relevant_info\n",
    "\n",
    "class VirtualAssistant:\n",
    "    \"\"\"Assistant virtuel avec personnalité utilisant l'API Gemini.\"\"\"\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        self.memory = Memory()\n",
    "        self.session_start = datetime.now()\n",
    "    \n",
    "    def switch_personality(self, personality_type: str) -> str:\n",
    "        \"\"\"Change la personnalité de l'assistant et renvoie le message d'introduction.\"\"\"\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        return self.personality.intro\n",
    "    \n",
    "    def generate_system_prompt(self) -> str:\n",
    "        \"\"\"Génère le prompt système avec la personnalité et les informations contextuelles.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        date_str = current_time.strftime(\"%d/%m/%Y\")\n",
    "        time_str = current_time.strftime(\"%H:%M\")\n",
    "        \n",
    "        system_prompt = f\"\"\"Tu es un assistant virtuel conversationnel.\n",
    "\n",
    "{self.personality.get_personality_prompt()}\n",
    "\n",
    "Informations contextuelles:\n",
    "- Date actuelle: {date_str}\n",
    "- Heure actuelle: {time_str}\n",
    "- Durée de la session en cours: {str(current_time - self.session_start).split('.')[0]}\n",
    "\n",
    "Réponds aux questions et demandes de l'utilisateur de manière conversationnelle.\"\"\"\n",
    "        \n",
    "        # Ajouter des informations de la mémoire à long terme si nécessaire\n",
    "        if self.memory.long_term_memory:\n",
    "            system_prompt += \"\\n\\nInformations sur l'utilisateur que tu connais déjà:\\n\"\n",
    "            for key, value in self.memory.long_term_memory.items():\n",
    "                system_prompt += f\"- {key}: {value}\\n\"\n",
    "        \n",
    "        return system_prompt\n",
    "    \n",
    "    def process_query(self, message: str, history: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Traite une requête utilisateur et génère une réponse.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            # Construire le contexte à partir de l'historique et du prompt système\n",
    "            system_prompt = self.generate_system_prompt()\n",
    "            conversation_history = self.memory.get_recent_conversation()\n",
    "            \n",
    "            # Construire le prompt complet\n",
    "            prompt = f\"{system_prompt}\\n\\nHistorique récent de la conversation:\\n{conversation_history}\\nUtilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel à l'API Gemini\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            data = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.95,\n",
    "                    \"topK\": 40,\n",
    "                    \"maxOutputTokens\": 1024\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{API_URL}?key={GEMINI_API_KEY}\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            \n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extraire la réponse\n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", {})\n",
    "                if \"parts\" in raw_reply and raw_reply[\"parts\"]:\n",
    "                    assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"\")\n",
    "                else:\n",
    "                    assistant_reply = \"Je suis désolé, mais je n'ai pas pu générer une réponse.\"\n",
    "            else:\n",
    "                error_message = response_data.get(\"error\", {}).get(\"message\", \"Erreur inconnue\")\n",
    "                assistant_reply = f\"Je rencontre des difficultés techniques. Détails: {error_message}\"\n",
    "            \n",
    "            # Mémoriser l'interaction\n",
    "            self.memory.add_interaction(message, assistant_reply)\n",
    "            \n",
    "            # Analyser la requête pour des informations personnelles\n",
    "            self._extract_personal_info(message)\n",
    "            \n",
    "            # Mettre à jour l'historique pour l'interface\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "            \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement de la requête: {e}\")\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Je suis désolé, une erreur s'est produite lors du traitement de votre demande. Détails techniques: {str(e)}\"})\n",
    "            return history\n",
    "    \n",
    "    def _extract_personal_info(self, message: str):\n",
    "        \"\"\"Extraction basique d'informations personnelles pour la mémoire à long terme.\"\"\"\n",
    "        # Exemple très simple - dans une application réelle, on utiliserait NLP plus avancé\n",
    "        \n",
    "        # Vérifier si le message contient une présentation avec un nom\n",
    "        name_triggers = [\"je m'appelle\", \"mon nom est\", \"je suis\"]\n",
    "        for trigger in name_triggers:\n",
    "            if trigger in message.lower():\n",
    "                parts = message.lower().split(trigger)\n",
    "                if len(parts) > 1:\n",
    "                    potential_name = parts[1].strip().split()[0]\n",
    "                    # Capitaliser le nom supposé\n",
    "                    potential_name = potential_name.capitalize()\n",
    "                    self.memory.add_to_long_term_memory(\"nom\", potential_name)\n",
    "                    break\n",
    "        \n",
    "        # Autres extractions possibles (préférences, localisation, etc.)\n",
    "\n",
    "def create_interface() -> gr.Blocks:\n",
    "    \"\"\"Crée l'interface utilisateur pour l'assistant virtuel.\"\"\"\n",
    "    \n",
    "    # Créer l'assistant\n",
    "    assistant = VirtualAssistant()\n",
    "    \n",
    "    # CSS personnalisé\n",
    "    css = \"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 1000px !important;\n",
    "    }\n",
    "    .chat-message-container {\n",
    "        padding: 15px;\n",
    "        border-radius: 15px;\n",
    "        margin-bottom: 10px;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #f0f7ff;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #e6f7e6;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .personality-btn {\n",
    "        padding: 8px 15px;\n",
    "        border-radius: 20px;\n",
    "        border: none;\n",
    "        margin: 5px;\n",
    "        font-weight: bold;\n",
    "        cursor: pointer;\n",
    "        transition: all 0.3s;\n",
    "    }\n",
    "    .personality-btn:hover {\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    .personality-amical {\n",
    "        background-color: #ffde7d;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-professionnel {\n",
    "        background-color: #7db9ff;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-expert {\n",
    "        background-color: #b69cff;\n",
    "        color: #333;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\"), css=css) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">✨ Assistant Virtuel Personnalisable</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Un assistant conversationnel avec différentes personnalités</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-user-circle\"></i> Personnalité\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    personality_intro = gr.Markdown(assistant.personality.intro)\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        amical_btn = gr.Button(\n",
    "                            f\"Léo {assistant.personality.PERSONALITIES['amical']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-amical\"]\n",
    "                        )\n",
    "                        pro_btn = gr.Button(\n",
    "                            f\"Sophie {assistant.personality.PERSONALITIES['professionnel']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-professionnel\"]\n",
    "                        )\n",
    "                        expert_btn = gr.Button(\n",
    "                            f\"Dr. Martin {assistant.personality.PERSONALITIES['expert']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-expert\"]\n",
    "                        )\n",
    "                \n",
    "                with gr.Accordion(\"À propos de cet assistant\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        Cet assistant virtuel utilise l'API Gemini de Google pour générer des réponses contextuelles.\n",
    "                        \n",
    "                        Chaque personnalité a son propre style de communication :\n",
    "                        \n",
    "                        - **Léo** est amical et décontracté, utilisant un langage informel et des émojis\n",
    "                        - **Sophie** est professionnelle et concise, privilégiant l'efficacité\n",
    "                        - **Dr. Martin** est un expert analytique fournissant des réponses détaillées et documentées\n",
    "                        \n",
    "                        L'assistant peut se souvenir de certaines informations tout au long de la conversation.\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                with gr.Accordion(\"Capacités et limitations\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        ### Capacités :\n",
    "                        - Répondre à des questions générales\n",
    "                        - Fournir des explications et définitions\n",
    "                        - Proposer des idées et suggestions\n",
    "                        - Maintenir une conversation cohérente\n",
    "                        - S'adapter à différents styles de communication\n",
    "                        \n",
    "                        ### Limitations :\n",
    "                        - Connaissance limitée aux événements antérieurs à sa date d'entraînement\n",
    "                        - Ne peut pas accéder à Internet ou exécuter du code\n",
    "                        - Ne peut pas voir ou analyser d'images\n",
    "                        - Peut parfois générer des informations incorrectes\n",
    "                        \"\"\",\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\",\n",
    "                        type=\"messages\",\n",
    "                        avatar_images=[None, assistant.personality.avatar]\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Comment puis-je vous aider aujourd'hui ?\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "        \n",
    "        # Événements\n",
    "        def clear_chat():\n",
    "            return []\n",
    "        \n",
    "        def switch_personality(personality_type):\n",
    "            intro_message = assistant.switch_personality(personality_type)\n",
    "            return intro_message, [], gr.update(avatar_images=[None, assistant.personality.avatar])\n",
    "        \n",
    "        # Traitement des interactions utilisateur\n",
    "        submit_btn.click(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        # Changement de personnalité\n",
    "        amical_btn.click(\n",
    "            lambda: switch_personality(\"amical\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        pro_btn.click(\n",
    "            lambda: switch_personality(\"professionnel\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        expert_btn.click(\n",
    "            lambda: switch_personality(\"expert\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        # Accueil\n",
    "        demo.load(\n",
    "            lambda: assistant.personality.intro,\n",
    "            outputs=[personality_intro]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_interface()\n",
    "        # Détection de l'environnement Hugging Face Spaces\n",
    "        if os.getenv(\"SPACE_ID\"):\n",
    "            # Configuration pour Hugging Face Spaces\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\",\n",
    "                share=False\n",
    "            )\n",
    "        else:\n",
    "            # Configuration pour un environnement local\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\", \n",
    "                server_port=7880, \n",
    "                share=True,\n",
    "                favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3266f8fa-d91e-47d2-b1d8-51ed81eebedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11760\\1311349699.py:278: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:11:58,582 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:11:59,679 - INFO: HTTP Request: GET http://localhost:7888/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:12:01,721 - INFO: HTTP Request: HEAD http://localhost:7888/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:12:04,015 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://1cc489c3559dd699d1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:12:08,003 - INFO: HTTP Request: HEAD https://1cc489c3559dd699d1.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1cc489c3559dd699d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2147, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1939, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\file.py\", line 227, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 503, in __init__\n",
      "    super().__init__(*args)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py\", line 132, in __init__\n",
      "    raise TypeError(\n",
      "    ...<2 lines>...\n",
      "        f\"not {type(path).__name__!r}\")\n",
      "TypeError: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'tuple'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Préparer l'aperçu (limité)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqué]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"📄 Fichier : {self.document_name} | 📊 {char_count} caractères\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse basée sur le prompt et l'historique de conversation\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqué pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel à l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            if \"candidates\" in result and result[\"candidates\"]:\n",
    "                raw_reply = result[\"candidates\"][0].get(\"content\", {})\n",
    "                if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                    reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "                else:\n",
    "                    reply = \"Erreur: Format de réponse inattendu\"\n",
    "            else:\n",
    "                reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            self.last_response = reply\n",
    "            history.append((message, reply))\n",
    "            \n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append((message, f\"Erreur : {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"Génère un fichier téléchargeable au format spécifié\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu à télécharger. Générez d'abord une réponse.\"\n",
    "        \n",
    "        try:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "            \n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"reponse_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traitement du texte pour FPDF (éviter les problèmes d'encodage)\n",
    "                clean_text = self.last_response.encode('latin-1', 'replace').decode('latin-1')\n",
    "                pdf.multi_cell(190, 7, clean_text)\n",
    "                pdf.output(filename)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"Réponse générée\", level=1)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filename)\n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter une conversion en tableau simple\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Créer des colonnes par défaut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_excel(filename, index=False)\n",
    "            elif file_format == \"csv\":\n",
    "                # Tenter une conversion en CSV\n",
    "                lines = self.last_response.split(\"\\n\")\n",
    "                filtered_lines = [line for line in lines if line.strip()]\n",
    "                \n",
    "                # Créer des colonnes par défaut\n",
    "                df = pd.DataFrame({\"Contenu\": filtered_lines})\n",
    "                df.to_csv(filename, index=False)\n",
    "            \n",
    "            return (filename, display_name), \"✅ Fichier généré avec succès\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"❌ Erreur lors de la génération du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Thème personnalisé - inspiré de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"Téléversez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document chargé\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de génération\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"Télécharger le fichier généré\", \n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents grâce à l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **Téléchargez un document**\n",
    "                   - Formats supportés : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte à l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme référence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez précis dans vos requêtes\n",
    "                   - Exemple : \"Peux-tu résumer ce document ?\" ou \"Quels sont les points principaux abordés ?\"\n",
    "                \n",
    "                4. **Exportez le résultat**\n",
    "                   - Téléchargez les réponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requêtes efficaces :\n",
    "                \n",
    "                - \"Résume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations clés présentées dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des événements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7888, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36fffcf4-845d-464f-8499-5a90f255d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11760\\3655890259.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:23:29,072 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:23:30,213 - INFO: HTTP Request: GET http://localhost:7889/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:23:32,283 - INFO: HTTP Request: HEAD http://localhost:7889/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 08:23:34,499 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://5e98bf49590ae0b23e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:23:38,697 - INFO: HTTP Request: HEAD https://5e98bf49590ae0b23e.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5e98bf49590ae0b23e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 08:24:53,143 - INFO: Fichier créé avec succès: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\reponse_20250414_082453.docx\n",
      "2025-04-14 08:25:49,547 - INFO: Fichier créé avec succès: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\reponse_20250414_082549.xlsx\n",
      "2025-04-14 08:26:28,296 - INFO: Fichier créé avec succès: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\reponse_20250414_082628.pdf\n",
      "2025-04-14 08:26:50,763 - INFO: Fichier créé avec succès: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\reponse_20250414_082650.txt\n"
     ]
    }
   ],
   "source": [
    " file_format == \"csv\":\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8-sig')  # BOM pour Excel\n",
    "            \n",
    "            # Vérifier que le fichier existe bien\n",
    "            if not os.path.exists(filepath):\n",
    "                logger.error(f\"Le fichier '{filepath}' n'a pas été créé\")\n",
    "                return None, \"❌ Erreur: Le fichier n'a pas été créé correctement\"\n",
    "            \n",
    "            logger.info(f\"Fichier créé avec succès: {filepath}\")\n",
    "            return filepath, \"✅ Fichier généré avec succès\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\", exc_info=True)\n",
    "            return None, f\"❌ Erreur lors de la génération du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Thème personnalisé - inspiré de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"Téléversez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document chargé\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de génération\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"Télécharger le fichier généré\", \n",
    "                                    interactive=False,\n",
    "                                    type=\"filepath\"  # Important: utiliser filepath pour le téléchargement\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents grâce à l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **Téléchargez un document**\n",
    "                   - Formats supportés : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte à l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme référence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez précis dans vos requêtes\n",
    "                   - Exemple : \"Peux-tu résumer ce document ?\" ou \"Quels sont les points principaux abordés ?\"\n",
    "                \n",
    "                4. **Exportez le résultat**\n",
    "                   - Téléchargez les réponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requêtes efficaces :\n",
    "                \n",
    "                - \"Résume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations clés présentées dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des événements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7889, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f637d38-8681-4b49-8df6-67f61f9ea4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 13:59:04,605 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 13:59:05,222 - INFO: HTTP Request: GET http://localhost:7879/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 13:59:07,287 - INFO: HTTP Request: HEAD http://localhost:7879/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 13:59:08,832 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://b2c433ed1af1d3f4f6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 13:59:13,173 - INFO: HTTP Request: HEAD https://b2c433ed1af1d3f4f6.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b2c433ed1af1d3f4f6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration de l'API\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class PersonalityProfile:\n",
    "    \"\"\"Définit le profil de personnalité de l'assistant.\"\"\"\n",
    "    \n",
    "    PERSONALITIES = {\n",
    "        \"amical\": {\n",
    "            \"nom\": \"Léo\",\n",
    "            \"description\": \"un assistant virtuel amical et enjoué qui utilise un langage informel et aime glisser des blagues dans ses réponses\",\n",
    "            \"intro\": \"Salut ! Je suis Léo, ton assistant virtuel. Je suis là pour t'aider avec un sourire. Qu'est-ce que je peux faire pour toi aujourd'hui ? 😊\",\n",
    "            \"ton\": \"amical, décontracté, enthousiaste\",\n",
    "            \"émojis\": True,\n",
    "            \"expressions\": [\"Super !\", \"Génial !\", \"Pas de souci !\", \"Bien sûr !\"],\n",
    "            \"avatar\": \"👨‍💼\"\n",
    "        },\n",
    "        \"professionnel\": {\n",
    "            \"nom\": \"Sophie\",\n",
    "            \"description\": \"une assistante virtuelle professionnelle et efficace qui s'exprime avec précision et concision\",\n",
    "            \"intro\": \"Bonjour, je suis Sophie, votre assistante virtuelle. Comment puis-je vous aider aujourd'hui ?\",\n",
    "            \"ton\": \"professionnel, concis, efficace\",\n",
    "            \"émojis\": False,\n",
    "            \"expressions\": [\"Certainement.\", \"Bien compris.\", \"Je vous propose...\", \"Voici les informations demandées.\"],\n",
    "            \"avatar\": \"👩‍💼\"\n",
    "        },\n",
    "        \"expert\": {\n",
    "            \"nom\": \"Dr. Martin\",\n",
    "            \"description\": \"un assistant virtuel expert et analytique qui fournit des réponses détaillées et documentées\",\n",
    "            \"intro\": \"Bonjour, je suis le Dr. Martin. Je suis spécialisé dans l'analyse et la résolution de problèmes complexes. Comment puis-je vous apporter mon expertise aujourd'hui ?\",\n",
    "            \"ton\": \"expert, analytique, pédagogique\",\n",
    "            \"émojis\": False,\n",
    "            \"expressions\": [\"D'après mon analyse...\", \"Selon les données disponibles...\", \"Je recommande...\", \"Il est important de noter que...\"],\n",
    "            \"avatar\": \"🧠\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        if personality_type not in self.PERSONALITIES:\n",
    "            personality_type = \"amical\"  # Valeur par défaut\n",
    "        \n",
    "        self.type = personality_type\n",
    "        self.profile = self.PERSONALITIES[personality_type]\n",
    "        \n",
    "        self.nom = self.profile[\"nom\"]\n",
    "        self.description = self.profile[\"description\"]\n",
    "        self.intro = self.profile[\"intro\"]\n",
    "        self.ton = self.profile[\"ton\"]\n",
    "        self.use_emojis = self.profile[\"émojis\"]\n",
    "        self.expressions = self.profile[\"expressions\"]\n",
    "        self.avatar = self.profile[\"avatar\"]\n",
    "    \n",
    "    def get_random_expression(self) -> str:\n",
    "        \"\"\"Renvoie une expression aléatoire typique de la personnalité.\"\"\"\n",
    "        return random.choice(self.expressions)\n",
    "    \n",
    "    def get_personality_prompt(self) -> str:\n",
    "        \"\"\"Renvoie la description de la personnalité pour le système.\"\"\"\n",
    "        return f\"\"\"Tu es {self.nom}, {self.description}. \n",
    "Ton ton est {self.ton}.\n",
    "{\"Tu utilises souvent des émojis pour exprimer des émotions.\" if self.use_emojis else \"Tu évites d'utiliser des émojis sauf si nécessaire.\"}\n",
    "Assure-toi que tes réponses reflètent cette personnalité de manière cohérente.\"\"\"\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"Gère la mémoire à court et long terme du chatbot.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_short_term_memory: int = 10):\n",
    "        self.short_term_memory = []\n",
    "        self.long_term_memory = {}\n",
    "        self.max_short_term_memory = max_short_term_memory\n",
    "    \n",
    "    def add_interaction(self, user_message: str, assistant_response: str):\n",
    "        \"\"\"Ajoute une interaction à la mémoire à court terme.\"\"\"\n",
    "        self.short_term_memory.append({\"user\": user_message, \"assistant\": assistant_response, \"timestamp\": time.time()})\n",
    "        \n",
    "        # Limiter la taille de la mémoire à court terme\n",
    "        if len(self.short_term_memory) > self.max_short_term_memory:\n",
    "            self.short_term_memory.pop(0)\n",
    "    \n",
    "    def add_to_long_term_memory(self, key: str, value: str):\n",
    "        \"\"\"Ajoute une information importante à la mémoire à long terme.\"\"\"\n",
    "        self.long_term_memory[key] = {\"value\": value, \"timestamp\": time.time()}\n",
    "    \n",
    "    def get_recent_conversation(self, max_entries: int = 5) -> str:\n",
    "        \"\"\"Renvoie les conversations récentes formatées pour le contexte.\"\"\"\n",
    "        recent = self.short_term_memory[-max_entries:] if max_entries < len(self.short_term_memory) else self.short_term_memory\n",
    "        formatted = \"\"\n",
    "        \n",
    "        for interaction in recent:\n",
    "            formatted += f\"Utilisateur: {interaction['user']}\\n\"\n",
    "            formatted += f\"Assistant: {interaction['assistant']}\\n\\n\"\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def get_relevant_long_term_memory(self, query: str) -> Dict:\n",
    "        \"\"\"Renvoie les informations pertinentes de la mémoire à long terme.\"\"\"\n",
    "        # Recherche simple par mots-clés\n",
    "        relevant_info = {}\n",
    "        for key, info in self.long_term_memory.items():\n",
    "            if key.lower() in query.lower():\n",
    "                relevant_info[key] = info[\"value\"]\n",
    "        \n",
    "        return relevant_info\n",
    "\n",
    "class VirtualAssistant:\n",
    "    \"\"\"Assistant virtuel avec personnalité utilisant l'API Gemini.\"\"\"\n",
    "    \n",
    "    def __init__(self, personality_type: str = \"amical\"):\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        self.memory = Memory()\n",
    "        self.session_start = datetime.now()\n",
    "    \n",
    "    def switch_personality(self, personality_type: str) -> str:\n",
    "        \"\"\"Change la personnalité de l'assistant et renvoie le message d'introduction.\"\"\"\n",
    "        self.personality = PersonalityProfile(personality_type)\n",
    "        return self.personality.intro\n",
    "    \n",
    "    def generate_system_prompt(self) -> str:\n",
    "        \"\"\"Génère le prompt système avec la personnalité et les informations contextuelles.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        date_str = current_time.strftime(\"%d/%m/%Y\")\n",
    "        time_str = current_time.strftime(\"%H:%M\")\n",
    "        \n",
    "        system_prompt = f\"\"\"Tu es un assistant virtuel conversationnel.\n",
    "\n",
    "{self.personality.get_personality_prompt()}\n",
    "\n",
    "Informations contextuelles:\n",
    "- Date actuelle: {date_str}\n",
    "- Heure actuelle: {time_str}\n",
    "- Durée de la session en cours: {str(current_time - self.session_start).split('.')[0]}\n",
    "\n",
    "Réponds aux questions et demandes de l'utilisateur de manière conversationnelle.\"\"\"\n",
    "        \n",
    "        # Ajouter des informations de la mémoire à long terme si nécessaire\n",
    "        if self.memory.long_term_memory:\n",
    "            system_prompt += \"\\n\\nInformations sur l'utilisateur que tu connais déjà:\\n\"\n",
    "            for key, value in self.memory.long_term_memory.items():\n",
    "                system_prompt += f\"- {key}: {value}\\n\"\n",
    "        \n",
    "        return system_prompt\n",
    "    \n",
    "    def process_query(self, message: str, history: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Traite une requête utilisateur et génère une réponse.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            # Construire le contexte à partir de l'historique et du prompt système\n",
    "            system_prompt = self.generate_system_prompt()\n",
    "            conversation_history = self.memory.get_recent_conversation()\n",
    "            \n",
    "            # Construire le prompt complet\n",
    "            prompt = f\"{system_prompt}\\n\\nHistorique récent de la conversation:\\n{conversation_history}\\nUtilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel à l'API Gemini\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            data = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.95,\n",
    "                    \"topK\": 40,\n",
    "                    \"maxOutputTokens\": 1024\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{API_URL}?key={GEMINI_API_KEY}\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            \n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extraire la réponse\n",
    "            if \"candidates\" in response_data and response_data[\"candidates\"]:\n",
    "                raw_reply = response_data[\"candidates\"][0].get(\"content\", {})\n",
    "                if \"parts\" in raw_reply and raw_reply[\"parts\"]:\n",
    "                    assistant_reply = raw_reply[\"parts\"][0].get(\"text\", \"\")\n",
    "                else:\n",
    "                    assistant_reply = \"Je suis désolé, mais je n'ai pas pu générer une réponse.\"\n",
    "            else:\n",
    "                error_message = response_data.get(\"error\", {}).get(\"message\", \"Erreur inconnue\")\n",
    "                assistant_reply = f\"Je rencontre des difficultés techniques. Détails: {error_message}\"\n",
    "            \n",
    "            # Mémoriser l'interaction\n",
    "            self.memory.add_interaction(message, assistant_reply)\n",
    "            \n",
    "            # Analyser la requête pour des informations personnelles\n",
    "            self._extract_personal_info(message)\n",
    "            \n",
    "            # Mettre à jour l'historique pour l'interface\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "            \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement de la requête: {e}\")\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Je suis désolé, une erreur s'est produite lors du traitement de votre demande. Détails techniques: {str(e)}\"})\n",
    "            return history\n",
    "    \n",
    "    def _extract_personal_info(self, message: str):\n",
    "        \"\"\"Extraction basique d'informations personnelles pour la mémoire à long terme.\"\"\"\n",
    "        # Exemple très simple - dans une application réelle, on utiliserait NLP plus avancé\n",
    "        \n",
    "        # Vérifier si le message contient une présentation avec un nom\n",
    "        name_triggers = [\"je m'appelle\", \"mon nom est\", \"je suis\"]\n",
    "        for trigger in name_triggers:\n",
    "            if trigger in message.lower():\n",
    "                parts = message.lower().split(trigger)\n",
    "                if len(parts) > 1:\n",
    "                    potential_name = parts[1].strip().split()[0]\n",
    "                    # Capitaliser le nom supposé\n",
    "                    potential_name = potential_name.capitalize()\n",
    "                    self.memory.add_to_long_term_memory(\"nom\", potential_name)\n",
    "                    break\n",
    "        \n",
    "        # Autres extractions possibles (préférences, localisation, etc.)\n",
    "\n",
    "def create_interface() -> gr.Blocks:\n",
    "    \"\"\"Crée l'interface utilisateur pour l'assistant virtuel.\"\"\"\n",
    "    \n",
    "    # Créer l'assistant\n",
    "    assistant = VirtualAssistant()\n",
    "    \n",
    "    # CSS personnalisé\n",
    "    css = \"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 1000px !important;\n",
    "    }\n",
    "    .chat-message-container {\n",
    "        padding: 15px;\n",
    "        border-radius: 15px;\n",
    "        margin-bottom: 10px;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #f0f7ff;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #e6f7e6;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .personality-btn {\n",
    "        padding: 8px 15px;\n",
    "        border-radius: 20px;\n",
    "        border: none;\n",
    "        margin: 5px;\n",
    "        font-weight: bold;\n",
    "        cursor: pointer;\n",
    "        transition: all 0.3s;\n",
    "    }\n",
    "    .personality-btn:hover {\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    .personality-amical {\n",
    "        background-color: #ffde7d;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-professionnel {\n",
    "        background-color: #7db9ff;\n",
    "        color: #333;\n",
    "    }\n",
    "    .personality-expert {\n",
    "        background-color: #b69cff;\n",
    "        color: #333;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\"), css=css) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "                <h1 style=\"font-size: 2.5rem; font-weight: 700; color: #1a56db;\">✨ Assistant Virtuel Personnalisable</h1>\n",
    "                <p style=\"font-size: 1.1rem; color: #4b5563;\">Un assistant conversationnel avec différentes personnalités</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            elem_id=\"app-title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=320):\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        <h3 style=\"font-size: 1.3rem; font-weight: 600; color: #2563eb; margin-bottom: 0.5rem;\">\n",
    "                            <i class=\"fas fa-user-circle\"></i> Personnalité\n",
    "                        </h3>\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                    personality_intro = gr.Markdown(assistant.personality.intro)\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        amical_btn = gr.Button(\n",
    "                            f\"Léo {assistant.personality.PERSONALITIES['amical']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-amical\"]\n",
    "                        )\n",
    "                        pro_btn = gr.Button(\n",
    "                            f\"Sophie {assistant.personality.PERSONALITIES['professionnel']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-professionnel\"]\n",
    "                        )\n",
    "                        expert_btn = gr.Button(\n",
    "                            f\"Dr. Martin {assistant.personality.PERSONALITIES['expert']['avatar']}\",\n",
    "                            elem_classes=[\"personality-btn\", \"personality-expert\"]\n",
    "                        )\n",
    "                \n",
    "                with gr.Accordion(\"À propos de cet assistant\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        Cet assistant virtuel utilise l'API Gemini de Google pour générer des réponses contextuelles.\n",
    "                        \n",
    "                        Chaque personnalité a son propre style de communication :\n",
    "                        \n",
    "                        - **Léo** est amical et décontracté, utilisant un langage informel et des émojis\n",
    "                        - **Sophie** est professionnelle et concise, privilégiant l'efficacité\n",
    "                        - **Dr. Martin** est un expert analytique fournissant des réponses détaillées et documentées\n",
    "                        \n",
    "                        L'assistant peut se souvenir de certaines informations tout au long de la conversation.\n",
    "                        \"\"\",\n",
    "                    )\n",
    "                    \n",
    "                with gr.Accordion(\"Capacités et limitations\", open=False):\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        ### Capacités :\n",
    "                        - Répondre à des questions générales\n",
    "                        - Fournir des explications et définitions\n",
    "                        - Proposer des idées et suggestions\n",
    "                        - Maintenir une conversation cohérente\n",
    "                        - S'adapter à différents styles de communication\n",
    "                        \n",
    "                        ### Limitations :\n",
    "                        - Connaissance limitée aux événements antérieurs à sa date d'entraînement\n",
    "                        - Ne peut pas accéder à Internet ou exécuter du code\n",
    "                        - Ne peut pas voir ou analyser d'images\n",
    "                        - Peut parfois générer des informations incorrectes\n",
    "                        \"\"\",\n",
    "                    )\n",
    "            \n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                with gr.Group():\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"\",\n",
    "                        height=500,\n",
    "                        show_copy_button=True,\n",
    "                        elem_id=\"chatbot\",\n",
    "                        type=\"messages\",\n",
    "                        avatar_images=[None, assistant.personality.avatar]\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            placeholder=\"Comment puis-je vous aider aujourd'hui ?\",\n",
    "                            label=\"\",\n",
    "                            lines=3,\n",
    "                            max_lines=10,\n",
    "                            show_label=False,\n",
    "                            elem_id=\"message-input\"\n",
    "                        )\n",
    "                        submit_btn = gr.Button(\n",
    "                            \"Envoyer\", \n",
    "                            variant=\"primary\",\n",
    "                            elem_id=\"submit-btn\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        clear_btn = gr.Button(\n",
    "                            \"Nouvelle conversation\", \n",
    "                            variant=\"secondary\",\n",
    "                            elem_id=\"clear-btn\"\n",
    "                        )\n",
    "        \n",
    "        # Événements\n",
    "        def clear_chat():\n",
    "            return []\n",
    "        \n",
    "        def switch_personality(personality_type):\n",
    "            intro_message = assistant.switch_personality(personality_type)\n",
    "            return intro_message, [], gr.update(avatar_images=[None, assistant.personality.avatar])\n",
    "        \n",
    "        # Traitement des interactions utilisateur\n",
    "        submit_btn.click(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            assistant.process_query,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: gr.update(value=\"\"),\n",
    "            None,\n",
    "            [msg]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        # Changement de personnalité\n",
    "        amical_btn.click(\n",
    "            lambda: switch_personality(\"amical\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        pro_btn.click(\n",
    "            lambda: switch_personality(\"professionnel\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        expert_btn.click(\n",
    "            lambda: switch_personality(\"expert\"),\n",
    "            outputs=[personality_intro, chatbot, chatbot]\n",
    "        )\n",
    "        \n",
    "        # Accueil\n",
    "        demo.load(\n",
    "            lambda: assistant.personality.intro,\n",
    "            outputs=[personality_intro]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        interface = create_interface()\n",
    "        # Détection de l'environnement Hugging Face Spaces\n",
    "        if os.getenv(\"SPACE_ID\"):\n",
    "            # Configuration pour Hugging Face Spaces\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\",\n",
    "                share=False\n",
    "            )\n",
    "        else:\n",
    "            # Configuration pour un environnement local\n",
    "            interface.launch(\n",
    "                server_name=\"0.0.0.0\", \n",
    "                server_port=7879, \n",
    "                share=True,\n",
    "                favicon_path=\"https://www.svgrepo.com/show/306500/chat.svg\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b948d6c4-60f6-41e8-a8da-5116eff05cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\4052332530.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:12:09,849 - INFO: HTTP Request: GET http://localhost:7897/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:12:12,152 - INFO: HTTP Request: HEAD http://localhost:7897/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:12:16,185 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://def10a76e4bbe18c5f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:12:39,114 - INFO: HTTP Request: HEAD https://def10a76e4bbe18c5f.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://def10a76e4bbe18c5f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Préparer l'aperçu (limité)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqué]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"📄 Fichier : {self.document_name} | 📊 {char_count} caractères\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse basée sur le prompt et l'historique de conversation\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqué pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel à l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            if \"candidates\" in result and result[\"candidates\"]:\n",
    "                raw_reply = result[\"candidates\"][0].get(\"content\", {})\n",
    "                if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                    reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "                else:\n",
    "                    reply = \"Erreur: Format de réponse inattendu\"\n",
    "            else:\n",
    "                reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            self.last_response = reply\n",
    "            history.append((message, reply))\n",
    "            \n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append((message, f\"Erreur : {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"Génère un fichier téléchargeable au format spécifié\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu à télécharger. Générez d'abord une réponse.\"\n",
    "        \n",
    "        try:\n",
    "            # Créer un répertoire temporaire si nécessaire\n",
    "            temp_dir = tempfile.gettempdir()\n",
    "            \n",
    "            # Créer un nom de fichier avec timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            base_filename = f\"reponse_{timestamp}\"\n",
    "            filepath = os.path.join(temp_dir, f\"{base_filename}.{file_format}\")\n",
    "            \n",
    "            # Nom d'affichage pour l'interface\n",
    "            display_name = f\"{base_filename}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Division du texte en lignes pour éviter les problèmes d'encodage\n",
    "                lines = self.last_response.split('\\n')\n",
    "                for line in lines:\n",
    "                    # Remplacer les caractères problématiques\n",
    "                    clean_line = line.encode('latin-1', 'replace').decode('latin-1')\n",
    "                    pdf.multi_cell(190, 7, clean_line)\n",
    "                pdf.output(filepath)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"Réponse générée\", level=1)\n",
    "                doc.add_paragraph(self.last_response)\n",
    "                doc.save(filepath)\n",
    "            elif file_format == \"xlsx\":\n",
    "                # Meilleure conversion en tableau\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                df.to_excel(filepath, index=False)\n",
    "            elif file_format == \"csv\":\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8-sig')  # BOM pour Excel\n",
    "            \n",
    "            # Vérifier que le fichier existe bien\n",
    "            if not os.path.exists(filepath):\n",
    "                logger.error(f\"Le fichier '{filepath}' n'a pas été créé\")\n",
    "                return None, \"❌ Erreur: Le fichier n'a pas été créé correctement\"\n",
    "            \n",
    "            logger.info(f\"Fichier créé avec succès: {filepath}\")\n",
    "            return filepath, \"✅ Fichier généré avec succès\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\", exc_info=True)\n",
    "            return None, f\"❌ Erreur lors de la génération du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Thème personnalisé - inspiré de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"Téléversez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document chargé\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de génération\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"Télécharger le fichier généré\", \n",
    "                                    interactive=False,\n",
    "                                    type=\"filepath\"  # Important: utiliser filepath pour le téléchargement\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents grâce à l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **Téléchargez un document**\n",
    "                   - Formats supportés : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte à l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme référence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez précis dans vos requêtes\n",
    "                   - Exemple : \"Peux-tu résumer ce document ?\" ou \"Quels sont les points principaux abordés ?\"\n",
    "                \n",
    "                4. **Exportez le résultat**\n",
    "                   - Téléchargez les réponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requêtes efficaces :\n",
    "                \n",
    "                - \"Résume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations clés présentées dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des événements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7897, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c049a5ba-b5ae-4d8a-93e8-5970aab401ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\907061348.py:357: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:18:57,635 - INFO: HTTP Request: GET http://localhost:7898/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:18:59,460 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:18:59,702 - INFO: HTTP Request: HEAD http://localhost:7898/ \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:19:05,888 - INFO: HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://a2500aa26104e9b212.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:19:28,224 - INFO: HTTP Request: HEAD https://a2500aa26104e9b212.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a2500aa26104e9b212.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:21:43,884 - INFO: Fichier créé avec succès: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\tmpl4pdzsny.docx (37256 octets)\n",
      "2025-04-15 14:25:42,671 - INFO: Fichier créé avec succès: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\tmp6wq0p56u.docx (37386 octets)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Préparer l'aperçu (limité)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqué]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"📄 Fichier : {self.document_name} | 📊 {char_count} caractères\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse basée sur le prompt et l'historique de conversation\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                context = f\"Contexte du document '{self.document_name}':\\n{self.current_document_text[:2000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 2000:\n",
    "                    context += \"[... Reste du document tronqué pour rester dans les limites de l'API ...]\\n\\n\"\n",
    "            \n",
    "            # Ajouter l'historique au contexte\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Utilisateur: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n",
    "            \n",
    "            final_prompt = f\"{context}Utilisateur: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Appel à l'API\n",
    "            data = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            \n",
    "            res = requests.post(f\"{API_URL}?key={GEMINI_API_KEY}\", json=data, headers=headers)\n",
    "            result = res.json()\n",
    "            \n",
    "            if \"candidates\" in result and result[\"candidates\"]:\n",
    "                raw_reply = result[\"candidates\"][0].get(\"content\", {})\n",
    "                if isinstance(raw_reply, dict) and \"parts\" in raw_reply:\n",
    "                    reply = raw_reply[\"parts\"][0].get(\"text\", \"Réponse vide\")\n",
    "                else:\n",
    "                    reply = \"Erreur: Format de réponse inattendu\"\n",
    "            else:\n",
    "                reply = \"Désolé, je n'ai pas pu générer de réponse.\"\n",
    "            \n",
    "            self.last_response = reply\n",
    "            history.append((message, reply))\n",
    "            \n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur API : {e}\")\n",
    "            history.append((message, f\"Erreur : {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"Génère un fichier téléchargeable au format spécifié\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu à télécharger. Générez d'abord une réponse.\"\n",
    "        \n",
    "        try:\n",
    "            # Créer un fichier temporaire qui ne sera pas supprimé immédiatement\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            temp_file.close()\n",
    "            filepath = temp_file.name\n",
    "            \n",
    "            # Nom d'affichage pour l'interface\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"reponse_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traiter le texte pour éviter les problèmes d'encodage\n",
    "                encoded_text = self.last_response.encode('ascii', 'replace').decode('ascii')\n",
    "                \n",
    "                # Diviser en paragraphes pour une meilleure mise en page\n",
    "                paragraphs = encoded_text.split('\\n\\n')\n",
    "                for paragraph in paragraphs:\n",
    "                    if paragraph.strip():\n",
    "                        pdf.multi_cell(190, 7, paragraph)\n",
    "                        pdf.ln(3)  # Espace entre paragraphes\n",
    "                \n",
    "                pdf.output(filepath)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"Réponse générée\", level=1)\n",
    "                \n",
    "                # Ajouter le texte en préservant les paragraphes\n",
    "                paragraphs = self.last_response.split('\\n\\n')\n",
    "                for para in paragraphs:\n",
    "                    if para.strip():\n",
    "                        doc.add_paragraph(para)\n",
    "                \n",
    "                doc.save(filepath)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter de structurer les données en tableau si possible\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                \n",
    "                # Détection de structure tabulaire (si le texte contient des séparateurs cohérents)\n",
    "                if any('\\t' in line for line in lines) or all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                    # Tenter de créer un DataFrame structuré\n",
    "                    if all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                        # Format Markdown table\n",
    "                        clean_lines = [line.strip().strip('|') for line in lines if '|' in line]\n",
    "                        rows = [row.split('|') for row in clean_lines]\n",
    "                        rows = [[cell.strip() for cell in row] for row in rows]\n",
    "                        \n",
    "                        # Supprimer la ligne de séparation markdown si présente\n",
    "                        if len(rows) > 1 and all('-' in cell for cell in rows[1]):\n",
    "                            rows.pop(1)\n",
    "                        \n",
    "                        if rows:\n",
    "                            headers = rows[0] if len(rows) > 1 else [f\"Col{i+1}\" for i in range(len(rows[0]))]\n",
    "                            data = rows[1:] if len(rows) > 1 else rows\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                        else:\n",
    "                            df = pd.DataFrame({\"Contenu\": lines})\n",
    "                    else:\n",
    "                        # Format avec tabulations ou autre séparateur détecté\n",
    "                        df = pd.DataFrame([line.split('\\t') for line in lines])\n",
    "                else:\n",
    "                    # Format simple (une colonne)\n",
    "                    df = pd.DataFrame({\"Contenu\": lines})\n",
    "                \n",
    "                # Sauvegarder en Excel\n",
    "                df.to_excel(filepath, index=False)\n",
    "            \n",
    "            elif file_format == \"csv\":\n",
    "                # Similaire à xlsx mais au format CSV\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                \n",
    "                # Détection des structures tabulaires comme pour xlsx\n",
    "                if any('\\t' in line for line in lines) or all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                    if all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                        clean_lines = [line.strip().strip('|') for line in lines if '|' in line]\n",
    "                        rows = [row.split('|') for row in clean_lines]\n",
    "                        rows = [[cell.strip() for cell in row] for row in rows]\n",
    "                        \n",
    "                        if len(rows) > 1 and all('-' in cell for cell in rows[1]):\n",
    "                            rows.pop(1)\n",
    "                        \n",
    "                        if rows:\n",
    "                            headers = rows[0] if len(rows) > 1 else [f\"Col{i+1}\" for i in range(len(rows[0]))]\n",
    "                            data = rows[1:] if len(rows) > 1 else rows\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                        else:\n",
    "                            df = pd.DataFrame({\"Contenu\": lines})\n",
    "                    else:\n",
    "                        df = pd.DataFrame([line.split('\\t') for line in lines])\n",
    "                else:\n",
    "                    df = pd.DataFrame({\"Contenu\": lines})\n",
    "                \n",
    "                # Sauvegarder en CSV avec encodage pour Excel\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # Vérifier que le fichier existe bien\n",
    "            if not os.path.exists(filepath):\n",
    "                logger.error(f\"Le fichier '{filepath}' n'a pas été créé\")\n",
    "                return None, \"❌ Erreur: Le fichier n'a pas été créé correctement\"\n",
    "            \n",
    "            # Vérifier la taille du fichier\n",
    "            file_size = os.path.getsize(filepath)\n",
    "            if file_size == 0:\n",
    "                logger.error(f\"Le fichier '{filepath}' est vide\")\n",
    "                return None, \"❌ Erreur: Le fichier généré est vide\"\n",
    "                \n",
    "            logger.info(f\"Fichier créé avec succès: {filepath} ({file_size} octets)\")\n",
    "            return filepath, f\"✅ Fichier '{display_name}' généré avec succès ({file_size} octets)\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\", exc_info=True)\n",
    "            return None, f\"❌ Erreur lors de la génération du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Thème personnalisé - inspiré de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Gemini API\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"Téléversez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document chargé\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de génération\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                # Utiliser File avec type=\"filepath\" pour le téléchargement\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"Télécharger le fichier généré\", \n",
    "                                    interactive=False,\n",
    "                                    type=\"filepath\",\n",
    "                                    elem_classes=\"download-btn\"\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant** est un outil qui vous permet de discuter avec vos documents grâce à l'IA.\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **Téléchargez un document**\n",
    "                   - Formats supportés : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte à l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme référence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez précis dans vos requêtes\n",
    "                   - Exemple : \"Peux-tu résumer ce document ?\" ou \"Quels sont les points principaux abordés ?\"\n",
    "                \n",
    "                4. **Exportez le résultat**\n",
    "                   - Téléchargez les réponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Exemples de requêtes efficaces :\n",
    "                \n",
    "                - \"Résume le contenu de ce document en 5 points.\"\n",
    "                - \"Quelles sont les informations clés présentées dans ce texte ?\"\n",
    "                - \"Explique-moi ce document comme si j'avais 10 ans.\"\n",
    "                - \"Quelles recommandations contient ce rapport ?\"\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Gemini API</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des événements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7898, share=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f727797a-9cf4-4913-a721-2e1d13f0b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:36:16,458 - INFO: Connexion à LM Studio réussie. Modèle: mistral-7b-instruct-v0.3\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\1871222720.py:388: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:36:19,580 - INFO: HTTP Request: GET http://localhost:7899/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:36:21,890 - INFO: HTTP Request: HEAD http://localhost:7899/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7899/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:37:42,348 - ERROR: Erreur API (400): {\"error\":\"Error rendering prompt with jinja template: \\\"Error: Only user and assistant roles are supported!\\n    at C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:254303\\n    at _0x1f852a.value (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:252374)\\n    at _0x5c2bc5.evaluateCallExpression (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:241449)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:251261)\\n    at _0x5c2bc5.evaluateBlock (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:240633)\\n    at _0x5c2bc5.evaluateIf (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:245045)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:249965)\\n    at _0x5c2bc5.evaluateBlock (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:240633)\\n    at _0x5c2bc5.evaluateIf (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:245045)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:249965)\\\". This is usually an issue with the model's prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration pour LM Studio local API\n",
    "LM_STUDIO_API_URL = \"http://localhost:1234/v1/chat/completions\"  # URL par défaut de LM Studio\n",
    "LM_STUDIO_MODEL = \"mistral-7b-instruct-v0.3\"  # Le modèle que vous avez téléchargé\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\\n\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "                    return text[:max_chars]\n",
    "            elif file_extension == '.docx':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    result = mammoth.extract_raw_text(file)\n",
    "                    return result.value[:max_chars]\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()[:max_chars]\n",
    "            else:\n",
    "                return \"Format de fichier non supporté\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du fichier : {e}\")\n",
    "            return f\"Erreur lors du traitement du fichier : {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.current_document_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "    \n",
    "    def process_document(self, file):\n",
    "        \"\"\"Traite un fichier et extrait son contenu\"\"\"\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        \n",
    "        try:\n",
    "            self.document_name = file.name.split(\"/\")[-1]\n",
    "            self.current_document_text = self.document_processor.extract_text_from_file(file.name)\n",
    "            \n",
    "            # Préparer l'aperçu (limité)\n",
    "            preview = self.current_document_text[:1500]\n",
    "            if len(self.current_document_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqué]\"\n",
    "                \n",
    "            char_count = len(self.current_document_text)\n",
    "            file_info = f\"📄 Fichier : {self.document_name} | 📊 {char_count} caractères\"\n",
    "            \n",
    "            return file_info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {str(e)}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Tuple[str, str]], context_type: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Génère une réponse basée sur le prompt et l'historique de conversation en utilisant LM Studio\"\"\"\n",
    "        if not message:\n",
    "            history.append((\"\", \"Veuillez entrer un message.\"))\n",
    "            return history\n",
    "        \n",
    "        try:\n",
    "            # Formatter le contexte du document si activé\n",
    "            document_context = \"\"\n",
    "            if context_type == \"Avec contexte\" and self.current_document_text:\n",
    "                document_context = f\"Je vous fournis le contexte d'un document intitulé '{self.document_name}':\\n\\n{self.current_document_text[:3000]}\\n\\n\"\n",
    "                if len(self.current_document_text) > 3000:\n",
    "                    document_context += \"[Le document est plus long, ceci est un extrait.]\\n\\n\"\n",
    "                document_context += \"Basez vos réponses sur ce document lorsque c'est pertinent.\\n\\n\"\n",
    "            \n",
    "            # Préparer les messages pour l'API de chat\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"Vous êtes un assistant documentaire utile et précis. {document_context}\"}\n",
    "            ]\n",
    "            \n",
    "            # Ajouter l'historique des conversations\n",
    "            for user_msg, assistant_msg in history:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "            \n",
    "            # Ajouter le message actuel\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            # Préparation de la requête pour l'API LM Studio\n",
    "            payload = {\n",
    "                \"model\": LM_STUDIO_MODEL,\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 1024,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            # Appel à l'API LM Studio\n",
    "            response = requests.post(LM_STUDIO_API_URL, headers=headers, data=json.dumps(payload))\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                response_json = response.json()\n",
    "                reply = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "                self.last_response = reply\n",
    "                history.append((message, reply))\n",
    "            else:\n",
    "                error_msg = f\"Erreur API ({response.status_code}): {response.text}\"\n",
    "                logger.error(error_msg)\n",
    "                history.append((message, f\"Erreur: Impossible de générer une réponse. Vérifiez que LM Studio est en cours d'exécution sur localhost:1234.\"))\n",
    "            \n",
    "            return history\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            error_msg = \"Erreur de connexion: Impossible de se connecter à LM Studio. Vérifiez qu'il est en cours d'exécution.\"\n",
    "            logger.error(error_msg)\n",
    "            history.append((message, error_msg))\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur inattendue: {e}\")\n",
    "            history.append((message, f\"Erreur: {str(e)}\"))\n",
    "            return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        \"\"\"Génère un fichier téléchargeable au format spécifié\"\"\"\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu à télécharger. Générez d'abord une réponse.\"\n",
    "        \n",
    "        try:\n",
    "            # Créer un fichier temporaire qui ne sera pas supprimé immédiatement\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "            temp_file.close()\n",
    "            filepath = temp_file.name\n",
    "            \n",
    "            # Nom d'affichage pour l'interface\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            display_name = f\"reponse_{timestamp}.{file_format}\"\n",
    "            \n",
    "            if file_format == \"txt\":\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(self.last_response)\n",
    "            \n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF()\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                \n",
    "                # Traiter le texte pour éviter les problèmes d'encodage\n",
    "                encoded_text = self.last_response.encode('ascii', 'replace').decode('ascii')\n",
    "                \n",
    "                # Diviser en paragraphes pour une meilleure mise en page\n",
    "                paragraphs = encoded_text.split('\\n\\n')\n",
    "                for paragraph in paragraphs:\n",
    "                    if paragraph.strip():\n",
    "                        pdf.multi_cell(190, 7, paragraph)\n",
    "                        pdf.ln(3)  # Espace entre paragraphes\n",
    "                \n",
    "                pdf.output(filepath)\n",
    "            \n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document()\n",
    "                doc.add_heading(\"Réponse générée\", level=1)\n",
    "                \n",
    "                # Ajouter le texte en préservant les paragraphes\n",
    "                paragraphs = self.last_response.split('\\n\\n')\n",
    "                for para in paragraphs:\n",
    "                    if para.strip():\n",
    "                        doc.add_paragraph(para)\n",
    "                \n",
    "                doc.save(filepath)\n",
    "            \n",
    "            elif file_format == \"xlsx\":\n",
    "                # Tenter de structurer les données en tableau si possible\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                \n",
    "                # Détection de structure tabulaire\n",
    "                if any('\\t' in line for line in lines) or all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                    # Tenter de créer un DataFrame structuré\n",
    "                    if all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                        # Format Markdown table\n",
    "                        clean_lines = [line.strip().strip('|') for line in lines if '|' in line]\n",
    "                        rows = [row.split('|') for row in clean_lines]\n",
    "                        rows = [[cell.strip() for cell in row] for row in rows]\n",
    "                        \n",
    "                        # Supprimer la ligne de séparation markdown si présente\n",
    "                        if len(rows) > 1 and all('-' in cell for cell in rows[1]):\n",
    "                            rows.pop(1)\n",
    "                        \n",
    "                        if rows:\n",
    "                            headers = rows[0] if len(rows) > 1 else [f\"Col{i+1}\" for i in range(len(rows[0]))]\n",
    "                            data = rows[1:] if len(rows) > 1 else rows\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                        else:\n",
    "                            df = pd.DataFrame({\"Contenu\": lines})\n",
    "                    else:\n",
    "                        # Format avec tabulations ou autre séparateur détecté\n",
    "                        df = pd.DataFrame([line.split('\\t') for line in lines])\n",
    "                else:\n",
    "                    # Format simple (une colonne)\n",
    "                    df = pd.DataFrame({\"Contenu\": lines})\n",
    "                \n",
    "                # Sauvegarder en Excel\n",
    "                df.to_excel(filepath, index=False)\n",
    "            \n",
    "            elif file_format == \"csv\":\n",
    "                # Similaire à xlsx mais au format CSV\n",
    "                lines = [line.strip() for line in self.last_response.split(\"\\n\") if line.strip()]\n",
    "                \n",
    "                # Détection des structures tabulaires comme pour xlsx\n",
    "                if any('\\t' in line for line in lines) or all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                    if all('|' in line for line in lines[:min(5, len(lines))]):\n",
    "                        clean_lines = [line.strip().strip('|') for line in lines if '|' in line]\n",
    "                        rows = [row.split('|') for row in clean_lines]\n",
    "                        rows = [[cell.strip() for cell in row] for row in rows]\n",
    "                        \n",
    "                        if len(rows) > 1 and all('-' in cell for cell in rows[1]):\n",
    "                            rows.pop(1)\n",
    "                        \n",
    "                        if rows:\n",
    "                            headers = rows[0] if len(rows) > 1 else [f\"Col{i+1}\" for i in range(len(rows[0]))]\n",
    "                            data = rows[1:] if len(rows) > 1 else rows\n",
    "                            df = pd.DataFrame(data, columns=headers)\n",
    "                        else:\n",
    "                            df = pd.DataFrame({\"Contenu\": lines})\n",
    "                    else:\n",
    "                        df = pd.DataFrame([line.split('\\t') for line in lines])\n",
    "                else:\n",
    "                    df = pd.DataFrame({\"Contenu\": lines})\n",
    "                \n",
    "                # Sauvegarder en CSV avec encodage pour Excel\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # Vérifier que le fichier existe bien\n",
    "            if not os.path.exists(filepath):\n",
    "                logger.error(f\"Le fichier '{filepath}' n'a pas été créé\")\n",
    "                return None, \"❌ Erreur: Le fichier n'a pas été créé correctement\"\n",
    "            \n",
    "            # Vérifier la taille du fichier\n",
    "            file_size = os.path.getsize(filepath)\n",
    "            if file_size == 0:\n",
    "                logger.error(f\"Le fichier '{filepath}' est vide\")\n",
    "                return None, \"❌ Erreur: Le fichier généré est vide\"\n",
    "                \n",
    "            logger.info(f\"Fichier créé avec succès: {filepath} ({file_size} octets)\")\n",
    "            return filepath, f\"✅ Fichier '{display_name}' généré avec succès ({file_size} octets)\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\", exc_info=True)\n",
    "            return None, f\"❌ Erreur lors de la génération du fichier : {str(e)}\"\n",
    "\n",
    "def build_ui():\n",
    "    \"\"\"Construit l'interface utilisateur moderne\"\"\"\n",
    "    assistant = DocumentChatAssistant()\n",
    "    \n",
    "    # Thème personnalisé - inspiré de TestCaseGenius\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant (Local LLM)\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tool-section {margin-top: 20px; padding-top: 20px; border-top: 1px solid #e5e7eb;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .status-box {padding: 10px; border-radius: 5px; font-weight: 500;}\n",
    "        .status-success {background-color: #dcfce7; color: #166534;}\n",
    "        .status-error {background-color: #fee2e2; color: #b91c1c;}\n",
    "        .status-info {background-color: #e0f2fe; color: #0369a1;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "        .model-info {padding: 5px 10px; background-color: #f3f4f6; border-radius: 4px; font-size: 0.85rem; color: #4b5563;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant (Local LLM)\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B\n",
    "            \"\"\")\n",
    "            \n",
    "        # Indicateur de statut du modèle local\n",
    "        with gr.Row():\n",
    "            model_status = gr.HTML(\n",
    "                f\"\"\"<div class=\"model-info\">✓ Modèle local: <strong>{LM_STUDIO_MODEL}</strong> | Serveur: <strong>{LM_STUDIO_API_URL}</strong></div>\"\"\", \n",
    "                elem_classes=\"model-info\"\n",
    "            )\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        # Section document\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"], \n",
    "                                label=\"Téléversez un document\"\n",
    "                            )\n",
    "                            file_info = gr.Textbox(\n",
    "                                label=\"Informations du fichier\", \n",
    "                                placeholder=\"Aucun document chargé\",\n",
    "                                interactive=False\n",
    "                            )\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(\n",
    "                                    label=\"Contenu du document\", \n",
    "                                    lines=12,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                            \n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            with gr.Row():\n",
    "                                context_mode = gr.Radio(\n",
    "                                    [\"Standard\", \"Avec contexte\"], \n",
    "                                    value=\"Avec contexte\", \n",
    "                                    label=\"Mode de génération\"\n",
    "                                )\n",
    "                                \n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                with gr.Row():\n",
    "                                    download_format = gr.Dropdown(\n",
    "                                        [\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], \n",
    "                                        value=\"docx\", \n",
    "                                        label=\"Format d'export\"\n",
    "                                    )\n",
    "                                    download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(\n",
    "                                    label=\"Statut\", \n",
    "                                    visible=True,\n",
    "                                    interactive=False\n",
    "                                )\n",
    "                                # Utiliser File avec type=\"filepath\" pour le téléchargement\n",
    "                                download_file = gr.File(\n",
    "                                    label=\"Télécharger le fichier généré\", \n",
    "                                    interactive=False,\n",
    "                                    type=\"filepath\",\n",
    "                                    elem_classes=\"download-btn\"\n",
    "                                )\n",
    "                            \n",
    "                    with gr.Column(scale=2):\n",
    "                        # Section chat\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(\n",
    "                                label=\"Conversation\", \n",
    "                                height=500,\n",
    "                                elem_classes=\"chatbox\"\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                user_input = gr.Textbox(\n",
    "                                    label=\"Votre message\",\n",
    "                                    placeholder=\"Posez une question sur votre document...\",\n",
    "                                    lines=2\n",
    "                                )\n",
    "                            with gr.Row():\n",
    "                                with gr.Column(scale=3):\n",
    "                                    send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                with gr.Column(scale=1):\n",
    "                                    clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "                \n",
    "                **Document Chat Assistant (Local LLM)** est un outil qui vous permet de discuter avec vos documents en utilisant un modèle d'IA local (Mistral-7B).\n",
    "                \n",
    "                #### Prérequis :\n",
    "                \n",
    "                1. **LM Studio** doit être installé et en cours d'exécution sur votre ordinateur\n",
    "                2. Le modèle **mistral-7b-instruct-v0.3** doit être chargé dans LM Studio\n",
    "                3. Le serveur API local doit être activé (généralement sur localhost:1234)\n",
    "                \n",
    "                #### Comment utiliser l'outil :\n",
    "                \n",
    "                1. **Téléchargez un document**\n",
    "                   - Formats supportés : PDF, DOCX, TXT\n",
    "                   - Le document servira de contexte à l'IA\n",
    "                \n",
    "                2. **Choisissez vos options**\n",
    "                   - **Mode Standard** : conversation normale sans contexte\n",
    "                   - **Mode Avec contexte** : utilise votre document comme référence\n",
    "                \n",
    "                3. **Posez des questions**\n",
    "                   - Soyez précis dans vos requêtes\n",
    "                   - Exemple : \"Peux-tu résumer ce document ?\" ou \"Quels sont les points principaux abordés ?\"\n",
    "                \n",
    "                4. **Exportez le résultat**\n",
    "                   - Téléchargez les réponses de l'IA au format TXT, PDF, DOCX, XLSX ou CSV\n",
    "                \n",
    "                #### Avantages de l'utilisation en local :\n",
    "                \n",
    "                - **Confidentialité** : Vos documents et conversations restent sur votre ordinateur\n",
    "                - **Sans connexion** : Fonctionne sans accès à Internet\n",
    "                - **Gratuit** : Pas de frais d'API\n",
    "                \n",
    "                #### Dépannage :\n",
    "                \n",
    "                - Si vous obtenez une erreur de connexion, vérifiez que LM Studio est bien lancé\n",
    "                - Vérifiez que le serveur local est activé dans LM Studio (bouton \"Start Server\")\n",
    "                - Si le modèle est lent, essayez de réduire la taille des documents\n",
    "                \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Connexion des événements\n",
    "        file_input.upload(\n",
    "            assistant.process_document, \n",
    "            inputs=[file_input], \n",
    "            outputs=[file_info, preview_accordion, preview]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            assistant.generate_response, \n",
    "            inputs=[user_input, chatbot, context_mode], \n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\", \n",
    "            outputs=[user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        download_btn.click(\n",
    "            assistant.download_response, \n",
    "            inputs=[download_format], \n",
    "            outputs=[download_file, download_status]\n",
    "        )\n",
    "        \n",
    "    return ui\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Vérifier la connexion à LM Studio avant de lancer l'interface\n",
    "        try:\n",
    "            test_payload = {\n",
    "                \"model\": LM_STUDIO_MODEL,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Test de connexion\"}],\n",
    "                \"max_tokens\": 10\n",
    "            }\n",
    "            response = requests.post(\n",
    "                LM_STUDIO_API_URL, \n",
    "                headers={\"Content-Type\": \"application/json\"}, \n",
    "                data=json.dumps(test_payload),\n",
    "                timeout=5\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                logger.info(f\"Connexion à LM Studio réussie. Modèle: {LM_STUDIO_MODEL}\")\n",
    "            else:\n",
    "                logger.warning(f\"LM Studio est accessible mais a retourné une erreur: {response.status_code}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logger.warning(\"Impossible de se connecter à LM Studio. L'application va démarrer, mais vérifiez que LM Studio est en cours d'exécution.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Erreur lors du test de connexion à LM Studio: {e}\")\n",
    "            \n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7899, share=False)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bdbb9bf-fa93-45b5-8526-4977f340cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:55:41,349 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:55:41,433 - INFO: HTTP Request: GET http://localhost:7091/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 14:55:43,489 - INFO: HTTP Request: HEAD http://localhost:7091/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7091/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:56:45,792 - ERROR: API Error 400: {\"error\":\"Error rendering prompt with jinja template: \\\"Error: Only user and assistant roles are supported!\\n    at C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:254303\\n    at _0x1f852a.value (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:252374)\\n    at _0x5c2bc5.evaluateCallExpression (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:241449)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:251261)\\n    at _0x5c2bc5.evaluateBlock (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:240633)\\n    at _0x5c2bc5.evaluateIf (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:245045)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:249965)\\n    at _0x5c2bc5.evaluateBlock (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:240633)\\n    at _0x5c2bc5.evaluateIf (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:245045)\\n    at _0x5c2bc5.evaluate (C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Programs\\\\LM Studio\\\\resources\\\\app\\\\.webpack\\\\lib\\\\llmworker.js:105:249965)\\\". This is usually an issue with the model's prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration de LM Studio\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "LM_STUDIO_CHAT_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/chat/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non supporté\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, str]:\n",
    "        if file is None:\n",
    "            return \"\", \"\"\n",
    "        try:\n",
    "            self.document_name = os.path.basename(file.name)\n",
    "            self.current_text = self.processor.extract_text_from_file(file.name)\n",
    "            preview = self.current_text[:1500] + (\"\\n\\n[...]\" if len(self.current_text) > 1500 else \"\")\n",
    "            info = f\"📄 {self.document_name} | {len(self.current_text)} caractères\"\n",
    "            return info, preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur traitement document: {e}\")\n",
    "            return \"\", f\"Erreur: {e}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Dict[str, str]], mode: str) -> List[Dict[str, str]]:\n",
    "        if history is None:\n",
    "            history = []\n",
    "        if not message:\n",
    "            return history\n",
    "        # Construire le contexte\n",
    "        context = \"\"\n",
    "        if mode == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:3000]\n",
    "            context = (\n",
    "                f\"Contexte (doc: {self.document_name}):\\n{excerpt}\"\n",
    "                + (\"\\n[...]\" if len(self.current_text) > 3000 else \"\")\n",
    "                + \"\\n\\n\"\n",
    "            )\n",
    "        system_msg = {\"role\": \"system\", \"content\": f\"Vous êtes un assistant. {context}\"}\n",
    "        # Messages pour l'API\n",
    "        api_messages = [system_msg] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"messages\": api_messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_CHAT_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            if resp.status_code == 200:\n",
    "                data = resp.json()\n",
    "                reply = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                self.last_response = reply\n",
    "                history.append({\"role\": \"user\", \"content\": message})\n",
    "                history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            else:\n",
    "                err = f\"API Error {resp.status_code}: {resp.text}\"\n",
    "                logger.error(err)\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"Erreur: impossible de générer une réponse. Vérifiez LM Studio sur {LM_STUDIO_BASE_URL}\"})\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Connection error: {e}\")\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Erreur connexion à LM Studio: {e}\"})\n",
    "        return history\n",
    "\n",
    "    def download_response(self, fmt: str) -> Tuple[str, str]:\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucune réponse générée\"\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{fmt}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{fmt}\"\n",
    "        try:\n",
    "            if fmt == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif fmt == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", size=11)\n",
    "                for p in self.last_response.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif fmt == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"Réponse\", level=1)\n",
    "                for p in self.last_response.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif fmt in [\"xlsx\", \"csv\"]:\n",
    "                lines = [l for l in self.last_response.split(\"\\n\") if l.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                if fmt == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {fmt}\"\n",
    "            size = os.path.getsize(path)\n",
    "            logger.info(f\"Fichier généré: {path} ({size} octets)\")\n",
    "            return path, f\"Fichier '{name}' généré ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export: {e}\")\n",
    "            return None, f\"Erreur export: {e}\"\n",
    "\n",
    "# Construction de l'interface Gradio\n",
    "assistant = DocumentChatAssistant()\n",
    "with gr.Blocks(title=\"Document Chat Assistant\") as demo:\n",
    "    gr.Markdown(\"# 💬 Chat Assistant Local (Mistral-7B)\")\n",
    "    gr.HTML(f\"<p>Serveur LM Studio: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Chat avec documents\"):\n",
    "            file_input = gr.File(label=\"Téléversez un document (.pdf, .docx, .txt)\")\n",
    "            file_info = gr.Textbox(interactive=False, label=\"Infos fichier\")\n",
    "            preview = gr.Textbox(lines=10, interactive=False, label=\"Aperçu du document\")\n",
    "            mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode\")\n",
    "            chatbot = gr.Chatbot(type=\"messages\", label=\"Conversation\")\n",
    "            user_input = gr.Textbox(lines=2, placeholder=\"Votre message...\", label=\"Message\")\n",
    "            send = gr.Button(\"Envoyer\")\n",
    "            fmt = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format export\")\n",
    "            dl_btn = gr.Button(\"Télécharger\")\n",
    "            dl_file = gr.File(interactive=False, type=\"filepath\", label=\"Fichier exporté\")\n",
    "            dl_status = gr.Textbox(interactive=False, label=\"Statut\")\n",
    "\n",
    "            file_input.upload(\n",
    "                assistant.process_document,\n",
    "                inputs=[file_input],\n",
    "                outputs=[file_info, preview]\n",
    "            )\n",
    "            send.click(\n",
    "                assistant.generate_response,\n",
    "                inputs=[user_input, chatbot, mode],\n",
    "                outputs=[chatbot]\n",
    "            ).then(\n",
    "                lambda: \"\",\n",
    "                outputs=[user_input]\n",
    "            )\n",
    "            user_input.submit(\n",
    "                assistant.generate_response,\n",
    "                inputs=[user_input, chatbot, mode],\n",
    "                outputs=[chatbot]\n",
    "            ).then(\n",
    "                lambda: \"\",\n",
    "                outputs=[user_input]\n",
    "            )\n",
    "            dl_btn.click(\n",
    "                assistant.download_response,\n",
    "                inputs=[fmt],\n",
    "                outputs=[dl_file, dl_status]\n",
    "            )\n",
    "        with gr.TabItem(\"Aide\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## Guide d'utilisation\n",
    "            1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "            2. Vérifiez que le serveur local (127.0.0.1:1234) est démarré\n",
    "            3. Téléversez un document et posez vos questions\n",
    "            4. Exportez la réponse si besoin\n",
    "            \"\"\")\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7091)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a788a070-3c5d-44ce-bb77-a1447adab695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:05:20,253 - INFO: HTTP Request: GET http://localhost:7902/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 15:05:22,005 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 15:05:22,317 - INFO: HTTP Request: HEAD http://localhost:7902/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7902/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:06:37,728 - ERROR: Connection error: HTTPConnectionPool(host='127.0.0.1', port=1234): Read timed out. (read timeout=30)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# LM Studio configuration (via .env or defaults)\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "LM_STUDIO_CHAT_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/chat/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non supporté\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.last_response = \"\"\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, gr.update, str]:\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        try:\n",
    "            self.document_name = os.path.basename(file.name)\n",
    "            self.current_text = self.processor.extract_text_from_file(file.name)\n",
    "            preview = self.current_text[:1500]\n",
    "            if len(self.current_text) > 1500:\n",
    "                preview += \"\\n\\n[... Texte tronqué]\"\n",
    "            info = f\"📄 Fichier : {self.document_name} | 📊 {len(self.current_text)} caractères\"\n",
    "            return info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return \"\", gr.update(visible=False), f\"Erreur de traitement : {e}\"\n",
    "\n",
    "    def generate_response(self, message: str, history: List[Dict[str, str]], context_type: str) -> List[Dict[str, str]]:\n",
    "        # history: list of {'role': 'user'/'assistant', 'content': ...}\n",
    "        if history is None:\n",
    "            history = []\n",
    "        if not message:\n",
    "            return history\n",
    "        # Build context prefix\n",
    "        prefix = \"\"\n",
    "        if context_type == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:2000]\n",
    "            prefix = f\"Contexte du document '{self.document_name}':\\n{excerpt}\"\n",
    "            if len(self.current_text) > 2000:\n",
    "                prefix += \"\\n[... Reste du document tronqué ...]\"\n",
    "            prefix += \"\\n\\n\"\n",
    "        # New user message\n",
    "        user_content = prefix + message\n",
    "        # Prepare messages list without system role\n",
    "        api_messages = history.copy()\n",
    "        api_messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"messages\": api_messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_CHAT_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            if resp.status_code == 200:\n",
    "                data = resp.json()\n",
    "                reply = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                logger.error(f\"API Error {resp.status_code}: {resp.text}\")\n",
    "                reply = f\"Erreur: impossible de générer une réponse. Vérifiez LM Studio sur {LM_STUDIO_BASE_URL}\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Connection error: {e}\")\n",
    "            reply = f\"Erreur connexion à LM Studio: {e}\"\n",
    "        # Append assistant reply\n",
    "        history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        self.last_response = reply\n",
    "        return history\n",
    "\n",
    "    def download_response(self, file_format: str):\n",
    "        if not self.last_response:\n",
    "            return None, \"Aucun contenu à télécharger.\"\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(self.last_response)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", size=11)\n",
    "                for p in self.last_response.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"Réponse générée\", level=1)\n",
    "                for p in self.last_response.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                lines = [l for l in self.last_response.split(\"\\n\") if l.strip()]\n",
    "                df = pd.DataFrame({\"Contenu\": lines})\n",
    "                if file_format == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {file_format}\"\n",
    "            size = os.path.getsize(path)\n",
    "            logger.info(f\"Fichier généré: {path} ({size} octets)\")\n",
    "            return path, f\"✅ Fichier '{name}' généré ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export : {e}\")\n",
    "            return None, f\"❌ Erreur export : {e}\"\n",
    "\n",
    "# Build UI\n",
    "assistant = DocumentChatAssistant()\n",
    "\n",
    "def build_ui():\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B (LM Studio)\n",
    "            \"\"\")\n",
    "\n",
    "        gr.HTML(f\"<p>Serveur local: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"Téléversez un document\")\n",
    "                            file_info = gr.Textbox(label=\"Informations du fichier\", interactive=False)\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(label=\"Contenu du document\", lines=12, interactive=False)\n",
    "\n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode de génération\")\n",
    "\n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format d'export\")\n",
    "                                download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(label=\"Statut\", interactive=False)\n",
    "                                download_file = gr.File(label=\"Télécharger le fichier généré\", interactive=False, type=\"filepath\", elem_classes=\"download-btn\")\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(label=\"Conversation\", height=500, type=\"messages\")\n",
    "                            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question sur votre document...\", lines=2)\n",
    "                            with gr.Row():\n",
    "                                send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "\n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "\n",
    "                1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "                2. Vérifiez que le serveur local (127.0.0.1:1234) est démarré\n",
    "                3. Téléversez un document et posez vos questions\n",
    "                4. Exportez la réponse si besoin\n",
    "                \"\"\")\n",
    "\n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(assistant.process_document, inputs=[file_input], outputs=[file_info, preview_accordion, preview])\n",
    "        send_btn.click(assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot]).then(lambda: \"\", outputs=[user_input])\n",
    "        user_input.submit(assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot]).then(lambda: \"\", outputs=[user_input])\n",
    "        clear_btn.click(lambda: [], outputs=[chatbot])\n",
    "        download_btn.click(assistant.download_response, inputs=[download_format], outputs=[download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7902)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c0f35f-6036-42d2-8c97-f38603d66e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\1164165499.py:224: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:17:50,145 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:17:51,133 - INFO: HTTP Request: GET http://localhost:7199/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:17:53,200 - INFO: HTTP Request: HEAD http://localhost:7199/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7199/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:18:28,030 - INFO: Envoi requête à http://127.0.0.1:1234/v1/completions\n",
      "2025-04-15 16:19:26,915 - INFO: Réponse code 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# LM Studio configuration (via .env or defaults)\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "# Use completions endpoint to avoid Jinja template issues\n",
    "LM_STUDIO_COMPLETIONS_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non supporté\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.history: List[Tuple[str, str]] = []  # list of (user, assistant)\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, gr.update, str]:\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        try:\n",
    "            path = file.name if hasattr(file, 'name') else file\n",
    "            self.document_name = os.path.basename(path)\n",
    "            self.current_text = self.processor.extract_text_from_file(path)\n",
    "            preview = self.current_text[:1500] + (\"\\n\\n[...]\" if len(self.current_text) > 1500 else \"\")\n",
    "            size_kb = os.path.getsize(path) / 1024\n",
    "            info = f\"📄 {self.document_name} ({size_kb:.1f} KB, {len(self.current_text)} caractères)\"\n",
    "            return info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return f\"Erreur: {e}\", gr.update(visible=False), \"\"\n",
    "\n",
    "    def generate_response(self, message: str, chat_history: List[Tuple[str, str]], context_type: str) -> Tuple[List[Tuple[str, str]], str]:\n",
    "        if not message.strip():\n",
    "            return chat_history, \"\"\n",
    "        # Build prompt\n",
    "        prompt = \"\"\n",
    "        if context_type == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:2000]\n",
    "            prompt += f\"Contexte du document '{self.document_name}':\\n{excerpt}\\n[...\\n]\\n\\n\"\n",
    "        # Append last messages\n",
    "        for u, a in chat_history[-3:]:  # keep last 3 exchanges\n",
    "            prompt += f\"Utilisateur: {u}\\nAssistant: {a}\\n\"\n",
    "        prompt += f\"Utilisateur: {message}\\nAssistant:\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"stop\": [\"Utilisateur:\", \"Assistant:\"]\n",
    "        }\n",
    "        try:\n",
    "            logger.info(f\"Envoi requête à {LM_STUDIO_COMPLETIONS_ENDPOINT}\")\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_COMPLETIONS_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=120\n",
    "            )\n",
    "            logger.info(f\"Réponse code {resp.status_code}\")\n",
    "            if resp.status_code != 200:\n",
    "                logger.error(f\"API Error {resp.status_code}: {resp.text}\")\n",
    "                reply = \"Erreur API, vérifiez que LM Studio est en cours d'exécution.\"\n",
    "            else:\n",
    "                data = resp.json()\n",
    "                reply = data.get('choices', [{}])[0].get('text', '').strip()\n",
    "                if not reply:\n",
    "                    reply = \"Aucune réponse reçue.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur requête: {e}\")\n",
    "            reply = f\"Erreur de connexion ou timeout: {e}\"\n",
    "\n",
    "        chat_history.append((message, reply))\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    def download_response(self, file_format: str) -> Tuple[str, str]:\n",
    "        if not self.history:\n",
    "            return None, \"Aucun contenu à télécharger.\"\n",
    "        content = self.history[-1][1]\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", 11)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"Réponse générée\", 1)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                df = pd.DataFrame({\"Contenu\": content.split(\"\\n\")})\n",
    "                if file_format == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {file_format}\"\n",
    "            size = os.path.getsize(path)\n",
    "            return path, f\"✅ Fichier '{name}' généré ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export: {e}\")\n",
    "            return None, f\"Erreur export: {e}\"\n",
    "\n",
    "# Build UI\n",
    "assistant = DocumentChatAssistant()\n",
    "\n",
    "def build_ui():\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B (LM Studio)\n",
    "            \"\"\")\n",
    "\n",
    "        gr.HTML(f\"<p>Serveur local: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"Téléversez un document\")\n",
    "                            file_info = gr.Textbox(label=\"Informations du fichier\", interactive=False)\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(label=\"Contenu du document\", lines=12, interactive=False)\n",
    "\n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode de génération\")\n",
    "\n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format d'export\")\n",
    "                                download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(label=\"Statut\", interactive=False)\n",
    "                                download_file = gr.File(label=\"Télécharger le fichier généré\", interactive=False, type=\"filepath\", elem_classes=\"download-btn\")\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n",
    "                            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question sur votre document...\", lines=2)\n",
    "                            with gr.Row():\n",
    "                                send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "\n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "\n",
    "                1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "                2. Vérifiez que le serveur local (127.0.0.1:1234) est démarré\n",
    "                3. Téléversez un document et posez vos questions\n",
    "                4. Exportez la réponse si besoin\n",
    "                \"\"\")\n",
    "\n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(assistant.process_document, inputs=[file_input], outputs=[file_info, preview_accordion, preview])\n",
    "        send_btn.click(assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        user_input.submit(assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "        download_btn.click(assistant.download_response, inputs=[download_format], outputs=[download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        app = build_ui()\n",
    "        app.launch(server_name=\"0.0.0.0\", server_port=7199)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Erreur de lancement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0135f167-cd2e-4b4c-a25e-cd02c6a321c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\2357147622.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:26:35,398 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:26:36,369 - INFO: HTTP Request: GET http://localhost:7192/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:26:38,425 - INFO: HTTP Request: HEAD http://localhost:7192/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7192/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:27:14,105 - INFO: Envoi requête à http://127.0.0.1:1234/v1/completions\n",
      "2025-04-15 16:28:03,041 - INFO: Réponse code 200\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1663, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\2357147622.py\", line 255, in handle_download\n",
      "    return gr.File.update(value=path), status\n",
      "           ^^^^^^^^^^^^^^\n",
      "AttributeError: type object 'File' has no attribute 'update'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# LM Studio configuration\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "LM_STUDIO_COMPLETIONS_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non supporté\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.history: List[Tuple[str, str]] = []\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, gr.update, str]:\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        try:\n",
    "            path = file.name if hasattr(file, 'name') else file\n",
    "            self.document_name = os.path.basename(path)\n",
    "            self.current_text = self.processor.extract_text_from_file(path)\n",
    "            preview = self.current_text[:1500] + (\"\\n\\n[...]\" if len(self.current_text) > 1500 else \"\")\n",
    "            size_kb = os.path.getsize(path) / 1024\n",
    "            info = f\"📄 {self.document_name} ({size_kb:.1f} KB, {len(self.current_text)} caractères)\"\n",
    "            return info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return f\"Erreur: {e}\", gr.update(visible=False), \"\"\n",
    "\n",
    "    def generate_response(self, message: str, chat_history: List[Tuple[str, str]], context_type: str) -> Tuple[List[Tuple[str, str]], str]:\n",
    "        if not message.strip():\n",
    "            return chat_history, \"\"\n",
    "        # Build prompt\n",
    "        prompt = \"\"\n",
    "        if context_type == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:2000]\n",
    "            prompt += f\"Contexte du document '{self.document_name}':\\n{excerpt}\\n[...\\n]\\n\\n\"\n",
    "        # Append last messages\n",
    "        for u, a in chat_history[-3:]:\n",
    "            prompt += f\"Utilisateur: {u}\\nAssistant: {a}\\n\"\n",
    "        prompt += f\"Utilisateur: {message}\\nAssistant:\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"stop\": [\"Utilisateur:\", \"Assistant:\"]\n",
    "        }\n",
    "        try:\n",
    "            logger.info(f\"Envoi requête à {LM_STUDIO_COMPLETIONS_ENDPOINT}\")\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_COMPLETIONS_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=120\n",
    "            )\n",
    "            logger.info(f\"Réponse code {resp.status_code}\")\n",
    "            if resp.status_code != 200:\n",
    "                logger.error(f\"API Error {resp.status_code}: {resp.text}\")\n",
    "                reply = \"Erreur API, vérifiez que LM Studio est en cours d'exécution.\"\n",
    "            else:\n",
    "                data = resp.json()\n",
    "                reply = data.get('choices', [{}])[0].get('text', '').strip() or \"Aucune réponse reçue.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur requête: {e}\")\n",
    "            reply = f\"Erreur de connexion ou timeout: {e}\"\n",
    "\n",
    "        # Update both chat history and internal history\n",
    "        chat_history.append((message, reply))\n",
    "        self.history = chat_history\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    def download_response(self, file_format: str) -> Tuple[str, str]:\n",
    "        if not self.history:\n",
    "            return None, \"Aucun contenu à télécharger.\"\n",
    "        content = self.history[-1][1]\n",
    "        if not content.strip():\n",
    "            return None, \"La réponse est vide.\"\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", 11)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"Réponse générée\", 1)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                df = pd.DataFrame({\"Contenu\": content.split(\"\\n\")})\n",
    "                if file_format == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {file_format}\"\n",
    "            size = os.path.getsize(path)\n",
    "            return path, f\"✅ Fichier '{name}' généré ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export: {e}\")\n",
    "            return None, f\"Erreur export: {e}\"\n",
    "\n",
    "# Build UI\n",
    "demo_assistant = DocumentChatAssistant()\n",
    "\n",
    "def build_ui():\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    ).set(\n",
    "        button_primary_background_fill=\"*primary_500\",\n",
    "        button_primary_background_fill_hover=\"*primary_600\",\n",
    "        button_primary_text_color=\"white\",\n",
    "        button_secondary_background_fill=\"*neutral_100\",\n",
    "        button_secondary_background_fill_hover=\"*neutral_200\",\n",
    "        block_label_background_fill=\"*neutral_50\",\n",
    "        block_label_text_color=\"*neutral_800\",\n",
    "        block_title_text_weight=\"600\",\n",
    "        block_background_fill=\"white\",\n",
    "        block_border_width=\"1px\",\n",
    "        block_border_color=\"*neutral_200\",\n",
    "        block_radius=\"lg\",\n",
    "        checkbox_background_color=\"*neutral_100\",\n",
    "        checkbox_background_color_selected=\"*primary_500\",\n",
    "        checkbox_border_color=\"*neutral_300\",\n",
    "        checkbox_border_color_focus=\"*primary_500\",\n",
    "        checkbox_border_color_hover=\"*primary_400\",\n",
    "        checkbox_border_color_selected=\"*primary_500\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B (LM Studio)\n",
    "            \"\"\")\n",
    "\n",
    "        gr.HTML(f\"<p>Serveur local: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"Téléversez un document\")\n",
    "                            file_info = gr.Textbox(label=\"Informations du fichier\", interactive=False)\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(label=\"Contenu du document\", lines=12, interactive=False)\n",
    "\n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode de génération\")\n",
    "\n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format d'export\")\n",
    "                                download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(label=\"Statut\", interactive=False)\n",
    "                                download_file = gr.File(label=\"Télécharger le fichier généré\", interactive=False, type=\"filepath\", elem_classes=\"download-btn\")\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n",
    "                            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question sur votre document...\", lines=2)\n",
    "                            with gr.Row():\n",
    "                                send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "\n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "\n",
    "                1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "                2. Vérifiez que le serveur local (127.0.0.1:1234) est démarré\n",
    "                3. Téléversez un document et posez vos questions\n",
    "                4. Exportez la réponse si besoin\n",
    "                \"\"\")\n",
    "\n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(demo_assistant.process_document, inputs=[file_input], outputs=[file_info, preview_accordion, preview])\n",
    "        send_btn.click(demo_assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        user_input.submit(demo_assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "        # Wrap download to update file component\n",
    "        def handle_download(fmt):\n",
    "            path, status = demo_assistant.download_response(fmt)\n",
    "            return gr.File.update(value=path), status\n",
    "        download_btn.click(handle_download, inputs=[download_format], outputs=[download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_ui().launch(server_name=\"0.0.0.0\", server_port=7192)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b46bd1bd-a439-47fc-9e0f-664cd07a3839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7208\\1582274602.py:206: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:38:09,241 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:38:10,394 - INFO: HTTP Request: GET http://localhost:7193/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-15 16:38:12,428 - INFO: HTTP Request: HEAD http://localhost:7193/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7193/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:38:50,298 - INFO: Envoi requête à http://127.0.0.1:1234/v1/completions\n",
      "2025-04-15 16:39:42,683 - INFO: Réponse code 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import gradio as gr\n",
    "import mammoth\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# LM Studio configuration\n",
    "LM_STUDIO_HOST = os.getenv(\"LM_STUDIO_HOST\", \"127.0.0.1\")\n",
    "LM_STUDIO_PORT = os.getenv(\"LM_STUDIO_PORT\", \"1234\")\n",
    "LM_STUDIO_BASE_URL = f\"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}\"\n",
    "LM_STUDIO_COMPLETIONS_ENDPOINT = f\"{LM_STUDIO_BASE_URL}/v1/completions\"\n",
    "LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_file(file_path: str, max_chars: int = 10000) -> str:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            elif ext == '.docx':\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    text = mammoth.extract_raw_text(f).value\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            else:\n",
    "                return \"Format non supporté\"\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur extraction texte: {e}\")\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "class DocumentChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.current_text = \"\"\n",
    "        self.document_name = \"\"\n",
    "        self.history: List[Tuple[str, str]] = []\n",
    "\n",
    "    def process_document(self, file) -> Tuple[str, gr.update, str]:\n",
    "        if file is None:\n",
    "            return \"\", gr.update(visible=False), \"Aucun fichier sélectionné\"\n",
    "        try:\n",
    "            path = file.name if hasattr(file, 'name') else file\n",
    "            self.document_name = os.path.basename(path)\n",
    "            self.current_text = self.processor.extract_text_from_file(path)\n",
    "            preview = self.current_text[:1500] + (\"\\n\\n[...]\" if len(self.current_text) > 1500 else \"\")\n",
    "            size_kb = os.path.getsize(path) / 1024\n",
    "            info = f\"📄 {self.document_name} ({size_kb:.1f} KB, {len(self.current_text)} caractères)\"\n",
    "            return info, gr.update(visible=True), preview\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur de traitement : {e}\")\n",
    "            return f\"Erreur: {e}\", gr.update(visible=False), \"\"\n",
    "\n",
    "    def generate_response(self, message: str, chat_history: List[Tuple[str, str]], context_type: str) -> Tuple[List[Tuple[str, str]], str]:\n",
    "        if not message.strip():\n",
    "            return chat_history, \"\"\n",
    "        # Build prompt\n",
    "        prompt = \"\"\n",
    "        if context_type == \"Avec contexte\" and self.current_text:\n",
    "            excerpt = self.current_text[:2000]\n",
    "            prompt += f\"Contexte du document '{self.document_name}':\\n{excerpt}\\n[...\\n]\\n\\n\"\n",
    "        # Append last messages\n",
    "        for u, a in chat_history[-3:]:\n",
    "            prompt += f\"Utilisateur: {u}\\nAssistant: {a}\\n\"\n",
    "        prompt += f\"Utilisateur: {message}\\nAssistant:\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LM_STUDIO_MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"stop\": [\"Utilisateur:\", \"Assistant:\"]\n",
    "        }\n",
    "        try:\n",
    "            logger.info(f\"Envoi requête à {LM_STUDIO_COMPLETIONS_ENDPOINT}\")\n",
    "            resp = requests.post(\n",
    "                LM_STUDIO_COMPLETIONS_ENDPOINT,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                json=payload,\n",
    "                timeout=120\n",
    "            )\n",
    "            logger.info(f\"Réponse code {resp.status_code}\")\n",
    "            if resp.status_code != 200:\n",
    "                logger.error(f\"API Error {resp.status_code}: {resp.text}\")\n",
    "                reply = \"Erreur API, vérifiez que LM Studio est en cours d'exécution.\"\n",
    "            else:\n",
    "                data = resp.json()\n",
    "                reply = data.get('choices', [{}])[0].get('text', '').strip() or \"Aucune réponse reçue.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur requête: {e}\")\n",
    "            reply = f\"Erreur de connexion ou timeout: {e}\"\n",
    "\n",
    "        # Update both chat history and internal history\n",
    "        chat_history.append((message, reply))\n",
    "        self.history = chat_history\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    def download_response(self, file_format: str) -> Tuple[str, str]:\n",
    "        if not self.history:\n",
    "            return None, \"Aucun contenu à télécharger.\"\n",
    "        content = self.history[-1][1]\n",
    "        if not content.strip():\n",
    "            return None, \"La réponse est vide.\"\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
    "        tmp.close()\n",
    "        path = tmp.name\n",
    "        name = f\"reponse_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{file_format}\"\n",
    "        try:\n",
    "            if file_format == \"txt\":\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "            elif file_format == \"pdf\":\n",
    "                pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", 11)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    pdf.multi_cell(190, 7, p); pdf.ln(3)\n",
    "                pdf.output(path)\n",
    "            elif file_format == \"docx\":\n",
    "                doc = Document(); doc.add_heading(\"Réponse générée\", 1)\n",
    "                for p in content.split(\"\\n\\n\"):\n",
    "                    doc.add_paragraph(p)\n",
    "                doc.save(path)\n",
    "            elif file_format in [\"xlsx\", \"csv\"]:\n",
    "                df = pd.DataFrame({\"Contenu\": content.split(\"\\n\")})\n",
    "                if file_format == \"xlsx\": df.to_excel(path, index=False)\n",
    "                else: df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                return None, f\"Format inconnu: {file_format}\"\n",
    "            size = os.path.getsize(path)\n",
    "            return path, f\"✅ Fichier '{name}' généré ({size} octets)\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export: {e}\")\n",
    "            return None, f\"Erreur export: {e}\"\n",
    "\n",
    "# Build UI\n",
    "demo_assistant = DocumentChatAssistant()\n",
    "\n",
    "def build_ui():\n",
    "    theme = gr.themes.Soft(\n",
    "        primary_hue=\"blue\",\n",
    "        secondary_hue=\"indigo\",\n",
    "        neutral_hue=\"slate\",\n",
    "        radius_size=gr.themes.sizes.radius_md,\n",
    "        font=[gr.themes.GoogleFont(\"Inter\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(theme=theme, title=\"Document Chat Assistant\", css=\"\"\"\n",
    "        .container {box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; padding: 20px; margin-bottom: 20px;}\n",
    "        .header {text-align: center; margin-bottom: 20px;}\n",
    "        .header h1 {font-size: 2.5rem; font-weight: 800;}\n",
    "        .header p {color: #6b7280; font-size: 1.1rem;}\n",
    "        .tab-nav {margin-bottom: 10px;}\n",
    "        footer {text-align: center; margin-top: 40px; color: #6b7280; font-size: 0.9rem;}\n",
    "        .gr-button.gr-button-lg {padding: 10px 20px; font-size: 16px;}\n",
    "        .download-btn {display: block; width: 100%; margin-top: 10px;}\n",
    "    \"\"\") as ui:\n",
    "        with gr.Row(elem_classes=\"header\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 💬 Document Chat Assistant\n",
    "            ### Assistante IA pour vos documents avec Mistral-7B (LM Studio)\n",
    "            \"\"\")\n",
    "\n",
    "        gr.HTML(f\"<p>Serveur local: <strong>{LM_STUDIO_BASE_URL}</strong></p>\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"Chat avec documents\", id=\"chat\", elem_classes=\"tab-nav\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 📄 Document source\")\n",
    "                            file_input = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"Téléversez un document\")\n",
    "                            file_info = gr.Textbox(label=\"Informations du fichier\", interactive=False)\n",
    "                            with gr.Accordion(\"Aperçu du document\", visible=False) as preview_accordion:\n",
    "                                preview = gr.Textbox(label=\"Contenu du document\", lines=12, interactive=False)\n",
    "\n",
    "                            gr.Markdown(\"### ⚙️ Options\")\n",
    "                            context_mode = gr.Radio([\"Standard\", \"Avec contexte\"], value=\"Avec contexte\", label=\"Mode de génération\")\n",
    "\n",
    "                            with gr.Accordion(\"Options d'export\", open=False):\n",
    "                                download_format = gr.Dropdown([\"txt\", \"pdf\", \"docx\", \"xlsx\", \"csv\"], value=\"docx\", label=\"Format d'export\")\n",
    "                                download_btn = gr.Button(\"📥 Télécharger\", variant=\"secondary\")\n",
    "                                download_status = gr.Textbox(label=\"Statut\", interactive=False)\n",
    "                                download_file = gr.File(label=\"Télécharger le fichier généré\", interactive=False, type=\"filepath\", elem_classes=\"download-btn\")\n",
    "\n",
    "                    with gr.Column(scale=2):\n",
    "                        with gr.Group(elem_classes=\"container\"):\n",
    "                            gr.Markdown(\"### 💬 Conversation\")\n",
    "                            chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n",
    "                            user_input = gr.Textbox(label=\"Votre message\", placeholder=\"Posez une question sur votre document...\", lines=2)\n",
    "                            with gr.Row():\n",
    "                                send_btn = gr.Button(\"💬 Envoyer\", variant=\"primary\", size=\"lg\")\n",
    "                                clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "\n",
    "            with gr.TabItem(\"Aide\", id=\"help\", elem_classes=\"tab-nav\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### 📖 Guide d'utilisation\n",
    "\n",
    "                1. Lancez LM Studio et chargez mistral-7b-instruct-v0.3\n",
    "                2. Vérifiez que le serveur local (127.0.0.1:1234) est démarré\n",
    "                3. Téléversez un document et posez vos questions\n",
    "                4. Exportez la réponse si besoin\n",
    "                \"\"\")\n",
    "\n",
    "        gr.Markdown(\"\"\"\n",
    "        <footer>\n",
    "            <p>© 2025 Document Chat Assistant | Propulsé par Mistral-7B sur LM Studio</p>\n",
    "        </footer>\n",
    "        \"\"\")\n",
    "\n",
    "        # Events\n",
    "        file_input.upload(demo_assistant.process_document, inputs=[file_input], outputs=[file_info, preview_accordion, preview])\n",
    "        send_btn.click(demo_assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        user_input.submit(demo_assistant.generate_response, inputs=[user_input, chatbot, context_mode], outputs=[chatbot, user_input])\n",
    "        clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "\n",
    "        def handle_download(fmt):\n",
    "            path, status = demo_assistant.download_response(fmt)\n",
    "            return gr.update(value=path), status\n",
    "\n",
    "        download_btn.click(handle_download, inputs=[download_format], outputs=[download_file, download_status])\n",
    "\n",
    "    return ui\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_ui().launch(server_name=\"0.0.0.0\", server_port=7193)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae24db8c-8ae2-4416-b406-3bc7456ef7c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflask\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flask, request, render_template_string\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from flask import Flask, request, render_template_string\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# URL du serveur LM Studio local\n",
    "API_URL = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "\n",
    "# Initialisation de l'application Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Fonction pour appeler l'API LM Studio\n",
    "def generate_response(prompt, max_tokens=256):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"mistral-7b-instruct-v0.3\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, data=json.dumps(payload))\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            return f\"Erreur lors de l'appel API: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Erreur de connexion au serveur LM Studio: {str(e)}\"\n",
    "\n",
    "# Route principale : formulaire et génération\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    story_text = \"\"\n",
    "    format_choice = \"gherkin\"\n",
    "    generated_test = None\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        # Récupérer la user story et le format depuis le formulaire\n",
    "        story_text = request.form.get(\"story\", \"\").strip()\n",
    "        format_choice = request.form.get(\"format\", \"gherkin\")\n",
    "        if story_text:\n",
    "            # Construire le prompt en fonction du format choisi\n",
    "            if format_choice == \"gherkin\":\n",
    "                prompt = (\n",
    "                    f\"Voici une user story : \\\"{story_text}\\\"\\n\"\n",
    "                    \"En tant qu'assistant de test, génère un scénario de test au format Gherkin \"\n",
    "                    \"(Given/When/Then) en français.\"\n",
    "                )\n",
    "            else:  # format \"action\"\n",
    "                prompt = (\n",
    "                    f\"Voici une user story : \\\"{story_text}\\\"\\n\"\n",
    "                    \"En tant qu'assistant de test, génère un cas de test détaillant les actions \"\n",
    "                    \"à effectuer et les résultats attendus pour chaque action, en français.\"\n",
    "                )\n",
    "            # Appel au modèle local pour générer la suite de texte\n",
    "            generated_test = generate_response(prompt, max_tokens=256)\n",
    "\n",
    "    # Template HTML simple (intégré sous forme de chaîne)\n",
    "    html_template = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "      <meta charset=\"UTF-8\">\n",
    "      <title>Génération de cas de test</title>\n",
    "      <style>\n",
    "        body { \n",
    "          font-family: Arial, sans-serif; \n",
    "          max-width: 800px; \n",
    "          margin: 0 auto; \n",
    "          padding: 20px; \n",
    "        }\n",
    "        textarea { width: 100%; }\n",
    "        pre { \n",
    "          background-color: #f5f5f5; \n",
    "          padding: 15px; \n",
    "          border-radius: 5px; \n",
    "          white-space: pre-wrap; \n",
    "        }\n",
    "        button { \n",
    "          background-color: #4CAF50; \n",
    "          color: white; \n",
    "          padding: 10px 15px; \n",
    "          border: none; \n",
    "          border-radius: 4px; \n",
    "          cursor: pointer; \n",
    "        }\n",
    "        button:hover { background-color: #45a049; }\n",
    "      </style>\n",
    "    </head>\n",
    "    <body>\n",
    "      <h1>Générer des cas de test à partir d'une User Story</h1>\n",
    "      <form method=\"post\">\n",
    "        <p>\n",
    "          <textarea name=\"story\" rows=\"6\" cols=\"60\" placeholder=\"Entrez la user story ici...\">{{ story|e }}</textarea>\n",
    "        </p>\n",
    "        <p>\n",
    "          Format de test : \n",
    "          <label><input type=\"radio\" name=\"format\" value=\"gherkin\" {% if format_choice != 'action' %}checked{% endif %}> Gherkin (Given/When/Then)</label>\n",
    "          <label><input type=\"radio\" name=\"format\" value=\"action\" {% if format_choice == 'action' %}checked{% endif %}> Actions / Résultats attendus</label>\n",
    "        </p>\n",
    "        <p><button type=\"submit\">Générer</button></p>\n",
    "      </form>\n",
    "      {% if generated_test %}\n",
    "        <h2>Cas de test généré :</h2>\n",
    "        <pre>{{ generated_test }}</pre>\n",
    "      {% endif %}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    # Rendre la page HTML avec les variables (story_text, format_choice, generated_test)\n",
    "    return render_template_string(html_template, story=story_text, format_choice=format_choice, generated_test=generated_test)\n",
    "\n",
    "# Démarrer le serveur Flask si ce script est exécuté directement\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c36cc80a-ef62-40f6-bfb7-1d8f4b53edef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15827f1f-2c12-47ee-a490-7deacf08df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f2b738-b288-41fe-963e-96affcc7b6d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (24040424.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m ensurepip --upgrade\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m ensurepip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9b9499b-1e71-4be2-b8e4-24c9cc4c07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0f31d92-b5f5-49d2-9b78-e8631cb94899",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2372076907.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m pip install flask requests\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb57c3ad-ba0c-43d6-9c18-be1a4abc3a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0014d772-234d-42dc-bf93-ce7eeaad91e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2247650335.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython get-pip.py\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python get-pip.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e94d07-0fcc-41c1-abac-3263618668bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4783864c-0aae-4446-a981-065d09e12235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08687e9-1ce6-4768-a041-53485632c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efe408-3ed0-4438-968e-1083c7714652",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f300466-7d89-4b3d-9104-47169aab3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5121f4dc-676d-480a-8d74-0868084757a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d19a15-b973-4c5f-acd1-2b8edfc7a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31042d11-872e-4a4a-924f-9606760d282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3754c351-c743-4b8a-b252-b68e1506e0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28086cdf-b359-4705-840b-3425a08ae979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ffe83-a249-4299-b167-54fca0f06c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
